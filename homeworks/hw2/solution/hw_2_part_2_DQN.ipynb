{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKTBPBJy-MIp"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "In HW2 part 2, we will be implementing neural Q-learning, a deep learning approach to the Q-learning algorithm.\n",
        "\n",
        "**Deep Q-Network (DQN)** is a reinforcement learning algorithm that combines Q-learning with deep neural networks, enabling it to learn optimal policies in complex high-dimensional environments. DQNs employ two neural networks:\n",
        "\n",
        "- The Q-network (main network): Trained frequently during the learning process to approximate the action-value function.\n",
        "- The target network: Used to compute target Q-values. Provides stable target Q-values for the Q-network updates. Periodically copied from the Q-network.\n",
        "\n",
        "During training, the Q-network is updated by minimizing the difference between its predicted Q-values and the target Q-values (which are computed by the target network). This separation reduces instability and divergence, which are common challenges in reinforcement learning with function approximation.\n",
        "\n",
        "The **DQN agent** interacts with the environment, collecting experiences in the form of transitions (s,a,r,sâ€²) and storing them in the *replay buffer*. During training, mini-batches of experiences are sampled and used to train the Q-network.\n",
        "\n",
        "In Part 2, you will be implementing **DQN** and **DQN agent** which effectively operate on the TextWorld environment. This assignment is divided into the following steps:\n",
        "\n",
        "- Step 1: Implement `DQN`, a neural network used for approximating the Q-function. In this assignment, you will be working with RNN-based text encoder to obtain state representations.\n",
        "- Step 2: Implement `DQNAgent`, an agent containing `DQN`, interacts with environment, save the experience to replay buffer, and train the neural network.\n",
        "- Step 3: Implement the `run_policy` function. Similar to Part 1. Update the code to work with `DQNAgent`\n",
        "\n",
        "After finishing step 1 through step 3, you will test your DQN agent on the same environment and testing suite as Part 1.\n",
        "\n",
        "**Notes:**\n",
        "- We encourage you to finish Part 1 first before starting Part 2.\n",
        "- All the test configurations of the environment are the same as in Part 1.\n",
        "- ***DO NOT REMOVE ANY COMMENTS THAT HAVE `# EXPORT` IN THEM. THE GRADING SCRIPT USES THESE COMMENTS TO EVALUATE YOUR FUNCTIONS. WE WILL NOT AUDIT SUBMISSIONS TO ADD THESE. IF THE AUTOGRADER FAILS TO RUN DUE TO YOUR MODIFICATION OF THESE COMMENTS, YOU WILL NOT RECEIVE CREDIT.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMJadvTO-O7z"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdVL2D-B-qYC"
      },
      "source": [
        "Install the `TextWorld-Express` engine, `graphviz` and `pydot` for visualization, and `torch` for neural network implementations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Q_jkFnjg-C7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da326fa5-1361-4b7c-a0f3-ea7affcba480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: textworld-express in /usr/local/lib/python3.10/dist-packages (1.0.5)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from textworld-express) (0.10.9.7)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.10/dist-packages (from textworld-express) (3.10.12)\n",
            "Requirement already satisfied: prompt-toolkit in /usr/local/lib/python3.10/dist-packages (from textworld-express) (3.0.48)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit->textworld-express) (0.2.13)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.10/dist-packages (3.0.3)\n",
            "Requirement already satisfied: pyparsing>=3.0.9 in /usr/local/lib/python3.10/dist-packages (from pydot) (3.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install textworld-express\n",
        "!pip install graphviz\n",
        "!pip install pydot\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0GaE8xE-T6A"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_ScAIKmf-Vmm"
      },
      "outputs": [],
      "source": [
        "# export\n",
        "# imports for environment\n",
        "from textworld_express import TextWorldExpressEnv\n",
        "import gymnasium\n",
        "import graphviz\n",
        "import pydot\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "import re\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import random\n",
        "\n",
        "# imports for DQN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U9ElZek-y5I"
      },
      "source": [
        "# Load a game and utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg_yOwlT_ByQ"
      },
      "source": [
        "Set the random seed for repeatablity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "cIGNj42d-93N"
      },
      "outputs": [],
      "source": [
        "SEED = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8_CqmGx_GOo"
      },
      "source": [
        "Initialize the game environment. `ENV` is a global that encapulates the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "CRwKCeqK_JOy"
      },
      "outputs": [],
      "source": [
        "ENV = TextWorldExpressEnv(envStepLimit=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtJDssBp_Kd_"
      },
      "source": [
        "Set the game generator to generate a particular game (coin game or map reader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "piAle474_SPG"
      },
      "outputs": [],
      "source": [
        "GAME_TYPE = \"coin\"\n",
        "GAME_PARAMS = \"numLocations=5,includeDoors=1,numDistractorItems=0\"\n",
        "ENV.load(gameName=GAME_TYPE, gameParams=GAME_PARAMS)\n",
        "obs, infos = ENV.reset(seed=SEED, gameFold=\"train\", generateGoldPath=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Environment Interaction Functions (see the description in Part 1)"
      ],
      "metadata": {
        "id": "7SkcfE1CA_a4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "e5zs1sspRvDy"
      },
      "outputs": [],
      "source": [
        "# export\n",
        "def reset_mdp(env):\n",
        "  obs, infos = env.reset(seed=SEED, gameFold=\"train\", generateGoldPath=True)\n",
        "  valids = infos['validActions']\n",
        "  valids.remove('inventory')\n",
        "  valids.remove('look around')\n",
        "  inv = infos['inventory']\n",
        "  modified_obs = obs_with_inventory(infos['look'], inv)\n",
        "  # return make_state_mdp(infos['look'], parse_inventory(infos['inventory'])), valids\n",
        "  return {'observation': infos['look'],\n",
        "          'inventory': infos['inventory'],\n",
        "          'valid actions': valids}\n",
        "\n",
        "\n",
        "def do_action_mdp(action, env):\n",
        "  obs, reward, done, infos = env.step(action)\n",
        "  #obs_look, reward_look, done_look, infos_look = env.step('look around')\n",
        "  valid_actions = infos['validActions']\n",
        "  valid_actions.remove('inventory')\n",
        "  valid_actions.remove('look around')\n",
        "  # return make_state_mdp(infos['look'], parse_inventory(infos['inventory'])), reward, done, valid_actions\n",
        "  return infos['look'], reward, done, {'observation': infos['look'],\n",
        "                                       'inventory': infos['inventory'],\n",
        "                                       'valid actions': valid_actions}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the function `pad_sequences`, which is used in Step 2 to pad text sequences into the same size for batching."
      ],
      "metadata": {
        "id": "UKVzzXoVldvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequences(sequences, maxlen=None, dtype='int32', value=0.):\n",
        "    '''\n",
        "    Partially borrowed from Keras\n",
        "    # Arguments\n",
        "        sequences: list of lists where each element is a sequence\n",
        "        maxlen: int, maximum length\n",
        "        dtype: type to cast the resulting sequence.\n",
        "        value: float, value to pad the sequences to the desired value.\n",
        "    # Returns\n",
        "        x: numpy array with dimensions (number_of_sequences, maxlen)\n",
        "    '''\n",
        "    lengths = [len(s) for s in sequences]\n",
        "    nb_samples = len(sequences)\n",
        "    if maxlen is None:\n",
        "        maxlen = np.max(lengths)\n",
        "    # take the sample shape from the first non empty sequence\n",
        "    # checking for consistency in the main loop below.\n",
        "    sample_shape = tuple()\n",
        "    for s in sequences:\n",
        "        if len(s) > 0:\n",
        "            sample_shape = np.asarray(s).shape[1:]\n",
        "            break\n",
        "    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
        "    for idx, s in enumerate(sequences):\n",
        "        if len(s) == 0:\n",
        "            continue  # empty list was found\n",
        "        # pre truncating\n",
        "        trunc = s[-maxlen:]\n",
        "        # check `trunc` has expected shape\n",
        "        trunc = np.asarray(trunc, dtype=dtype)\n",
        "        if trunc.shape[1:] != sample_shape:\n",
        "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
        "                             (trunc.shape[1:], idx, sample_shape))\n",
        "        # post padding\n",
        "        x[idx, :len(trunc)] = trunc\n",
        "    return x"
      ],
      "metadata": {
        "id": "p3sUHa7klfta"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The action set used in Part 2 is as follows:"
      ],
      "metadata": {
        "id": "BkVHzerAk4Y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "action_set = {\n",
        "  'look around': 0,\n",
        "  'close door to west': 1,\n",
        "  'close door to east': 2,\n",
        "  'close door to south': 3,\n",
        "  'close door to north': 4,\n",
        "  'move west': 5,\n",
        "  'move east': 6,\n",
        "  'move south': 7,\n",
        "  'move north': 8,\n",
        "  'open door to west': 9,\n",
        "  'open door to east': 10,\n",
        "  'open door to south': 11,\n",
        "  'open door to north': 12,\n",
        "  'inventory': 13,\n",
        "  'take coin': 14,\n",
        "  'read map': 15,\n",
        "  'put map in box': 16,\n",
        "  'task': 17,\n",
        "  'take map': 18,\n",
        "  'put coin in box': 19\n",
        "}"
      ],
      "metadata": {
        "id": "3LZ2EKgWkn_X"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Part 2, we need to encode textual states, so we can use some tricks to shorten the state's length. We recommend using this `obs_with_inventory` function for this assignment."
      ],
      "metadata": {
        "id": "Mn5CeI-VtYiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def obs_with_inventory(obs, inv):\n",
        "  # some tricks to reduce the length of state\n",
        "  if 'Your inventory is currently empty' in inv:\n",
        "    inv = 'Inventory: empty'\n",
        "\n",
        "  if '(maximum capacity is 2 items)' in inv:\n",
        "    inv = inv.replace(\"(maximum capacity is 2 items)\", \"\")\n",
        "\n",
        "  return obs + '\\n' + inv\n"
      ],
      "metadata": {
        "id": "D1ynrEpntZEZ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJzznPc_wswe"
      },
      "source": [
        "# Important Notes for this Assignment\n",
        "\n",
        "\n",
        "*   A successful episode from the MDP will give a reward of 1.0\n",
        "*   A partially successful episode from an MDP environment will give a reward of 0.5\n",
        "*   If you increase NUM_EPISODES too high, it will take too long in the autograder.\n",
        "*   We will be checking for hard coded values / outputs, so please don't take any shortcuts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1. Implement `DQN`\n",
        "\n",
        "In step 1, we decide how the network will encode observations and calculate Q-values. We define a class `DQN`, a neural network used for approximating the Q-function. This network estimates the expected future rewards for each possible action in a given state. Since neural networks in the DQN takes inputs in the form of tensor, we need to encode a state to obtain state representations. We adopt a simple RNN-based state network following the paper [Interactive Fiction Games: A Colossal Adventure](https://arxiv.org/pdf/1909.05398) to encode textual states from the Textworld-Express.\n",
        "\n",
        "The RNN and state network are already defined below. You need to complete `DQN` class, and optinally change the `StateNetwork` class if needed. Three classes are defined below are:\n",
        "- `PackedEncoderRNN`: This class is a recurrent neural network (RNN) to process sequential data, like text. You don't need to change this class in this assignment.\n",
        "- `StateNetwork` encodes the observations and inventory information from the TextWorld game, creating a compact representation of the game state. (Optional) Current implementation uses one RNN to encode the state. You can explore using more RNNs following [this](https://arxiv.org/pdf/1909.05398) paper to encode observation and inventory seperate and concatenate them as a final state representations.\n",
        "- `DQN`: The core of the deep Q-Network containing `stateNetwork`."
      ],
      "metadata": {
        "id": "2sGpjl4N4YP9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "3ejeYWxRrtiY"
      },
      "outputs": [],
      "source": [
        "class PackedEncoderRNN(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(PackedEncoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "  def forward(self, input, hidden=None):\n",
        "    embedded = self.embedding(input).permute(1, 0, 2) # T x Batch x EmbDim\n",
        "    if hidden is None:\n",
        "        hidden = self.initHidden(input.size(0))\n",
        "\n",
        "    # Pack the padded batch of sequences\n",
        "    lengths = torch.tensor([torch.nonzero(n)[-1] + 1 for n in input], dtype=torch.long).cpu()\n",
        "    packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, enforce_sorted=False)\n",
        "    output, hidden = self.gru(packed, hidden)\n",
        "\n",
        "    # Unpack the padded sequence\n",
        "    output, _ = nn.utils.rnn.pad_packed_sequence(output)\n",
        "\n",
        "    # Return only the last timestep of output for each sequence\n",
        "    lengths = lengths.cuda()\n",
        "    idx = (lengths-1).view(-1, 1).expand(len(lengths), output.size(2)).unsqueeze(0)\n",
        "    output = output.gather(0, idx).squeeze(0)\n",
        "    return output, hidden\n",
        "\n",
        "  def initHidden(self, batch_size):\n",
        "    return torch.zeros(1, batch_size, self.hidden_size).cuda()\n",
        "\n",
        "\n",
        "class StateNetwork(nn.Module):\n",
        "  \"\"\"\n",
        "    No need to change, but feel free to improve if needed.\n",
        "  \"\"\"\n",
        "  def __init__(self, config):\n",
        "    super(StateNetwork, self).__init__()\n",
        "    self.config = config\n",
        "    self.enc_state = PackedEncoderRNN(config.vocab_size, config.hidden_size)\n",
        "    self.fcx = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "    self.fch = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    batch_size = inputs.shape[0]\n",
        "    x_o, h_o = self.enc_state(inputs, self.enc_state.initHidden(batch_size))\n",
        "\n",
        "    x = F.relu(self.fcx(x_o))\n",
        "    h = F.relu(self.fch(h_o))\n",
        "\n",
        "    return x, h\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super(DQN, self).__init__()\n",
        "    self.state_network = StateNetwork(config)\n",
        "    self.act_scorer = nn.Linear(config.hidden_size, config.act_size)\n",
        "\n",
        "  def forward(self, state):\n",
        "    \"\"\"\n",
        "      the output should be (BATCH_SIZE, ACTION_SIZE): the estimated Q-values for each action\n",
        "    \"\"\"\n",
        "    # raise NotImplementedError\n",
        "    x, _ = self.state_network(state)\n",
        "    return self.act_scorer(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your DQN with simple actions. Note that this is mainly for sanity check for RNN implementation, and does not guarantee that your implementation is correct. You will test your implementation after Step 2 and Step 3 on the actual environment."
      ],
      "metadata": {
        "id": "Gt3zgQxTX0x0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test DQN\n",
        "\n",
        "class DQNConfig:\n",
        "  vocab_size = 50257\n",
        "  act_size = len(action_set)\n",
        "  embedding_size = 64\n",
        "  hidden_size = 256\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "config = DQNConfig()\n",
        "dqn = DQN(config).to(device)\n",
        "x = torch.tensor([0, 1, 2, 3, 4, 5]).to(device).unsqueeze(0)\n",
        "print(\"****** Q-values ****** (not trained)\")\n",
        "q_values = dqn(x)\n",
        "for act, actid in action_set.items():\n",
        "  print(f\"{act:20}: {q_values[0][actid]:0.6f}\")"
      ],
      "metadata": {
        "id": "FoRXWfrsXx3p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3beb5f4-471a-4d0d-a438-5f2268b762da"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****** Q-values ****** (not trained)\n",
            "look around         : -0.041797\n",
            "close door to west  : 0.027684\n",
            "close door to east  : -0.017741\n",
            "close door to south : -0.024807\n",
            "close door to north : -0.047225\n",
            "move west           : -0.074349\n",
            "move east           : 0.101021\n",
            "move south          : -0.009119\n",
            "move north          : 0.101794\n",
            "open door to west   : 0.046262\n",
            "open door to east   : -0.048911\n",
            "open door to south  : -0.050215\n",
            "open door to north  : 0.084541\n",
            "inventory           : -0.124426\n",
            "take coin           : -0.112793\n",
            "read map            : 0.014005\n",
            "put map in box      : -0.007356\n",
            "task                : 0.010428\n",
            "take map            : -0.097566\n",
            "put coin in box     : 0.011432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhgPPbfb9ANG"
      },
      "source": [
        "# Step 2: Implement `DQNAgent`\n",
        "\n",
        "`DQNAgent` encapsulates the `DQN` model and the main agent's logic such as interacting with environemnts, storing experience into replay buffer, estimating Q-values using `DQN`. `DQNAgent` initializes the agent's hyperparameters, including the state and action sizes, learning rate, discount factor (gamma), exploration rate (epsilon), and others which are necessary for q-learning algorithm as well.\n",
        "\n",
        "Make `train` function the starting point of all the training procedure. In the test time, we use following code to run your agent\n",
        "```\n",
        "agent = DQNAgent(action_set,\n",
        "                 learning_rate=LEARNING_RATE,\n",
        "                 gamma=GAMMA,\n",
        "                 epsilon=EPSILON,\n",
        "                 DQNConfig())\n",
        "\n",
        "agent.train(ENV, NUM_EPISODES, THRESHOLD)\n",
        "```\n",
        "\n",
        "The `train` function takes the following parameters:\n",
        "\n",
        "- env: a pointer to the environment (`ENV`).\n",
        "- num_episodes: the number of episodes to run before termination of the entire algorithm.\n",
        "- threshold: the number of steps in an episode before terminating a single episode.\n",
        "\n",
        "Except `train` function, all implementaions are on your own design. Here are some tips for Step 2 (also feel free to explore your own way):\n",
        "- use epsilon decay technique\n",
        "- use 1000 for the period to update the target network.\n",
        "- Do not update the Q-network every step. Updating the entwork every 4 step would be enough.\n",
        "\n",
        "Note that your function will interact with the environment through `reset_mdp()` and `do_action_mdp()`. Be sure to reset the environment before running, and terminate the episode if `do_action_mdp()` indicates the episode has terminated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "R02kCLNnrvUa"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "  def __init__(self,\n",
        "               action_set,\n",
        "               dqn_config,\n",
        "               learning_rate=0.0005,\n",
        "               gamma=0.9,\n",
        "               epsilon=1.0,\n",
        "               epsilon_decay=0.995,\n",
        "               epsilon_min=0.01,\n",
        "               batch_size=64,\n",
        "               memory_size=100000,\n",
        "               update_freq=4,\n",
        "               update_freq_target=1000):\n",
        "    self.act2id = {a: i for i, a in enumerate(action_set)}\n",
        "    self.id2act = {i: a for i, a in enumerate(action_set)}\n",
        "\n",
        "    self.update_freq = update_freq\n",
        "    self.update_freq_target = update_freq_target\n",
        "    self.max_seq_len = 256 # DO NOT CHANGE `max_seq_len`\n",
        "    self.tokenizer =  AutoTokenizer.from_pretrained('gpt2')\n",
        "\n",
        "    self.action_size = len(action_set)\n",
        "    self.learning_rate = learning_rate\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.epsilon_decay = epsilon_decay\n",
        "    self.epsilon_min = epsilon_min\n",
        "    self.batch_size = batch_size\n",
        "    self.replay_buffer = deque(maxlen=memory_size)\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.model = DQN(dqn_config).to(self.device)\n",
        "    self.target_model = DQN(dqn_config).to(self.device)\n",
        "    self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "\n",
        "  def tokenize_and_pad_states(self, states):\n",
        "    \"\"\"\n",
        "    Tokenizes and pads a list of textual states.\n",
        "    Uses the pre-trained tokenizer to convert textual descriptions of\n",
        "    the environment into numerical representations (tokens) and then pads\n",
        "    the sequences to a uniform length.\n",
        "\n",
        "    Args:\n",
        "        states: A list of string representations of the environment state.\n",
        "\n",
        "    Returns:\n",
        "        A padded NumPy array of tokenized states.\n",
        "    \"\"\"\n",
        "    input_ids = self.tokenizer(states)['input_ids']\n",
        "    return pad_sequences(input_ids, maxlen=self.max_seq_len)\n",
        "\n",
        "  def remember(self, state, action, reward, next_state, done):\n",
        "    \"\"\"Stores experiences in the agent's replay buffer.\"\"\"\n",
        "    self.replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def act(self, state, valid_actions=[]) -> int:\n",
        "    \"\"\"Selects an action using an epsilon-greedy policy.\n",
        "    The agent balances exploration (choosing random actions) and exploitation\n",
        "    (choosing the action with the highest estimated Q-value) using epsilon.\n",
        "\n",
        "    Args:\n",
        "        state (str): The current state of the environment.\n",
        "        valid_actions (list): A list of valid actions in the current state.\n",
        "\n",
        "    Returns:\n",
        "        int: The ID of the selected action.\n",
        "    \"\"\"\n",
        "    if random.uniform(0, 1) < self.epsilon:\n",
        "      return random.choice([self.act2id[a] for a in valid_actions])\n",
        "    else:\n",
        "      if not isinstance(state, list):\n",
        "        state = [state]\n",
        "\n",
        "      state_tokens = agent.tokenize_and_pad_states(state)\n",
        "      state = torch.tensor(state_tokens, dtype=torch.int64, device=self.device)\n",
        "      with torch.no_grad():\n",
        "        q_values = self.model(state)\n",
        "      return q_values.argmax().item()\n",
        "\n",
        "  def get_action_str(self, act_id):\n",
        "    return self.id2act[act_id]\n",
        "\n",
        "  def replay(self):\n",
        "    \"\"\"Performs experience replay and updates the model's parameters.\"\"\"\n",
        "\n",
        "    # Check if enough experiences are available\n",
        "    if len(self.replay_buffer) < self.batch_size:\n",
        "      return\n",
        "\n",
        "    minibatch = random.sample(self.replay_buffer, self.batch_size)\n",
        "    states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "\n",
        "    states = self.tokenize_and_pad_states(states)\n",
        "    next_states = self.tokenize_and_pad_states(next_states)\n",
        "\n",
        "    actions_tensor = torch.tensor(list(actions), dtype=torch.int64, device=self.device).unsqueeze(1)\n",
        "    rewards_tensor = torch.tensor(list(rewards), dtype=torch.float32, device=self.device).unsqueeze(1)\n",
        "    dones_tensor = torch.tensor(list(dones), dtype=torch.bool, device=self.device).unsqueeze(1)\n",
        "\n",
        "    # Compute Q-values for current states\n",
        "    states = torch.tensor(np.array(np.stack(states)), dtype=torch.int32, device=self.device)\n",
        "    states = states.squeeze(1)\n",
        "    q_values = self.model(states).gather(1, actions_tensor)\n",
        "\n",
        "    # Compute target Q-values using target network\n",
        "    with torch.no_grad():\n",
        "      next_states = torch.tensor(np.array(np.stack(next_states)),  dtype=torch.int32, device=self.device)\n",
        "      next_states = next_states.squeeze(1)\n",
        "      next_q_values = self.target_model(next_states).max(1, keepdim=True)[0]  # Get max Q-values for next states\n",
        "      target_q_values = rewards_tensor + (self.gamma * next_q_values * ~dones_tensor)  # Bellman equation\n",
        "\n",
        "    loss = F.mse_loss(q_values, target_q_values)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(self.model.parameters(), 40)\n",
        "    self.optimizer.step()\n",
        "\n",
        "  def train(self, env, num_episodes, threshold):\n",
        "    \"\"\"Trains the DQN agent in the given environment.\n",
        "\n",
        "    Args:\n",
        "      env: The environment to train the agent in.\n",
        "      agent: The DQN agent to train.\n",
        "      num_episodes: The number of episodes to train for.\n",
        "      threshold: The maximum number of steps to take in each episode.\n",
        "\n",
        "    Returns:\n",
        "      A tuple containing:\n",
        "        - A list of rewards obtained in each episode.\n",
        "        - The trained DQN agent.\n",
        "    \"\"\"\n",
        "    all_rewards = []  # Store rewards for each episode\n",
        "    total_step = 0\n",
        "    for episode in range(num_episodes):\n",
        "      state_info = reset_mdp(env)\n",
        "      state = obs_with_inventory(state_info['observation'], state_info['inventory'])\n",
        "\n",
        "      total_reward = 0\n",
        "      for step in range(threshold):\n",
        "        total_step += 1\n",
        "        valid_actions = state_info['valid actions']\n",
        "\n",
        "        action_id = self.act(state, valid_actions)  # Choose action using epsilon-greedy policy\n",
        "        action_str = self.get_action_str(action_id)\n",
        "\n",
        "        next_obs, reward, done, next_state_info = do_action_mdp(action_str, env)\n",
        "        next_state = obs_with_inventory(next_state_info['observation'], next_state_info['inventory'])\n",
        "\n",
        "        self.remember(state, action_id, reward, next_state, done) # Store experience in replay buffer\n",
        "\n",
        "        if total_step % self.update_freq == 0:\n",
        "          self.replay()\n",
        "\n",
        "        # Update the target model periodically\n",
        "        if total_step % self.update_freq_target == 0:\n",
        "          self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "          break\n",
        "\n",
        "        state = next_state\n",
        "        state_info = next_state_info\n",
        "\n",
        "      all_rewards.append(total_reward)\n",
        "\n",
        "      # Print episode info\n",
        "      print(f\"Episode: {episode + 1}/{num_episodes}, Total Reward: {total_reward}, Epsilon: {self.epsilon:.4f}, Total Step: {step}\")\n",
        "\n",
        "      # Decay epsilon\n",
        "      self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
        "\n",
        "    return all_rewards\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js7x9FBAr_0e"
      },
      "source": [
        "Set the parameters for your DQNAgent. DQNAgent requires additional parameters for training neural networks such as learning rate, batch_size, optimizer, etc.\n",
        "\n",
        "**As same as Part 1, the hyperparameters: NUM_EPISODES, THRESHOLD, GAMMA, EPSILON These might need to be changed from the default values. These variables are just for the simple test below. The autograder will use the variables you set in `set_parameters` below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "fvCsvElH0TSD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ade0f6ba1195472fb29f9d4141832cde",
            "f2274c8560d340a1ae38e85f25817c90",
            "01d2bdd36f814e3fb134a3e9448c85ad",
            "8a9983fac2884ddb9507bfa3065cf12c",
            "dd4fbf4066374253ad16143f96a1b1f6",
            "7bd0c526ec5548ef94c53abdedd8fa95",
            "f78d1ce3a6da4c71a515616f4e448b6a",
            "9176817e2ea84b62979d94147e480530",
            "ef3c33a2a74246999f326a8331208ec8",
            "0e06cc9e741f4d3a817ac368d30b19ff",
            "f943b84dce384188ac572eab0a24967f",
            "c1e26d30e6fe4b3f8eacce4b2e499363",
            "8c6ff33b38d94cb48a79d37671741eca",
            "63e00825f6e34fbba86863fd162bf93a",
            "9967e32e6143496f8893e4a8941a2043",
            "d9da8f9cd4ba4eab91778d86a309baa6",
            "3d4cd11d435f47c09ee3b560fc53044f",
            "6cb62602bbff41c9a0a9a0e2ac7fde3d",
            "acb90a41322e4018a19e00cc2e97f236",
            "c364fb971bb148b89489b1dd4858e6a9",
            "3acbd604c6b3487eae42ffb73da068e9",
            "490f64b6f88540c69b52187946a6fc93",
            "d5ba1c8986ff4ef593201abe949df2d2",
            "ba9e8fc74a8f46ce87b1512c1e7c1546",
            "14bca33ada594a618759ee84c1ff3494",
            "70c03d55ad6e42b2ad28bd4e94d6c864",
            "42a76986c2114f0285f1d113cfaef9e8",
            "9b050daab9124a86be8855f82b651a31",
            "7798dbd0a7404904a11b0199ce3b8ed0",
            "100bb8dbe1a441e9adf1e81013b7ed5c",
            "d23744705b6c4a4986ae9a0051ffa767",
            "bf879b0dcb684ba68cf666aee89c9ccb",
            "0eb6a7c0ba214a8ea69091c6b561c5f2",
            "2a72ce34d14e49e9942b611381134e24",
            "47c8e12c443c4a6a947b4b7e8bccb8a6",
            "8d73812e12354b6fb8d9b9ad928c8d86",
            "28813c105e7946b9b4fe9ae500b39023",
            "a95d2510d9ad4df683fbfb22e5b4dde0",
            "a1c710352c2740abb073ced77e824987",
            "99bf57b7e6714f9ba8ac3e67b0852a22",
            "1280b721212e463398dad9a9269ec2e4",
            "b6837c8777f244b8a5a48aa23960d0ed",
            "d20276010dd343df9a5498795131f57e",
            "125b82cd74184e2f9399afafd391c511",
            "eba7a4d7aa8e425b9cbe8e31c459483e",
            "bfe3ae9de1674d0baa92919aa8ff476c",
            "fc4f9c38b9a1488c9aa2d17829723785",
            "b01ce2a8b721466590e8aff1a3bfd10d",
            "4a5f68ab463643a085166a75405b5d44",
            "45e81bb07cf64455a91aaebb9b592cb8",
            "f14b30395edd4bf0802d9c9a6fb7baad",
            "29a68ad51db7437cb1e708f82597ba4b",
            "e1d5a481148b40e59c1aeb859e6bf5ce",
            "fe936253ad4c4b049fe050ae6f7e62ef",
            "0901f8bc04c747a0841b5108f0a01da0"
          ]
        },
        "outputId": "81343d88-dc74-4331-c9ed-50ba99952e81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ade0f6ba1195472fb29f9d4141832cde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1e26d30e6fe4b3f8eacce4b2e499363"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5ba1c8986ff4ef593201abe949df2d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a72ce34d14e49e9942b611381134e24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eba7a4d7aa8e425b9cbe8e31c459483e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1/100, Total Reward: 1.0, Epsilon: 1.0000, Total Step: 41\n",
            "Episode: 2/100, Total Reward: 0.0, Epsilon: 0.9950, Total Step: 49\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 1.0, Epsilon: 0.9851, Total Step: 37\n",
            "Episode: 5/100, Total Reward: 0.0, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 1.0, Epsilon: 0.9752, Total Step: 5\n",
            "Episode: 7/100, Total Reward: 0.0, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 0.0, Epsilon: 0.9655, Total Step: 49\n",
            "Episode: 9/100, Total Reward: 0.0, Epsilon: 0.9607, Total Step: 49\n",
            "Episode: 10/100, Total Reward: 0.0, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 0.0, Epsilon: 0.9511, Total Step: 49\n",
            "Episode: 12/100, Total Reward: 0.0, Epsilon: 0.9464, Total Step: 49\n",
            "Episode: 13/100, Total Reward: 0.0, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 0.0, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 1.0, Epsilon: 0.9322, Total Step: 21\n",
            "Episode: 16/100, Total Reward: 0.0, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 0.0, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 1.0, Epsilon: 0.9183, Total Step: 38\n",
            "Episode: 19/100, Total Reward: 0.0, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 1.0, Epsilon: 0.9092, Total Step: 25\n",
            "Episode: 21/100, Total Reward: 0.0, Epsilon: 0.9046, Total Step: 49\n",
            "Episode: 22/100, Total Reward: 0.0, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 0.0, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 1.0, Epsilon: 0.8911, Total Step: 32\n",
            "Episode: 25/100, Total Reward: 0.0, Epsilon: 0.8867, Total Step: 49\n",
            "Episode: 26/100, Total Reward: 1.0, Epsilon: 0.8822, Total Step: 17\n",
            "Episode: 27/100, Total Reward: 1.0, Epsilon: 0.8778, Total Step: 29\n",
            "Episode: 28/100, Total Reward: 0.0, Epsilon: 0.8734, Total Step: 49\n",
            "Episode: 29/100, Total Reward: 0.0, Epsilon: 0.8691, Total Step: 49\n",
            "Episode: 30/100, Total Reward: 0.0, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 0.0, Epsilon: 0.8604, Total Step: 49\n",
            "Episode: 32/100, Total Reward: 1.0, Epsilon: 0.8561, Total Step: 18\n",
            "Episode: 33/100, Total Reward: 0.0, Epsilon: 0.8518, Total Step: 49\n",
            "Episode: 34/100, Total Reward: 0.0, Epsilon: 0.8475, Total Step: 49\n",
            "Episode: 35/100, Total Reward: 0.0, Epsilon: 0.8433, Total Step: 49\n",
            "Episode: 36/100, Total Reward: 0.0, Epsilon: 0.8391, Total Step: 49\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 17\n",
            "Episode: 38/100, Total Reward: 1.0, Epsilon: 0.8307, Total Step: 46\n",
            "Episode: 39/100, Total Reward: 0.0, Epsilon: 0.8266, Total Step: 49\n",
            "Episode: 40/100, Total Reward: 1.0, Epsilon: 0.8224, Total Step: 35\n",
            "Episode: 41/100, Total Reward: 1.0, Epsilon: 0.8183, Total Step: 7\n",
            "Episode: 42/100, Total Reward: 0.0, Epsilon: 0.8142, Total Step: 49\n",
            "Episode: 43/100, Total Reward: 1.0, Epsilon: 0.8102, Total Step: 13\n",
            "Episode: 44/100, Total Reward: 0.0, Epsilon: 0.8061, Total Step: 49\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 24\n",
            "Episode: 46/100, Total Reward: 0.0, Epsilon: 0.7981, Total Step: 49\n",
            "Episode: 47/100, Total Reward: 1.0, Epsilon: 0.7941, Total Step: 10\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 17\n",
            "Episode: 49/100, Total Reward: 0.0, Epsilon: 0.7862, Total Step: 49\n",
            "Episode: 50/100, Total Reward: 1.0, Epsilon: 0.7822, Total Step: 49\n",
            "Episode: 51/100, Total Reward: 1.0, Epsilon: 0.7783, Total Step: 22\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 31\n",
            "Episode: 53/100, Total Reward: 1.0, Epsilon: 0.7705, Total Step: 19\n",
            "Episode: 54/100, Total Reward: 0.0, Epsilon: 0.7667, Total Step: 49\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 29\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 21\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 8\n",
            "Episode: 58/100, Total Reward: 1.0, Epsilon: 0.7515, Total Step: 21\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 13\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 16\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 8\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 16\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 11\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 12\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 17\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 8\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 9\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 46\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 7\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 7\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 31\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 5\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 9\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 11\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 10\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 7\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 10\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 8\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 9\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 44\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 19\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 27\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 14\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 15\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 29\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 7\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 12\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 23\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 12\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 16\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 6\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 10\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 5\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 12\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 9\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 13\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 6\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 11\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 6\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 9\n",
            "[1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
          ]
        }
      ],
      "source": [
        "# set parameters\n",
        "class DQNConfig:\n",
        "  vocab_size = 50257 # vocab_size = tokenizer.vocab_size\n",
        "  act_size = len(action_set)\n",
        "  embedding_size = 64\n",
        "  hidden_size = 256\n",
        "\n",
        "NUM_EPISODES = 100\n",
        "THRESHOLD = 50\n",
        "LEARNING_RATE = 0.0005\n",
        "GAMMA = 0.9\n",
        "EPSILON = 1.0\n",
        "\n",
        "# Create the DQN agent\n",
        "agent = DQNAgent(\n",
        "    action_set,\n",
        "    DQNConfig(),\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    gamma=GAMMA,\n",
        "    epsilon=EPSILON,)\n",
        "\n",
        "all_rewards = agent.train(ENV, NUM_EPISODES, THRESHOLD)\n",
        "print(all_rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z2k4EkrGaMm"
      },
      "source": [
        "# Step 3. Implement Code to Run a Policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j5OrjrTtxWM"
      },
      "source": [
        "Step 3-1: Implement code to run the policy. This function takes the following parameters:\n",
        "- agent: your DQNAgent, as specified in step 1.\n",
        "- env: pointer to the environment (e.g., `ENV`).\n",
        "- threshold: the maximum number of steps to take before terminating.\n",
        "\n",
        "Your function should run a single episode from the initial state and return:\n",
        "- A list of actions taken during the episode (e.g., `[act_1, act_2, ... act_n]`).\n",
        "- The total sum reward of all actions taken as a float.\n",
        "\n",
        "If you finihsed Part 1 correctly, this will be a simple adjustment to work with `DQNAgent`. Make sure your agent use **greedy action selection**, choosing the action with the maximum Q values from the `DQN`. You may implement some function to run policy in the `DQNAgent` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "7jJCxMBTGdv9"
      },
      "outputs": [],
      "source": [
        "# export\n",
        "def run_policy(agent, env, threshold=50):\n",
        "  \"\"\"Runs the policy learned by the agent in the given environment.\n",
        "\n",
        "  Args:\n",
        "    agent: The agent whose policy to run.\n",
        "    env: The environment to run the policy in.\n",
        "    threshold: The maximum number of steps to take before terminating the episode.\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing:\n",
        "      - A list of actions taken during the episode.\n",
        "      - The total reward obtained during the episode.\n",
        "  \"\"\"\n",
        "  # Use greedy action selection\n",
        "  agent.epsilon = 0.0\n",
        "\n",
        "  actions = [] # Store the entire sequence of actions here\n",
        "  total_reward = 0.0 # Store the total sum reward of all actions executed here\n",
        "  ### YOUR CODE BELOW HERE\n",
        "  #raise NotImplementedError\n",
        "  ### YOUR CODE ABOVE HERE\n",
        "  state_info = reset_mdp(env)\n",
        "  state = obs_with_inventory(state_info['observation'], state_info['inventory'])\n",
        "\n",
        "  for _ in range(threshold):\n",
        "    action_id = agent.act(state)  # Choose action using epsilon-greedy policy\n",
        "    action_str = agent.get_action_str(action_id)\n",
        "\n",
        "    actions.append(action_str)\n",
        "\n",
        "    # Take action and observe next state and reward\n",
        "    next_obs, reward, done, next_state_info = do_action_mdp(action_str, env)\n",
        "    next_state = obs_with_inventory(next_state_info['observation'], next_state_info['inventory'])\n",
        "\n",
        "    total_reward += reward\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "    state = next_state\n",
        "    state_info = next_state_info\n",
        "  return actions, total_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goBOBYvBYzeR"
      },
      "source": [
        "Step 3-2: Test your `run_policy` function.\n",
        "\n",
        "Set the threshold value for episode length during policy execution (test time threshold)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "KpK_2wuMuZ7d"
      },
      "outputs": [],
      "source": [
        "# export\n",
        "TEST_THRESHOLD = 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWOGP5aWY59n"
      },
      "source": [
        "Run the policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "vhrGyDGBGrMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44378508-2378-4232-b593-38f1d584f7b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "plan: ['open door to south', 'move south', 'open door to east', 'move east', 'take coin']\n",
            "Total reward: 1.0\n"
          ]
        }
      ],
      "source": [
        "plan, total_reward = run_policy(agent, ENV, threshold = TEST_THRESHOLD)\n",
        "print(\"plan:\", plan)\n",
        "print(\"Total reward:\", total_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cBfmUDG1XHk"
      },
      "source": [
        "# New Environments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLeYTiUVY9Tj"
      },
      "source": [
        "The following cells are the same as in Part 1: creating new environemnts: `StochasticTextWorldExpressEnv` and `PunishmentTextWorldExpressEnv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "n-hc6VnL793a"
      },
      "outputs": [],
      "source": [
        "NEVER_PICK_ACTIONS = set(['look around', 'inventory'])\n",
        "ENV_VERBOSE = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "GsQWZj0H1WeS"
      },
      "outputs": [],
      "source": [
        "class StochasticTextWorldExpressEnv(TextWorldExpressEnv):\n",
        "\n",
        "  def __init__(self, serverPath=None, envStepLimit=100, stochasticity = 0.0):\n",
        "    # Call the super constructor\n",
        "    super().__init__(serverPath, envStepLimit)\n",
        "    # Store the valid actions and stochasticity\n",
        "    self.valid_actions = []\n",
        "    self.stochasticity = stochasticity\n",
        "\n",
        "  def reset(self, seed=None, gameFold=None, gameName=None, gameParams=None, generateGoldPath=False):\n",
        "    # Call the super method\n",
        "    observation, infos = super().reset(seed, gameFold, gameName, gameParams, generateGoldPath)\n",
        "    # Update the valid actions\n",
        "    self.valid_actions = infos['validActions']\n",
        "    return observation, infos\n",
        "\n",
        "  def step(self, action:str):\n",
        "    # If a random value is less than the stochasticity target, choose a random action\n",
        "    if random.random() < self.stochasticity:\n",
        "      temp_valids = copy.deepcopy(self.valid_actions)\n",
        "      # Remove inventory and look around from valid actions to choose from\n",
        "      temp_valids = list(set(self.valid_actions).difference(NEVER_PICK_ACTIONS))\n",
        "      # Pick a random action from whatever remains\n",
        "      action = random.choice(temp_valids)\n",
        "    # If debugging flag is on, print the action that will be executed\n",
        "    if ENV_VERBOSE:\n",
        "      print(\"[[action]]:\", action)\n",
        "    # Call the super class with either the action passed in or the randomly chosen one\n",
        "    observation, reward, isCompleted, infos = super().step(action)\n",
        "    # Update the valid actions\n",
        "    self.valid_actions = infos['validActions']\n",
        "    return observation, reward, isCompleted, infos\n",
        "\n",
        "class PunishmentTextWorldExpressEnv(TextWorldExpressEnv):\n",
        "\n",
        "  def __init__(self, serverPath=None, envStepLimit=100, punishment = 0.0):\n",
        "    # Call the super constructor\n",
        "    super().__init__(serverPath, envStepLimit)\n",
        "    # Store the punishment\n",
        "    self.punishment = punishment\n",
        "    # Store the previous observation\n",
        "    self.previous_observation = None\n",
        "\n",
        "  def step(self, action:str):\n",
        "    # Call the super method\n",
        "    observation, reward, isCompleted, infos = super().step(action)\n",
        "    # If the current look is the same as the previous look, then we have performed an illegal action\n",
        "    if infos['look'] == self.previous_observation:\n",
        "      reward = self.punishment\n",
        "    # Store the previous observation\n",
        "    self.previous_observation = infos['look']\n",
        "    return observation, reward, isCompleted, infos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKV4lUC1agso"
      },
      "source": [
        "New environments must be registered through the Gymnasium API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "OFWWPDS35bMn"
      },
      "outputs": [],
      "source": [
        "# register new environment\n",
        "gymnasium.register(id='TextWorldExpress-StochasticTextWorldExpressEnv-v0',\n",
        "                   entry_point='__main__:StochasticTextWorldExpressEnv')\n",
        "gymnasium.register(id='TextWorldExpress-PunishmentTextWorldExpressEnv-v0',\n",
        "                   entry_point='__main__:PunishmentTextWorldExpressEnv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywMkXNMwQsgZ"
      },
      "source": [
        "# Testing Suite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR9ZHmgw8UFa"
      },
      "source": [
        "This function will run all environments, all game types, all game parameters, and all seeds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "OLy0jJ9gRA0L"
      },
      "outputs": [],
      "source": [
        "def run_all(environments, games, seeds):\n",
        "  global ENV, GAME_TYPE, GAME_PARAMS, SEED\n",
        "  # Results will contain a key (env type, game type, game params, seed) and values will be plans and total_rewards\n",
        "  results = {}\n",
        "  test_id = 0\n",
        "  total_reward = 0\n",
        "  # Iterate through all environments given\n",
        "  for env in environments:\n",
        "    # set global environment\n",
        "    ENV = env\n",
        "    # Iterate through all game types, the keys of the games dict\n",
        "    for game_type in games:\n",
        "      # Set the global game type\n",
        "      GAME_TYPE = game_type\n",
        "      # Iterate through all game parameters for the given game type in game dict\n",
        "      for params in games[game_type]:\n",
        "        # set the global game params\n",
        "        GAME_PARAMS = params\n",
        "        # load the environment\n",
        "        ENV.load(gameName=GAME_TYPE, gameParams=GAME_PARAMS)\n",
        "        # Iterate through all seeds\n",
        "        for seed in seeds:\n",
        "          print(f\"TESTING {type(ENV)}, {GAME_TYPE}, {GAME_PARAMS}, {seed}\")\n",
        "          # set the global seed\n",
        "          SEED = seed\n",
        "\n",
        "          # Run the DQNAgent and get the policy\n",
        "          agent = DQNAgent(action_set,\n",
        "                           DQNConfig(),\n",
        "                           learning_rate=LEARNING_RATE,\n",
        "                           gamma=GAMMA,\n",
        "                           epsilon=EPSILON)\n",
        "\n",
        "          _ = agent.train(ENV, NUM_EPISODES, THRESHOLD)\n",
        "\n",
        "          # run the policy to get the plan\n",
        "          plan, reward = run_policy(agent, ENV, threshold = TEST_THRESHOLD)\n",
        "\n",
        "          test_id += 1\n",
        "          total_reward += reward\n",
        "\n",
        "          print(f\"TESTING {test_id}: total_reward {total_reward}/{test_id} \\t (reward: {reward})\")\n",
        "          # Store the plan in the results\n",
        "          results[(type(ENV), GAME_TYPE, GAME_PARAMS, SEED)] = (plan, total_reward)\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "jeBUqaKWbBQj"
      },
      "outputs": [],
      "source": [
        "seeds = list(range(5))\n",
        "environments = [TextWorldExpressEnv(envStepLimit=100),\n",
        "                StochasticTextWorldExpressEnv(envStepLimit=100, stochasticity=0.25),\n",
        "                PunishmentTextWorldExpressEnv(envStepLimit=100, punishment=-1.0)]\n",
        "games = {'coin':      ['numLocations=5,includeDoors=1,numDistractorItems=0',\n",
        "                       'numLocations=6,includeDoors=1,numDistractorItems=0',\n",
        "                       'numLocations=7,includeDoors=1,numDistractorItems=0',\n",
        "                       'numLocations=10,includeDoors=1,numDistractorItems=0'],\n",
        "         'mapreader': ['numLocations=5,maxDistanceApart=3,includeDoors=0,maxDistractorItemsPerLocation=0',\n",
        "                       'numLocations=8,maxDistanceApart=4,includeDoors=0,maxDistractorItemsPerLocation=0',\n",
        "                       'numLocations=11,maxDistanceApart=5,includeDoors=0,maxDistractorItemsPerLocation=0',\n",
        "                       'numLocations=15,maxDistanceApart=8,includeDoors=0,maxDistractorItemsPerLocation=0']}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaZoBv5v8Qa9"
      },
      "source": [
        "Set parameters. Do not alter this cell outside of the changing the numeric values.\n",
        "\n",
        "**You might need to change these parameters to get a good result on the harder environments**\n",
        "\n",
        "Please note that increasing `NUM_EPISODES` will result in an increase in time to run the cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "-DaxveUb8PQv"
      },
      "outputs": [],
      "source": [
        "# export\n",
        "def set_parameters():\n",
        "    global NUM_EPISODES, THRESHOLD, LEARNING_RATE, GAMMA, EPSILON, TEST_THRESHOLD\n",
        "    NUM_EPISODES = 300\n",
        "    THRESHOLD = 50\n",
        "    LEARNING_RATE = 0.0005\n",
        "    GAMMA = 0.9\n",
        "    EPSILON = 1.0\n",
        "    TEST_THRESHOLD = 50\n",
        "    NUM_EPISODES = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "T9dAzWLerjKI"
      },
      "outputs": [],
      "source": [
        "# export\n",
        "set_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzB7i03p8dij"
      },
      "source": [
        "Run all tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Xxm2giWM8fcJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "99a85a3a-70a3-4a76-dcf7-09cfb5a401e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=5,includeDoors=1,numDistractorItems=0, 0\n",
            "Episode: 1/100, Total Reward: 1.0, Epsilon: 1.0000, Total Step: 11\n",
            "Episode: 2/100, Total Reward: 1.0, Epsilon: 0.9950, Total Step: 38\n",
            "Episode: 3/100, Total Reward: 1.0, Epsilon: 0.9900, Total Step: 0\n",
            "Episode: 4/100, Total Reward: 1.0, Epsilon: 0.9851, Total Step: 3\n",
            "Episode: 5/100, Total Reward: 1.0, Epsilon: 0.9801, Total Step: 6\n",
            "Episode: 6/100, Total Reward: 1.0, Epsilon: 0.9752, Total Step: 2\n",
            "Episode: 7/100, Total Reward: 1.0, Epsilon: 0.9704, Total Step: 13\n",
            "Episode: 8/100, Total Reward: 1.0, Epsilon: 0.9655, Total Step: 28\n",
            "Episode: 9/100, Total Reward: 1.0, Epsilon: 0.9607, Total Step: 24\n",
            "Episode: 10/100, Total Reward: 1.0, Epsilon: 0.9559, Total Step: 0\n",
            "Episode: 11/100, Total Reward: 1.0, Epsilon: 0.9511, Total Step: 1\n",
            "Episode: 12/100, Total Reward: 1.0, Epsilon: 0.9464, Total Step: 6\n",
            "Episode: 13/100, Total Reward: 1.0, Epsilon: 0.9416, Total Step: 0\n",
            "Episode: 14/100, Total Reward: 1.0, Epsilon: 0.9369, Total Step: 13\n",
            "Episode: 15/100, Total Reward: 1.0, Epsilon: 0.9322, Total Step: 0\n",
            "Episode: 16/100, Total Reward: 1.0, Epsilon: 0.9276, Total Step: 13\n",
            "Episode: 17/100, Total Reward: 1.0, Epsilon: 0.9229, Total Step: 8\n",
            "Episode: 18/100, Total Reward: 1.0, Epsilon: 0.9183, Total Step: 2\n",
            "Episode: 19/100, Total Reward: 1.0, Epsilon: 0.9137, Total Step: 10\n",
            "Episode: 20/100, Total Reward: 1.0, Epsilon: 0.9092, Total Step: 0\n",
            "Episode: 21/100, Total Reward: 1.0, Epsilon: 0.9046, Total Step: 0\n",
            "Episode: 22/100, Total Reward: 1.0, Epsilon: 0.9001, Total Step: 39\n",
            "Episode: 23/100, Total Reward: 1.0, Epsilon: 0.8956, Total Step: 1\n",
            "Episode: 24/100, Total Reward: 1.0, Epsilon: 0.8911, Total Step: 1\n",
            "Episode: 25/100, Total Reward: 1.0, Epsilon: 0.8867, Total Step: 3\n",
            "Episode: 26/100, Total Reward: 1.0, Epsilon: 0.8822, Total Step: 2\n",
            "Episode: 27/100, Total Reward: 1.0, Epsilon: 0.8778, Total Step: 15\n",
            "Episode: 28/100, Total Reward: 1.0, Epsilon: 0.8734, Total Step: 14\n",
            "Episode: 29/100, Total Reward: 1.0, Epsilon: 0.8691, Total Step: 0\n",
            "Episode: 30/100, Total Reward: 1.0, Epsilon: 0.8647, Total Step: 5\n",
            "Episode: 31/100, Total Reward: 1.0, Epsilon: 0.8604, Total Step: 0\n",
            "Episode: 32/100, Total Reward: 1.0, Epsilon: 0.8561, Total Step: 1\n",
            "Episode: 33/100, Total Reward: 1.0, Epsilon: 0.8518, Total Step: 1\n",
            "Episode: 34/100, Total Reward: 1.0, Epsilon: 0.8475, Total Step: 0\n",
            "Episode: 35/100, Total Reward: 1.0, Epsilon: 0.8433, Total Step: 2\n",
            "Episode: 36/100, Total Reward: 1.0, Epsilon: 0.8391, Total Step: 2\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 0\n",
            "Episode: 38/100, Total Reward: 1.0, Epsilon: 0.8307, Total Step: 6\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 1\n",
            "Episode: 40/100, Total Reward: 1.0, Epsilon: 0.8224, Total Step: 9\n",
            "Episode: 41/100, Total Reward: 1.0, Epsilon: 0.8183, Total Step: 2\n",
            "Episode: 42/100, Total Reward: 1.0, Epsilon: 0.8142, Total Step: 4\n",
            "Episode: 43/100, Total Reward: 1.0, Epsilon: 0.8102, Total Step: 1\n",
            "Episode: 44/100, Total Reward: 1.0, Epsilon: 0.8061, Total Step: 0\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 1\n",
            "Episode: 46/100, Total Reward: 1.0, Epsilon: 0.7981, Total Step: 1\n",
            "Episode: 47/100, Total Reward: 1.0, Epsilon: 0.7941, Total Step: 1\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 0\n",
            "Episode: 49/100, Total Reward: 1.0, Epsilon: 0.7862, Total Step: 36\n",
            "Episode: 50/100, Total Reward: 1.0, Epsilon: 0.7822, Total Step: 1\n",
            "Episode: 51/100, Total Reward: 1.0, Epsilon: 0.7783, Total Step: 26\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 1\n",
            "Episode: 53/100, Total Reward: 1.0, Epsilon: 0.7705, Total Step: 0\n",
            "Episode: 54/100, Total Reward: 1.0, Epsilon: 0.7667, Total Step: 1\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 2\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 1\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 5\n",
            "Episode: 58/100, Total Reward: 1.0, Epsilon: 0.7515, Total Step: 6\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 0\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 6\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 1\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 3\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 7\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 1\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 0\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 1\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 25\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 0\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 8\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 6\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 0\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 5\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 0\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 2\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 0\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 0\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 3\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 0\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 0\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 0\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 2\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 3\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 1\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 0\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 0\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 0\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 5\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 1\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 0\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 0\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 1\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 3\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 0\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 0\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 1\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 2\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 0\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 3\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 1\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 5\n",
            "TESTING 1: total_reward 1.0/1 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=5,includeDoors=1,numDistractorItems=0, 1\n",
            "Episode: 1/100, Total Reward: 1.0, Epsilon: 1.0000, Total Step: 24\n",
            "Episode: 2/100, Total Reward: 0.0, Epsilon: 0.9950, Total Step: 49\n",
            "Episode: 3/100, Total Reward: 1.0, Epsilon: 0.9900, Total Step: 11\n",
            "Episode: 4/100, Total Reward: 1.0, Epsilon: 0.9851, Total Step: 37\n",
            "Episode: 5/100, Total Reward: 0.0, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 0.0, Epsilon: 0.9752, Total Step: 49\n",
            "Episode: 7/100, Total Reward: 0.0, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 1.0, Epsilon: 0.9655, Total Step: 35\n",
            "Episode: 9/100, Total Reward: 1.0, Epsilon: 0.9607, Total Step: 24\n",
            "Episode: 10/100, Total Reward: 1.0, Epsilon: 0.9559, Total Step: 14\n",
            "Episode: 11/100, Total Reward: 1.0, Epsilon: 0.9511, Total Step: 31\n",
            "Episode: 12/100, Total Reward: 1.0, Epsilon: 0.9464, Total Step: 14\n",
            "Episode: 13/100, Total Reward: 1.0, Epsilon: 0.9416, Total Step: 20\n",
            "Episode: 14/100, Total Reward: 1.0, Epsilon: 0.9369, Total Step: 23\n",
            "Episode: 15/100, Total Reward: 1.0, Epsilon: 0.9322, Total Step: 7\n",
            "Episode: 16/100, Total Reward: 1.0, Epsilon: 0.9276, Total Step: 9\n",
            "Episode: 17/100, Total Reward: 0.0, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 1.0, Epsilon: 0.9183, Total Step: 14\n",
            "Episode: 19/100, Total Reward: 0.0, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 0.0, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 1.0, Epsilon: 0.9046, Total Step: 44\n",
            "Episode: 22/100, Total Reward: 1.0, Epsilon: 0.9001, Total Step: 19\n",
            "Episode: 23/100, Total Reward: 1.0, Epsilon: 0.8956, Total Step: 22\n",
            "Episode: 24/100, Total Reward: 1.0, Epsilon: 0.8911, Total Step: 5\n",
            "Episode: 25/100, Total Reward: 1.0, Epsilon: 0.8867, Total Step: 45\n",
            "Episode: 26/100, Total Reward: 1.0, Epsilon: 0.8822, Total Step: 17\n",
            "Episode: 27/100, Total Reward: 0.0, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 1.0, Epsilon: 0.8734, Total Step: 9\n",
            "Episode: 29/100, Total Reward: 0.0, Epsilon: 0.8691, Total Step: 49\n",
            "Episode: 30/100, Total Reward: 1.0, Epsilon: 0.8647, Total Step: 9\n",
            "Episode: 31/100, Total Reward: 1.0, Epsilon: 0.8604, Total Step: 43\n",
            "Episode: 32/100, Total Reward: 1.0, Epsilon: 0.8561, Total Step: 39\n",
            "Episode: 33/100, Total Reward: 1.0, Epsilon: 0.8518, Total Step: 21\n",
            "Episode: 34/100, Total Reward: 1.0, Epsilon: 0.8475, Total Step: 19\n",
            "Episode: 35/100, Total Reward: 1.0, Epsilon: 0.8433, Total Step: 45\n",
            "Episode: 36/100, Total Reward: 1.0, Epsilon: 0.8391, Total Step: 27\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 25\n",
            "Episode: 38/100, Total Reward: 1.0, Epsilon: 0.8307, Total Step: 14\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 18\n",
            "Episode: 40/100, Total Reward: 1.0, Epsilon: 0.8224, Total Step: 11\n",
            "Episode: 41/100, Total Reward: 0.0, Epsilon: 0.8183, Total Step: 49\n",
            "Episode: 42/100, Total Reward: 1.0, Epsilon: 0.8142, Total Step: 32\n",
            "Episode: 43/100, Total Reward: 1.0, Epsilon: 0.8102, Total Step: 13\n",
            "Episode: 44/100, Total Reward: 1.0, Epsilon: 0.8061, Total Step: 10\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 6\n",
            "Episode: 46/100, Total Reward: 1.0, Epsilon: 0.7981, Total Step: 7\n",
            "Episode: 47/100, Total Reward: 1.0, Epsilon: 0.7941, Total Step: 8\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 24\n",
            "Episode: 49/100, Total Reward: 1.0, Epsilon: 0.7862, Total Step: 26\n",
            "Episode: 50/100, Total Reward: 1.0, Epsilon: 0.7822, Total Step: 15\n",
            "Episode: 51/100, Total Reward: 1.0, Epsilon: 0.7783, Total Step: 11\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 8\n",
            "Episode: 53/100, Total Reward: 0.0, Epsilon: 0.7705, Total Step: 49\n",
            "Episode: 54/100, Total Reward: 1.0, Epsilon: 0.7667, Total Step: 3\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 18\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 42\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 12\n",
            "Episode: 58/100, Total Reward: 1.0, Epsilon: 0.7515, Total Step: 2\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 15\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 5\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 4\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 12\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 3\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 11\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 42\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 5\n",
            "Episode: 67/100, Total Reward: 0.0, Epsilon: 0.7183, Total Step: 49\n",
            "Episode: 68/100, Total Reward: 0.0, Epsilon: 0.7147, Total Step: 49\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 7\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 12\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 8\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 31\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 9\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 22\n",
            "Episode: 75/100, Total Reward: 0.0, Epsilon: 0.6901, Total Step: 49\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 7\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 4\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 19\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 21\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 4\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 44\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 13\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 13\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 4\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 49\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 49\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 5\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 9\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 28\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 3\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 4\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 10\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 8\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 4\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 7\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 2\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 3\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 5\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 9\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 2\n",
            "TESTING 2: total_reward 2.0/2 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=5,includeDoors=1,numDistractorItems=0, 2\n",
            "Episode: 1/100, Total Reward: 1.0, Epsilon: 1.0000, Total Step: 7\n",
            "Episode: 2/100, Total Reward: 1.0, Epsilon: 0.9950, Total Step: 1\n",
            "Episode: 3/100, Total Reward: 1.0, Epsilon: 0.9900, Total Step: 18\n",
            "Episode: 4/100, Total Reward: 1.0, Epsilon: 0.9851, Total Step: 2\n",
            "Episode: 5/100, Total Reward: 1.0, Epsilon: 0.9801, Total Step: 1\n",
            "Episode: 6/100, Total Reward: 1.0, Epsilon: 0.9752, Total Step: 4\n",
            "Episode: 7/100, Total Reward: 1.0, Epsilon: 0.9704, Total Step: 1\n",
            "Episode: 8/100, Total Reward: 1.0, Epsilon: 0.9655, Total Step: 1\n",
            "Episode: 9/100, Total Reward: 1.0, Epsilon: 0.9607, Total Step: 6\n",
            "Episode: 10/100, Total Reward: 1.0, Epsilon: 0.9559, Total Step: 7\n",
            "Episode: 11/100, Total Reward: 1.0, Epsilon: 0.9511, Total Step: 4\n",
            "Episode: 12/100, Total Reward: 0.0, Epsilon: 0.9464, Total Step: 49\n",
            "Episode: 13/100, Total Reward: 1.0, Epsilon: 0.9416, Total Step: 1\n",
            "Episode: 14/100, Total Reward: 1.0, Epsilon: 0.9369, Total Step: 7\n",
            "Episode: 15/100, Total Reward: 1.0, Epsilon: 0.9322, Total Step: 4\n",
            "Episode: 16/100, Total Reward: 1.0, Epsilon: 0.9276, Total Step: 0\n",
            "Episode: 17/100, Total Reward: 1.0, Epsilon: 0.9229, Total Step: 6\n",
            "Episode: 18/100, Total Reward: 1.0, Epsilon: 0.9183, Total Step: 0\n",
            "Episode: 19/100, Total Reward: 1.0, Epsilon: 0.9137, Total Step: 25\n",
            "Episode: 20/100, Total Reward: 1.0, Epsilon: 0.9092, Total Step: 1\n",
            "Episode: 21/100, Total Reward: 1.0, Epsilon: 0.9046, Total Step: 1\n",
            "Episode: 22/100, Total Reward: 1.0, Epsilon: 0.9001, Total Step: 2\n",
            "Episode: 23/100, Total Reward: 1.0, Epsilon: 0.8956, Total Step: 0\n",
            "Episode: 24/100, Total Reward: 1.0, Epsilon: 0.8911, Total Step: 11\n",
            "Episode: 25/100, Total Reward: 1.0, Epsilon: 0.8867, Total Step: 0\n",
            "Episode: 26/100, Total Reward: 1.0, Epsilon: 0.8822, Total Step: 6\n",
            "Episode: 27/100, Total Reward: 1.0, Epsilon: 0.8778, Total Step: 7\n",
            "Episode: 28/100, Total Reward: 1.0, Epsilon: 0.8734, Total Step: 1\n",
            "Episode: 29/100, Total Reward: 1.0, Epsilon: 0.8691, Total Step: 3\n",
            "Episode: 30/100, Total Reward: 1.0, Epsilon: 0.8647, Total Step: 1\n",
            "Episode: 31/100, Total Reward: 1.0, Epsilon: 0.8604, Total Step: 8\n",
            "Episode: 32/100, Total Reward: 1.0, Epsilon: 0.8561, Total Step: 0\n",
            "Episode: 33/100, Total Reward: 1.0, Epsilon: 0.8518, Total Step: 4\n",
            "Episode: 34/100, Total Reward: 1.0, Epsilon: 0.8475, Total Step: 3\n",
            "Episode: 35/100, Total Reward: 1.0, Epsilon: 0.8433, Total Step: 0\n",
            "Episode: 36/100, Total Reward: 1.0, Epsilon: 0.8391, Total Step: 18\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 13\n",
            "Episode: 38/100, Total Reward: 1.0, Epsilon: 0.8307, Total Step: 2\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 5\n",
            "Episode: 40/100, Total Reward: 1.0, Epsilon: 0.8224, Total Step: 2\n",
            "Episode: 41/100, Total Reward: 1.0, Epsilon: 0.8183, Total Step: 1\n",
            "Episode: 42/100, Total Reward: 1.0, Epsilon: 0.8142, Total Step: 1\n",
            "Episode: 43/100, Total Reward: 1.0, Epsilon: 0.8102, Total Step: 1\n",
            "Episode: 44/100, Total Reward: 1.0, Epsilon: 0.8061, Total Step: 0\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 0\n",
            "Episode: 46/100, Total Reward: 1.0, Epsilon: 0.7981, Total Step: 0\n",
            "Episode: 47/100, Total Reward: 1.0, Epsilon: 0.7941, Total Step: 3\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 1\n",
            "Episode: 49/100, Total Reward: 1.0, Epsilon: 0.7862, Total Step: 36\n",
            "Episode: 50/100, Total Reward: 1.0, Epsilon: 0.7822, Total Step: 2\n",
            "Episode: 51/100, Total Reward: 1.0, Epsilon: 0.7783, Total Step: 1\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 6\n",
            "Episode: 53/100, Total Reward: 1.0, Epsilon: 0.7705, Total Step: 1\n",
            "Episode: 54/100, Total Reward: 1.0, Epsilon: 0.7667, Total Step: 1\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 0\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 1\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 0\n",
            "Episode: 58/100, Total Reward: 1.0, Epsilon: 0.7515, Total Step: 2\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 3\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 12\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 10\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 1\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 0\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 4\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 37\n",
            "Episode: 66/100, Total Reward: 0.0, Epsilon: 0.7219, Total Step: 49\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 1\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 4\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 0\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 16\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 1\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 2\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 0\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 0\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 0\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 0\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 3\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 3\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 1\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 2\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 1\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 2\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 3\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 0\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 0\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 0\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 1\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 0\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 2\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 4\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 0\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 2\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 2\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 0\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 0\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 3\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 0\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 0\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 0\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 3\n",
            "TESTING 3: total_reward 3.0/3 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=5,includeDoors=1,numDistractorItems=0, 3\n",
            "Episode: 1/100, Total Reward: 1.0, Epsilon: 1.0000, Total Step: 14\n",
            "Episode: 2/100, Total Reward: 0.0, Epsilon: 0.9950, Total Step: 49\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 0.0, Epsilon: 0.9851, Total Step: 49\n",
            "Episode: 5/100, Total Reward: 0.0, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 0.0, Epsilon: 0.9752, Total Step: 49\n",
            "Episode: 7/100, Total Reward: 0.0, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 0.0, Epsilon: 0.9655, Total Step: 49\n",
            "Episode: 9/100, Total Reward: 0.0, Epsilon: 0.9607, Total Step: 49\n",
            "Episode: 10/100, Total Reward: 0.0, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 1.0, Epsilon: 0.9511, Total Step: 37\n",
            "Episode: 12/100, Total Reward: 0.0, Epsilon: 0.9464, Total Step: 49\n",
            "Episode: 13/100, Total Reward: 0.0, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 0.0, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 0.0, Epsilon: 0.9322, Total Step: 49\n",
            "Episode: 16/100, Total Reward: 0.0, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 0.0, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 0.0, Epsilon: 0.9183, Total Step: 49\n",
            "Episode: 19/100, Total Reward: 0.0, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 1.0, Epsilon: 0.9092, Total Step: 28\n",
            "Episode: 21/100, Total Reward: 0.0, Epsilon: 0.9046, Total Step: 49\n",
            "Episode: 22/100, Total Reward: 0.0, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 0.0, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 0.0, Epsilon: 0.8911, Total Step: 49\n",
            "Episode: 25/100, Total Reward: 0.0, Epsilon: 0.8867, Total Step: 49\n",
            "Episode: 26/100, Total Reward: 1.0, Epsilon: 0.8822, Total Step: 15\n",
            "Episode: 27/100, Total Reward: 0.0, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 0.0, Epsilon: 0.8734, Total Step: 49\n",
            "Episode: 29/100, Total Reward: 1.0, Epsilon: 0.8691, Total Step: 12\n",
            "Episode: 30/100, Total Reward: 0.0, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 1.0, Epsilon: 0.8604, Total Step: 26\n",
            "Episode: 32/100, Total Reward: 0.0, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 0.0, Epsilon: 0.8518, Total Step: 49\n",
            "Episode: 34/100, Total Reward: 1.0, Epsilon: 0.8475, Total Step: 33\n",
            "Episode: 35/100, Total Reward: 1.0, Epsilon: 0.8433, Total Step: 21\n",
            "Episode: 36/100, Total Reward: 0.0, Epsilon: 0.8391, Total Step: 49\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 35\n",
            "Episode: 38/100, Total Reward: 0.0, Epsilon: 0.8307, Total Step: 49\n",
            "Episode: 39/100, Total Reward: 0.0, Epsilon: 0.8266, Total Step: 49\n",
            "Episode: 40/100, Total Reward: 1.0, Epsilon: 0.8224, Total Step: 20\n",
            "Episode: 41/100, Total Reward: 1.0, Epsilon: 0.8183, Total Step: 42\n",
            "Episode: 42/100, Total Reward: 0.0, Epsilon: 0.8142, Total Step: 49\n",
            "Episode: 43/100, Total Reward: 0.0, Epsilon: 0.8102, Total Step: 49\n",
            "Episode: 44/100, Total Reward: 1.0, Epsilon: 0.8061, Total Step: 14\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 45\n",
            "Episode: 46/100, Total Reward: 0.0, Epsilon: 0.7981, Total Step: 49\n",
            "Episode: 47/100, Total Reward: 1.0, Epsilon: 0.7941, Total Step: 17\n",
            "Episode: 48/100, Total Reward: 0.0, Epsilon: 0.7901, Total Step: 49\n",
            "Episode: 49/100, Total Reward: 1.0, Epsilon: 0.7862, Total Step: 34\n",
            "Episode: 50/100, Total Reward: 1.0, Epsilon: 0.7822, Total Step: 30\n",
            "Episode: 51/100, Total Reward: 1.0, Epsilon: 0.7783, Total Step: 24\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 11\n",
            "Episode: 53/100, Total Reward: 0.0, Epsilon: 0.7705, Total Step: 49\n",
            "Episode: 54/100, Total Reward: 0.0, Epsilon: 0.7667, Total Step: 49\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 9\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 30\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 18\n",
            "Episode: 58/100, Total Reward: 1.0, Epsilon: 0.7515, Total Step: 46\n",
            "Episode: 59/100, Total Reward: 0.0, Epsilon: 0.7477, Total Step: 49\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 20\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 26\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 13\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 31\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 22\n",
            "Episode: 65/100, Total Reward: 0.0, Epsilon: 0.7256, Total Step: 49\n",
            "Episode: 66/100, Total Reward: 0.0, Epsilon: 0.7219, Total Step: 49\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 27\n",
            "Episode: 68/100, Total Reward: 0.0, Epsilon: 0.7147, Total Step: 49\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 42\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 31\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 39\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 24\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 30\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 14\n",
            "Episode: 75/100, Total Reward: 0.0, Epsilon: 0.6901, Total Step: 49\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 20\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 13\n",
            "Episode: 78/100, Total Reward: 0.0, Epsilon: 0.6798, Total Step: 49\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 48\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 13\n",
            "Episode: 81/100, Total Reward: 0.0, Epsilon: 0.6696, Total Step: 49\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 20\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 31\n",
            "Episode: 84/100, Total Reward: 0.0, Epsilon: 0.6597, Total Step: 49\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 41\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 15\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 8\n",
            "Episode: 88/100, Total Reward: 0.0, Epsilon: 0.6466, Total Step: 49\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 13\n",
            "Episode: 90/100, Total Reward: 0.0, Epsilon: 0.6401, Total Step: 49\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 31\n",
            "Episode: 92/100, Total Reward: 0.0, Epsilon: 0.6337, Total Step: 49\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 13\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 18\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 7\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 5\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 24\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 44\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 9\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 12\n",
            "TESTING 4: total_reward 4.0/4 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=5,includeDoors=1,numDistractorItems=0, 4\n",
            "Episode: 1/100, Total Reward: 1.0, Epsilon: 1.0000, Total Step: 9\n",
            "Episode: 2/100, Total Reward: 0.0, Epsilon: 0.9950, Total Step: 49\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 0.0, Epsilon: 0.9851, Total Step: 49\n",
            "Episode: 5/100, Total Reward: 0.0, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 0.0, Epsilon: 0.9752, Total Step: 49\n",
            "Episode: 7/100, Total Reward: 1.0, Epsilon: 0.9704, Total Step: 20\n",
            "Episode: 8/100, Total Reward: 0.0, Epsilon: 0.9655, Total Step: 49\n",
            "Episode: 9/100, Total Reward: 1.0, Epsilon: 0.9607, Total Step: 30\n",
            "Episode: 10/100, Total Reward: 0.0, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 1.0, Epsilon: 0.9511, Total Step: 21\n",
            "Episode: 12/100, Total Reward: 1.0, Epsilon: 0.9464, Total Step: 31\n",
            "Episode: 13/100, Total Reward: 1.0, Epsilon: 0.9416, Total Step: 26\n",
            "Episode: 14/100, Total Reward: 0.0, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 1.0, Epsilon: 0.9322, Total Step: 13\n",
            "Episode: 16/100, Total Reward: 0.0, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 1.0, Epsilon: 0.9229, Total Step: 15\n",
            "Episode: 18/100, Total Reward: 1.0, Epsilon: 0.9183, Total Step: 36\n",
            "Episode: 19/100, Total Reward: 0.0, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 0.0, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 1.0, Epsilon: 0.9046, Total Step: 8\n",
            "Episode: 22/100, Total Reward: 1.0, Epsilon: 0.9001, Total Step: 12\n",
            "Episode: 23/100, Total Reward: 0.0, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 0.0, Epsilon: 0.8911, Total Step: 49\n",
            "Episode: 25/100, Total Reward: 0.0, Epsilon: 0.8867, Total Step: 49\n",
            "Episode: 26/100, Total Reward: 0.0, Epsilon: 0.8822, Total Step: 49\n",
            "Episode: 27/100, Total Reward: 0.0, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 0.0, Epsilon: 0.8734, Total Step: 49\n",
            "Episode: 29/100, Total Reward: 1.0, Epsilon: 0.8691, Total Step: 8\n",
            "Episode: 30/100, Total Reward: 0.0, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 1.0, Epsilon: 0.8604, Total Step: 17\n",
            "Episode: 32/100, Total Reward: 1.0, Epsilon: 0.8561, Total Step: 32\n",
            "Episode: 33/100, Total Reward: 0.0, Epsilon: 0.8518, Total Step: 49\n",
            "Episode: 34/100, Total Reward: 0.0, Epsilon: 0.8475, Total Step: 49\n",
            "Episode: 35/100, Total Reward: 0.0, Epsilon: 0.8433, Total Step: 49\n",
            "Episode: 36/100, Total Reward: 0.0, Epsilon: 0.8391, Total Step: 49\n",
            "Episode: 37/100, Total Reward: 0.0, Epsilon: 0.8349, Total Step: 49\n",
            "Episode: 38/100, Total Reward: 1.0, Epsilon: 0.8307, Total Step: 14\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 9\n",
            "Episode: 40/100, Total Reward: 0.0, Epsilon: 0.8224, Total Step: 49\n",
            "Episode: 41/100, Total Reward: 0.0, Epsilon: 0.8183, Total Step: 49\n",
            "Episode: 42/100, Total Reward: 1.0, Epsilon: 0.8142, Total Step: 3\n",
            "Episode: 43/100, Total Reward: 0.0, Epsilon: 0.8102, Total Step: 49\n",
            "Episode: 44/100, Total Reward: 1.0, Epsilon: 0.8061, Total Step: 15\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 10\n",
            "Episode: 46/100, Total Reward: 1.0, Epsilon: 0.7981, Total Step: 18\n",
            "Episode: 47/100, Total Reward: 0.0, Epsilon: 0.7941, Total Step: 49\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 8\n",
            "Episode: 49/100, Total Reward: 1.0, Epsilon: 0.7862, Total Step: 44\n",
            "Episode: 50/100, Total Reward: 1.0, Epsilon: 0.7822, Total Step: 19\n",
            "Episode: 51/100, Total Reward: 1.0, Epsilon: 0.7783, Total Step: 19\n",
            "Episode: 52/100, Total Reward: 0.0, Epsilon: 0.7744, Total Step: 49\n",
            "Episode: 53/100, Total Reward: 1.0, Epsilon: 0.7705, Total Step: 7\n",
            "Episode: 54/100, Total Reward: 0.0, Epsilon: 0.7667, Total Step: 49\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 13\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 10\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 18\n",
            "Episode: 58/100, Total Reward: 0.0, Epsilon: 0.7515, Total Step: 49\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 12\n",
            "Episode: 60/100, Total Reward: 0.0, Epsilon: 0.7440, Total Step: 49\n",
            "Episode: 61/100, Total Reward: 0.0, Epsilon: 0.7403, Total Step: 49\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 7\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 7\n",
            "Episode: 64/100, Total Reward: 0.0, Epsilon: 0.7292, Total Step: 49\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 7\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 10\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 9\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 20\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 20\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 41\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 19\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 12\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 16\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 4\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 11\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 49\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 8\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 9\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 4\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 9\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 7\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 7\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 45\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 20\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 4\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 4\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 10\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 3\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 5\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 5\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 21\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 8\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 9\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 10\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 10\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 12\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 6\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 9\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 25\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 15\n",
            "TESTING 5: total_reward 5.0/5 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=6,includeDoors=1,numDistractorItems=0, 0\n",
            "Episode: 1/100, Total Reward: 0.0, Epsilon: 1.0000, Total Step: 49\n",
            "Episode: 2/100, Total Reward: 0.0, Epsilon: 0.9950, Total Step: 49\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 1.0, Epsilon: 0.9851, Total Step: 15\n",
            "Episode: 5/100, Total Reward: 0.0, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 0.0, Epsilon: 0.9752, Total Step: 49\n",
            "Episode: 7/100, Total Reward: 0.0, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 1.0, Epsilon: 0.9655, Total Step: 41\n",
            "Episode: 9/100, Total Reward: 0.0, Epsilon: 0.9607, Total Step: 49\n",
            "Episode: 10/100, Total Reward: 0.0, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 1.0, Epsilon: 0.9511, Total Step: 9\n",
            "Episode: 12/100, Total Reward: 0.0, Epsilon: 0.9464, Total Step: 49\n",
            "Episode: 13/100, Total Reward: 1.0, Epsilon: 0.9416, Total Step: 20\n",
            "Episode: 14/100, Total Reward: 0.0, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 1.0, Epsilon: 0.9322, Total Step: 40\n",
            "Episode: 16/100, Total Reward: 0.0, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 1.0, Epsilon: 0.9229, Total Step: 10\n",
            "Episode: 18/100, Total Reward: 0.0, Epsilon: 0.9183, Total Step: 49\n",
            "Episode: 19/100, Total Reward: 0.0, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 1.0, Epsilon: 0.9092, Total Step: 35\n",
            "Episode: 21/100, Total Reward: 0.0, Epsilon: 0.9046, Total Step: 49\n",
            "Episode: 22/100, Total Reward: 1.0, Epsilon: 0.9001, Total Step: 46\n",
            "Episode: 23/100, Total Reward: 0.0, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 0.0, Epsilon: 0.8911, Total Step: 49\n",
            "Episode: 25/100, Total Reward: 0.0, Epsilon: 0.8867, Total Step: 49\n",
            "Episode: 26/100, Total Reward: 1.0, Epsilon: 0.8822, Total Step: 4\n",
            "Episode: 27/100, Total Reward: 0.0, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 1.0, Epsilon: 0.8734, Total Step: 31\n",
            "Episode: 29/100, Total Reward: 0.0, Epsilon: 0.8691, Total Step: 49\n",
            "Episode: 30/100, Total Reward: 1.0, Epsilon: 0.8647, Total Step: 11\n",
            "Episode: 31/100, Total Reward: 1.0, Epsilon: 0.8604, Total Step: 24\n",
            "Episode: 32/100, Total Reward: 0.0, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 1.0, Epsilon: 0.8518, Total Step: 9\n",
            "Episode: 34/100, Total Reward: 0.0, Epsilon: 0.8475, Total Step: 49\n",
            "Episode: 35/100, Total Reward: 0.0, Epsilon: 0.8433, Total Step: 49\n",
            "Episode: 36/100, Total Reward: 0.0, Epsilon: 0.8391, Total Step: 49\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 30\n",
            "Episode: 38/100, Total Reward: 0.0, Epsilon: 0.8307, Total Step: 49\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 31\n",
            "Episode: 40/100, Total Reward: 1.0, Epsilon: 0.8224, Total Step: 12\n",
            "Episode: 41/100, Total Reward: 1.0, Epsilon: 0.8183, Total Step: 29\n",
            "Episode: 42/100, Total Reward: 1.0, Epsilon: 0.8142, Total Step: 34\n",
            "Episode: 43/100, Total Reward: 1.0, Epsilon: 0.8102, Total Step: 26\n",
            "Episode: 44/100, Total Reward: 0.0, Epsilon: 0.8061, Total Step: 49\n",
            "Episode: 45/100, Total Reward: 0.0, Epsilon: 0.8021, Total Step: 49\n",
            "Episode: 46/100, Total Reward: 1.0, Epsilon: 0.7981, Total Step: 25\n",
            "Episode: 47/100, Total Reward: 1.0, Epsilon: 0.7941, Total Step: 33\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 48\n",
            "Episode: 49/100, Total Reward: 0.0, Epsilon: 0.7862, Total Step: 49\n",
            "Episode: 50/100, Total Reward: 1.0, Epsilon: 0.7822, Total Step: 16\n",
            "Episode: 51/100, Total Reward: 0.0, Epsilon: 0.7783, Total Step: 49\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 16\n",
            "Episode: 53/100, Total Reward: 0.0, Epsilon: 0.7705, Total Step: 49\n",
            "Episode: 54/100, Total Reward: 0.0, Epsilon: 0.7667, Total Step: 49\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 38\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 22\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 28\n",
            "Episode: 58/100, Total Reward: 1.0, Epsilon: 0.7515, Total Step: 12\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 15\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 15\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 9\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 14\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 42\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 22\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 11\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 44\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 6\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 22\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 10\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 13\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 24\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 41\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 6\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 10\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 9\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 20\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 25\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 18\n",
            "Episode: 79/100, Total Reward: 0.0, Epsilon: 0.6764, Total Step: 49\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 15\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 18\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 8\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 10\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 7\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 7\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 12\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 11\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 17\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 29\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 7\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 7\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 37\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 14\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 3\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 11\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 16\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 6\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 4\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 18\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 9\n",
            "TESTING 6: total_reward 6.0/6 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=6,includeDoors=1,numDistractorItems=0, 1\n",
            "Episode: 1/100, Total Reward: 1.0, Epsilon: 1.0000, Total Step: 33\n",
            "Episode: 2/100, Total Reward: 1.0, Epsilon: 0.9950, Total Step: 5\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 1.0, Epsilon: 0.9851, Total Step: 5\n",
            "Episode: 5/100, Total Reward: 0.0, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 1.0, Epsilon: 0.9752, Total Step: 49\n",
            "Episode: 7/100, Total Reward: 1.0, Epsilon: 0.9704, Total Step: 9\n",
            "Episode: 8/100, Total Reward: 0.0, Epsilon: 0.9655, Total Step: 49\n",
            "Episode: 9/100, Total Reward: 1.0, Epsilon: 0.9607, Total Step: 2\n",
            "Episode: 10/100, Total Reward: 1.0, Epsilon: 0.9559, Total Step: 40\n",
            "Episode: 11/100, Total Reward: 1.0, Epsilon: 0.9511, Total Step: 6\n",
            "Episode: 12/100, Total Reward: 0.0, Epsilon: 0.9464, Total Step: 49\n",
            "Episode: 13/100, Total Reward: 1.0, Epsilon: 0.9416, Total Step: 13\n",
            "Episode: 14/100, Total Reward: 1.0, Epsilon: 0.9369, Total Step: 27\n",
            "Episode: 15/100, Total Reward: 0.0, Epsilon: 0.9322, Total Step: 49\n",
            "Episode: 16/100, Total Reward: 0.0, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 1.0, Epsilon: 0.9229, Total Step: 8\n",
            "Episode: 18/100, Total Reward: 0.0, Epsilon: 0.9183, Total Step: 49\n",
            "Episode: 19/100, Total Reward: 1.0, Epsilon: 0.9137, Total Step: 34\n",
            "Episode: 20/100, Total Reward: 1.0, Epsilon: 0.9092, Total Step: 21\n",
            "Episode: 21/100, Total Reward: 1.0, Epsilon: 0.9046, Total Step: 20\n",
            "Episode: 22/100, Total Reward: 1.0, Epsilon: 0.9001, Total Step: 9\n",
            "Episode: 23/100, Total Reward: 0.0, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 1.0, Epsilon: 0.8911, Total Step: 12\n",
            "Episode: 25/100, Total Reward: 1.0, Epsilon: 0.8867, Total Step: 7\n",
            "Episode: 26/100, Total Reward: 1.0, Epsilon: 0.8822, Total Step: 11\n",
            "Episode: 27/100, Total Reward: 0.0, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 1.0, Epsilon: 0.8734, Total Step: 14\n",
            "Episode: 29/100, Total Reward: 1.0, Epsilon: 0.8691, Total Step: 29\n",
            "Episode: 30/100, Total Reward: 1.0, Epsilon: 0.8647, Total Step: 32\n",
            "Episode: 31/100, Total Reward: 1.0, Epsilon: 0.8604, Total Step: 31\n",
            "Episode: 32/100, Total Reward: 0.0, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 0.0, Epsilon: 0.8518, Total Step: 49\n",
            "Episode: 34/100, Total Reward: 1.0, Epsilon: 0.8475, Total Step: 25\n",
            "Episode: 35/100, Total Reward: 1.0, Epsilon: 0.8433, Total Step: 19\n",
            "Episode: 36/100, Total Reward: 1.0, Epsilon: 0.8391, Total Step: 9\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 27\n",
            "Episode: 38/100, Total Reward: 1.0, Epsilon: 0.8307, Total Step: 5\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 7\n",
            "Episode: 40/100, Total Reward: 1.0, Epsilon: 0.8224, Total Step: 34\n",
            "Episode: 41/100, Total Reward: 1.0, Epsilon: 0.8183, Total Step: 10\n",
            "Episode: 42/100, Total Reward: 1.0, Epsilon: 0.8142, Total Step: 15\n",
            "Episode: 43/100, Total Reward: 1.0, Epsilon: 0.8102, Total Step: 5\n",
            "Episode: 44/100, Total Reward: 0.0, Epsilon: 0.8061, Total Step: 49\n",
            "Episode: 45/100, Total Reward: 0.0, Epsilon: 0.8021, Total Step: 49\n",
            "Episode: 46/100, Total Reward: 1.0, Epsilon: 0.7981, Total Step: 32\n",
            "Episode: 47/100, Total Reward: 1.0, Epsilon: 0.7941, Total Step: 16\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 8\n",
            "Episode: 49/100, Total Reward: 1.0, Epsilon: 0.7862, Total Step: 9\n",
            "Episode: 50/100, Total Reward: 1.0, Epsilon: 0.7822, Total Step: 4\n",
            "Episode: 51/100, Total Reward: 1.0, Epsilon: 0.7783, Total Step: 39\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 2\n",
            "Episode: 53/100, Total Reward: 0.0, Epsilon: 0.7705, Total Step: 49\n",
            "Episode: 54/100, Total Reward: 1.0, Epsilon: 0.7667, Total Step: 4\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 39\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 13\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 22\n",
            "Episode: 58/100, Total Reward: 1.0, Epsilon: 0.7515, Total Step: 9\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 6\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 12\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 3\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 7\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 20\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 19\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 8\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 10\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 36\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 13\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 49\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 37\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 10\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 17\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 20\n",
            "Episode: 74/100, Total Reward: 0.0, Epsilon: 0.6936, Total Step: 49\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 20\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 9\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 10\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 2\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 24\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 9\n",
            "Episode: 81/100, Total Reward: 0.0, Epsilon: 0.6696, Total Step: 49\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 19\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 10\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 14\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 5\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 8\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 4\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 10\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 15\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 8\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 9\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 16\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 3\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 4\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 3\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 3\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 3\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 3\n",
            "Episode: 99/100, Total Reward: 0.0, Epsilon: 0.6119, Total Step: 49\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 14\n",
            "TESTING 7: total_reward 7.0/7 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=6,includeDoors=1,numDistractorItems=0, 2\n",
            "Episode: 1/100, Total Reward: 0.0, Epsilon: 1.0000, Total Step: 49\n",
            "Episode: 2/100, Total Reward: 1.0, Epsilon: 0.9950, Total Step: 32\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 1.0, Epsilon: 0.9851, Total Step: 18\n",
            "Episode: 5/100, Total Reward: 1.0, Epsilon: 0.9801, Total Step: 8\n",
            "Episode: 6/100, Total Reward: 1.0, Epsilon: 0.9752, Total Step: 15\n",
            "Episode: 7/100, Total Reward: 0.0, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 1.0, Epsilon: 0.9655, Total Step: 35\n",
            "Episode: 9/100, Total Reward: 0.0, Epsilon: 0.9607, Total Step: 49\n",
            "Episode: 10/100, Total Reward: 1.0, Epsilon: 0.9559, Total Step: 8\n",
            "Episode: 11/100, Total Reward: 1.0, Epsilon: 0.9511, Total Step: 17\n",
            "Episode: 12/100, Total Reward: 1.0, Epsilon: 0.9464, Total Step: 26\n",
            "Episode: 13/100, Total Reward: 1.0, Epsilon: 0.9416, Total Step: 27\n",
            "Episode: 14/100, Total Reward: 0.0, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 1.0, Epsilon: 0.9322, Total Step: 27\n",
            "Episode: 16/100, Total Reward: 0.0, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 1.0, Epsilon: 0.9229, Total Step: 48\n",
            "Episode: 18/100, Total Reward: 0.0, Epsilon: 0.9183, Total Step: 49\n",
            "Episode: 19/100, Total Reward: 0.0, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 1.0, Epsilon: 0.9092, Total Step: 27\n",
            "Episode: 21/100, Total Reward: 1.0, Epsilon: 0.9046, Total Step: 18\n",
            "Episode: 22/100, Total Reward: 0.0, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 1.0, Epsilon: 0.8956, Total Step: 28\n",
            "Episode: 24/100, Total Reward: 1.0, Epsilon: 0.8911, Total Step: 9\n",
            "Episode: 25/100, Total Reward: 1.0, Epsilon: 0.8867, Total Step: 16\n",
            "Episode: 26/100, Total Reward: 1.0, Epsilon: 0.8822, Total Step: 18\n",
            "Episode: 27/100, Total Reward: 0.0, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 1.0, Epsilon: 0.8734, Total Step: 31\n",
            "Episode: 29/100, Total Reward: 0.0, Epsilon: 0.8691, Total Step: 49\n",
            "Episode: 30/100, Total Reward: 1.0, Epsilon: 0.8647, Total Step: 26\n",
            "Episode: 31/100, Total Reward: 1.0, Epsilon: 0.8604, Total Step: 7\n",
            "Episode: 32/100, Total Reward: 0.0, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 0.0, Epsilon: 0.8518, Total Step: 49\n",
            "Episode: 34/100, Total Reward: 1.0, Epsilon: 0.8475, Total Step: 25\n",
            "Episode: 35/100, Total Reward: 0.0, Epsilon: 0.8433, Total Step: 49\n",
            "Episode: 36/100, Total Reward: 1.0, Epsilon: 0.8391, Total Step: 14\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 14\n",
            "Episode: 38/100, Total Reward: 1.0, Epsilon: 0.8307, Total Step: 35\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 14\n",
            "Episode: 40/100, Total Reward: 1.0, Epsilon: 0.8224, Total Step: 32\n",
            "Episode: 41/100, Total Reward: 1.0, Epsilon: 0.8183, Total Step: 29\n",
            "Episode: 42/100, Total Reward: 1.0, Epsilon: 0.8142, Total Step: 8\n",
            "Episode: 43/100, Total Reward: 0.0, Epsilon: 0.8102, Total Step: 49\n",
            "Episode: 44/100, Total Reward: 0.0, Epsilon: 0.8061, Total Step: 49\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 25\n",
            "Episode: 46/100, Total Reward: 1.0, Epsilon: 0.7981, Total Step: 15\n",
            "Episode: 47/100, Total Reward: 1.0, Epsilon: 0.7941, Total Step: 32\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 20\n",
            "Episode: 49/100, Total Reward: 1.0, Epsilon: 0.7862, Total Step: 34\n",
            "Episode: 50/100, Total Reward: 0.0, Epsilon: 0.7822, Total Step: 49\n",
            "Episode: 51/100, Total Reward: 0.0, Epsilon: 0.7783, Total Step: 49\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 30\n",
            "Episode: 53/100, Total Reward: 1.0, Epsilon: 0.7705, Total Step: 11\n",
            "Episode: 54/100, Total Reward: 1.0, Epsilon: 0.7667, Total Step: 18\n",
            "Episode: 55/100, Total Reward: 0.0, Epsilon: 0.7629, Total Step: 49\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 33\n",
            "Episode: 57/100, Total Reward: 0.0, Epsilon: 0.7553, Total Step: 49\n",
            "Episode: 58/100, Total Reward: 1.0, Epsilon: 0.7515, Total Step: 33\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 18\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 29\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 36\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 4\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 23\n",
            "Episode: 64/100, Total Reward: 0.0, Epsilon: 0.7292, Total Step: 49\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 12\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 6\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 14\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 39\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 12\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 17\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 17\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 5\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 15\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 9\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 13\n",
            "Episode: 76/100, Total Reward: 0.0, Epsilon: 0.6866, Total Step: 49\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 11\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 15\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 18\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 6\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 13\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 11\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 6\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 8\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 18\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 29\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 4\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 11\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 15\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 10\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 4\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 12\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 8\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 5\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 3\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 20\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 9\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 8\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 19\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 4\n",
            "TESTING 8: total_reward 8.0/8 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=6,includeDoors=1,numDistractorItems=0, 3\n",
            "Episode: 1/100, Total Reward: 0.0, Epsilon: 1.0000, Total Step: 49\n",
            "Episode: 2/100, Total Reward: 1.0, Epsilon: 0.9950, Total Step: 35\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 0.0, Epsilon: 0.9851, Total Step: 49\n",
            "Episode: 5/100, Total Reward: 0.0, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 0.0, Epsilon: 0.9752, Total Step: 49\n",
            "Episode: 7/100, Total Reward: 1.0, Epsilon: 0.9704, Total Step: 31\n",
            "Episode: 8/100, Total Reward: 0.0, Epsilon: 0.9655, Total Step: 49\n",
            "Episode: 9/100, Total Reward: 0.0, Epsilon: 0.9607, Total Step: 49\n",
            "Episode: 10/100, Total Reward: 0.0, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 0.0, Epsilon: 0.9511, Total Step: 49\n",
            "Episode: 12/100, Total Reward: 0.0, Epsilon: 0.9464, Total Step: 49\n",
            "Episode: 13/100, Total Reward: 0.0, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 0.0, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 1.0, Epsilon: 0.9322, Total Step: 23\n",
            "Episode: 16/100, Total Reward: 0.0, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 0.0, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 0.0, Epsilon: 0.9183, Total Step: 49\n",
            "Episode: 19/100, Total Reward: 0.0, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 0.0, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 0.0, Epsilon: 0.9046, Total Step: 49\n",
            "Episode: 22/100, Total Reward: 0.0, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 0.0, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 0.0, Epsilon: 0.8911, Total Step: 49\n",
            "Episode: 25/100, Total Reward: 0.0, Epsilon: 0.8867, Total Step: 49\n",
            "Episode: 26/100, Total Reward: 0.0, Epsilon: 0.8822, Total Step: 49\n",
            "Episode: 27/100, Total Reward: 1.0, Epsilon: 0.8778, Total Step: 12\n",
            "Episode: 28/100, Total Reward: 0.0, Epsilon: 0.8734, Total Step: 49\n",
            "Episode: 29/100, Total Reward: 0.0, Epsilon: 0.8691, Total Step: 49\n",
            "Episode: 30/100, Total Reward: 0.0, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 0.0, Epsilon: 0.8604, Total Step: 49\n",
            "Episode: 32/100, Total Reward: 0.0, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 0.0, Epsilon: 0.8518, Total Step: 49\n",
            "Episode: 34/100, Total Reward: 0.0, Epsilon: 0.8475, Total Step: 49\n",
            "Episode: 35/100, Total Reward: 0.0, Epsilon: 0.8433, Total Step: 49\n",
            "Episode: 36/100, Total Reward: 0.0, Epsilon: 0.8391, Total Step: 49\n",
            "Episode: 37/100, Total Reward: 0.0, Epsilon: 0.8349, Total Step: 49\n",
            "Episode: 38/100, Total Reward: 0.0, Epsilon: 0.8307, Total Step: 49\n",
            "Episode: 39/100, Total Reward: 0.0, Epsilon: 0.8266, Total Step: 49\n",
            "Episode: 40/100, Total Reward: 0.0, Epsilon: 0.8224, Total Step: 49\n",
            "Episode: 41/100, Total Reward: 0.0, Epsilon: 0.8183, Total Step: 49\n",
            "Episode: 42/100, Total Reward: 0.0, Epsilon: 0.8142, Total Step: 49\n",
            "Episode: 43/100, Total Reward: 0.0, Epsilon: 0.8102, Total Step: 49\n",
            "Episode: 44/100, Total Reward: 0.0, Epsilon: 0.8061, Total Step: 49\n",
            "Episode: 45/100, Total Reward: 0.0, Epsilon: 0.8021, Total Step: 49\n",
            "Episode: 46/100, Total Reward: 0.0, Epsilon: 0.7981, Total Step: 49\n",
            "Episode: 47/100, Total Reward: 0.0, Epsilon: 0.7941, Total Step: 49\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 10\n",
            "Episode: 49/100, Total Reward: 0.0, Epsilon: 0.7862, Total Step: 49\n",
            "Episode: 50/100, Total Reward: 1.0, Epsilon: 0.7822, Total Step: 28\n",
            "Episode: 51/100, Total Reward: 0.0, Epsilon: 0.7783, Total Step: 49\n",
            "Episode: 52/100, Total Reward: 0.0, Epsilon: 0.7744, Total Step: 49\n",
            "Episode: 53/100, Total Reward: 0.0, Epsilon: 0.7705, Total Step: 49\n",
            "Episode: 54/100, Total Reward: 1.0, Epsilon: 0.7667, Total Step: 26\n",
            "Episode: 55/100, Total Reward: 0.0, Epsilon: 0.7629, Total Step: 49\n",
            "Episode: 56/100, Total Reward: 0.0, Epsilon: 0.7590, Total Step: 49\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 49\n",
            "Episode: 58/100, Total Reward: 0.0, Epsilon: 0.7515, Total Step: 49\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 44\n",
            "Episode: 60/100, Total Reward: 0.0, Epsilon: 0.7440, Total Step: 49\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 30\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 47\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 22\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 33\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 31\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 13\n",
            "Episode: 67/100, Total Reward: 0.0, Epsilon: 0.7183, Total Step: 49\n",
            "Episode: 68/100, Total Reward: 0.0, Epsilon: 0.7147, Total Step: 49\n",
            "Episode: 69/100, Total Reward: 0.0, Epsilon: 0.7112, Total Step: 49\n",
            "Episode: 70/100, Total Reward: 0.0, Epsilon: 0.7076, Total Step: 49\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 21\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 11\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 42\n",
            "Episode: 74/100, Total Reward: 0.0, Epsilon: 0.6936, Total Step: 49\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 8\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 16\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 16\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 13\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 11\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 34\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 15\n",
            "Episode: 82/100, Total Reward: 0.0, Epsilon: 0.6663, Total Step: 49\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 19\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 41\n",
            "Episode: 85/100, Total Reward: 0.0, Epsilon: 0.6564, Total Step: 49\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 43\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 15\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 22\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 7\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 25\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 8\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 11\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 34\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 11\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 24\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 12\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 23\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 17\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 31\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 12\n",
            "TESTING 9: total_reward 8.0/9 \t (reward: 0.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=6,includeDoors=1,numDistractorItems=0, 4\n",
            "Episode: 1/100, Total Reward: 1.0, Epsilon: 1.0000, Total Step: 42\n",
            "Episode: 2/100, Total Reward: 1.0, Epsilon: 0.9950, Total Step: 22\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 0.0, Epsilon: 0.9851, Total Step: 49\n",
            "Episode: 5/100, Total Reward: 0.0, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 0.0, Epsilon: 0.9752, Total Step: 49\n",
            "Episode: 7/100, Total Reward: 0.0, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 0.0, Epsilon: 0.9655, Total Step: 49\n",
            "Episode: 9/100, Total Reward: 0.0, Epsilon: 0.9607, Total Step: 49\n",
            "Episode: 10/100, Total Reward: 0.0, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 0.0, Epsilon: 0.9511, Total Step: 49\n",
            "Episode: 12/100, Total Reward: 1.0, Epsilon: 0.9464, Total Step: 27\n",
            "Episode: 13/100, Total Reward: 0.0, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 0.0, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 0.0, Epsilon: 0.9322, Total Step: 49\n",
            "Episode: 16/100, Total Reward: 0.0, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 1.0, Epsilon: 0.9229, Total Step: 26\n",
            "Episode: 18/100, Total Reward: 0.0, Epsilon: 0.9183, Total Step: 49\n",
            "Episode: 19/100, Total Reward: 1.0, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 0.0, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 1.0, Epsilon: 0.9046, Total Step: 19\n",
            "Episode: 22/100, Total Reward: 0.0, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 0.0, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 0.0, Epsilon: 0.8911, Total Step: 49\n",
            "Episode: 25/100, Total Reward: 0.0, Epsilon: 0.8867, Total Step: 49\n",
            "Episode: 26/100, Total Reward: 0.0, Epsilon: 0.8822, Total Step: 49\n",
            "Episode: 27/100, Total Reward: 1.0, Epsilon: 0.8778, Total Step: 23\n",
            "Episode: 28/100, Total Reward: 0.0, Epsilon: 0.8734, Total Step: 49\n",
            "Episode: 29/100, Total Reward: 1.0, Epsilon: 0.8691, Total Step: 39\n",
            "Episode: 30/100, Total Reward: 0.0, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 0.0, Epsilon: 0.8604, Total Step: 49\n",
            "Episode: 32/100, Total Reward: 0.0, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 1.0, Epsilon: 0.8518, Total Step: 36\n",
            "Episode: 34/100, Total Reward: 0.0, Epsilon: 0.8475, Total Step: 49\n",
            "Episode: 35/100, Total Reward: 0.0, Epsilon: 0.8433, Total Step: 49\n",
            "Episode: 36/100, Total Reward: 0.0, Epsilon: 0.8391, Total Step: 49\n",
            "Episode: 37/100, Total Reward: 0.0, Epsilon: 0.8349, Total Step: 49\n",
            "Episode: 38/100, Total Reward: 1.0, Epsilon: 0.8307, Total Step: 20\n",
            "Episode: 39/100, Total Reward: 0.0, Epsilon: 0.8266, Total Step: 49\n",
            "Episode: 40/100, Total Reward: 1.0, Epsilon: 0.8224, Total Step: 44\n",
            "Episode: 41/100, Total Reward: 1.0, Epsilon: 0.8183, Total Step: 17\n",
            "Episode: 42/100, Total Reward: 1.0, Epsilon: 0.8142, Total Step: 19\n",
            "Episode: 43/100, Total Reward: 0.0, Epsilon: 0.8102, Total Step: 49\n",
            "Episode: 44/100, Total Reward: 1.0, Epsilon: 0.8061, Total Step: 40\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 43\n",
            "Episode: 46/100, Total Reward: 0.0, Epsilon: 0.7981, Total Step: 49\n",
            "Episode: 47/100, Total Reward: 0.0, Epsilon: 0.7941, Total Step: 49\n",
            "Episode: 48/100, Total Reward: 0.0, Epsilon: 0.7901, Total Step: 49\n",
            "Episode: 49/100, Total Reward: 1.0, Epsilon: 0.7862, Total Step: 28\n",
            "Episode: 50/100, Total Reward: 0.0, Epsilon: 0.7822, Total Step: 49\n",
            "Episode: 51/100, Total Reward: 0.0, Epsilon: 0.7783, Total Step: 49\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 21\n",
            "Episode: 53/100, Total Reward: 0.0, Epsilon: 0.7705, Total Step: 49\n",
            "Episode: 54/100, Total Reward: 0.0, Epsilon: 0.7667, Total Step: 49\n",
            "Episode: 55/100, Total Reward: 0.0, Epsilon: 0.7629, Total Step: 49\n",
            "Episode: 56/100, Total Reward: 0.0, Epsilon: 0.7590, Total Step: 49\n",
            "Episode: 57/100, Total Reward: 0.0, Epsilon: 0.7553, Total Step: 49\n",
            "Episode: 58/100, Total Reward: 1.0, Epsilon: 0.7515, Total Step: 39\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 46\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 48\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 33\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 14\n",
            "Episode: 63/100, Total Reward: 0.0, Epsilon: 0.7329, Total Step: 49\n",
            "Episode: 64/100, Total Reward: 0.0, Epsilon: 0.7292, Total Step: 49\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 47\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 38\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 45\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 17\n",
            "Episode: 69/100, Total Reward: 0.0, Epsilon: 0.7112, Total Step: 49\n",
            "Episode: 70/100, Total Reward: 0.0, Epsilon: 0.7076, Total Step: 49\n",
            "Episode: 71/100, Total Reward: 0.0, Epsilon: 0.7041, Total Step: 49\n",
            "Episode: 72/100, Total Reward: 0.0, Epsilon: 0.7005, Total Step: 49\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 31\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 37\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 39\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 18\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 42\n",
            "Episode: 78/100, Total Reward: 0.0, Epsilon: 0.6798, Total Step: 49\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 17\n",
            "Episode: 80/100, Total Reward: 0.0, Epsilon: 0.6730, Total Step: 49\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 13\n",
            "Episode: 82/100, Total Reward: 0.0, Epsilon: 0.6663, Total Step: 49\n",
            "Episode: 83/100, Total Reward: 0.0, Epsilon: 0.6630, Total Step: 49\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 11\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 32\n",
            "Episode: 86/100, Total Reward: 0.0, Epsilon: 0.6531, Total Step: 49\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 10\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 24\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 36\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 19\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 43\n",
            "Episode: 92/100, Total Reward: 0.0, Epsilon: 0.6337, Total Step: 49\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 31\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 20\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 15\n",
            "Episode: 96/100, Total Reward: 0.0, Epsilon: 0.6211, Total Step: 49\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 7\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 19\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 25\n",
            "Episode: 100/100, Total Reward: 0.0, Epsilon: 0.6088, Total Step: 49\n",
            "TESTING 10: total_reward 8.0/10 \t (reward: 0.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=7,includeDoors=1,numDistractorItems=0, 0\n",
            "Episode: 1/100, Total Reward: 1.0, Epsilon: 1.0000, Total Step: 40\n",
            "Episode: 2/100, Total Reward: 1.0, Epsilon: 0.9950, Total Step: 29\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 1.0, Epsilon: 0.9851, Total Step: 45\n",
            "Episode: 5/100, Total Reward: 0.0, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 0.0, Epsilon: 0.9752, Total Step: 49\n",
            "Episode: 7/100, Total Reward: 1.0, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 0.0, Epsilon: 0.9655, Total Step: 49\n",
            "Episode: 9/100, Total Reward: 0.0, Epsilon: 0.9607, Total Step: 49\n",
            "Episode: 10/100, Total Reward: 0.0, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 0.0, Epsilon: 0.9511, Total Step: 49\n",
            "Episode: 12/100, Total Reward: 0.0, Epsilon: 0.9464, Total Step: 49\n",
            "Episode: 13/100, Total Reward: 0.0, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 0.0, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 0.0, Epsilon: 0.9322, Total Step: 49\n",
            "Episode: 16/100, Total Reward: 0.0, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 0.0, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 0.0, Epsilon: 0.9183, Total Step: 49\n",
            "Episode: 19/100, Total Reward: 1.0, Epsilon: 0.9137, Total Step: 47\n",
            "Episode: 20/100, Total Reward: 0.0, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 1.0, Epsilon: 0.9046, Total Step: 31\n",
            "Episode: 22/100, Total Reward: 0.0, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 0.0, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 0.0, Epsilon: 0.8911, Total Step: 49\n",
            "Episode: 25/100, Total Reward: 1.0, Epsilon: 0.8867, Total Step: 30\n",
            "Episode: 26/100, Total Reward: 0.0, Epsilon: 0.8822, Total Step: 49\n",
            "Episode: 27/100, Total Reward: 0.0, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 0.0, Epsilon: 0.8734, Total Step: 49\n",
            "Episode: 29/100, Total Reward: 1.0, Epsilon: 0.8691, Total Step: 40\n",
            "Episode: 30/100, Total Reward: 0.0, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 1.0, Epsilon: 0.8604, Total Step: 39\n",
            "Episode: 32/100, Total Reward: 0.0, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 0.0, Epsilon: 0.8518, Total Step: 49\n",
            "Episode: 34/100, Total Reward: 0.0, Epsilon: 0.8475, Total Step: 49\n",
            "Episode: 35/100, Total Reward: 0.0, Epsilon: 0.8433, Total Step: 49\n",
            "Episode: 36/100, Total Reward: 0.0, Epsilon: 0.8391, Total Step: 49\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 25\n",
            "Episode: 38/100, Total Reward: 0.0, Epsilon: 0.8307, Total Step: 49\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 34\n",
            "Episode: 40/100, Total Reward: 0.0, Epsilon: 0.8224, Total Step: 49\n",
            "Episode: 41/100, Total Reward: 1.0, Epsilon: 0.8183, Total Step: 40\n",
            "Episode: 42/100, Total Reward: 0.0, Epsilon: 0.8142, Total Step: 49\n",
            "Episode: 43/100, Total Reward: 1.0, Epsilon: 0.8102, Total Step: 29\n",
            "Episode: 44/100, Total Reward: 1.0, Epsilon: 0.8061, Total Step: 29\n",
            "Episode: 45/100, Total Reward: 0.0, Epsilon: 0.8021, Total Step: 49\n",
            "Episode: 46/100, Total Reward: 1.0, Epsilon: 0.7981, Total Step: 29\n",
            "Episode: 47/100, Total Reward: 0.0, Epsilon: 0.7941, Total Step: 49\n",
            "Episode: 48/100, Total Reward: 0.0, Epsilon: 0.7901, Total Step: 49\n",
            "Episode: 49/100, Total Reward: 0.0, Epsilon: 0.7862, Total Step: 49\n",
            "Episode: 50/100, Total Reward: 1.0, Epsilon: 0.7822, Total Step: 42\n",
            "Episode: 51/100, Total Reward: 0.0, Epsilon: 0.7783, Total Step: 49\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 25\n",
            "Episode: 53/100, Total Reward: 0.0, Epsilon: 0.7705, Total Step: 49\n",
            "Episode: 54/100, Total Reward: 0.0, Epsilon: 0.7667, Total Step: 49\n",
            "Episode: 55/100, Total Reward: 0.0, Epsilon: 0.7629, Total Step: 49\n",
            "Episode: 56/100, Total Reward: 0.0, Epsilon: 0.7590, Total Step: 49\n",
            "Episode: 57/100, Total Reward: 0.0, Epsilon: 0.7553, Total Step: 49\n",
            "Episode: 58/100, Total Reward: 0.0, Epsilon: 0.7515, Total Step: 49\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 25\n",
            "Episode: 60/100, Total Reward: 0.0, Epsilon: 0.7440, Total Step: 49\n",
            "Episode: 61/100, Total Reward: 0.0, Epsilon: 0.7403, Total Step: 49\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 32\n",
            "Episode: 63/100, Total Reward: 0.0, Epsilon: 0.7329, Total Step: 49\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 22\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 36\n",
            "Episode: 66/100, Total Reward: 0.0, Epsilon: 0.7219, Total Step: 49\n",
            "Episode: 67/100, Total Reward: 0.0, Epsilon: 0.7183, Total Step: 49\n",
            "Episode: 68/100, Total Reward: 0.0, Epsilon: 0.7147, Total Step: 49\n",
            "Episode: 69/100, Total Reward: 0.0, Epsilon: 0.7112, Total Step: 49\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 39\n",
            "Episode: 71/100, Total Reward: 0.0, Epsilon: 0.7041, Total Step: 49\n",
            "Episode: 72/100, Total Reward: 0.0, Epsilon: 0.7005, Total Step: 49\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 49\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 17\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 16\n",
            "Episode: 76/100, Total Reward: 0.0, Epsilon: 0.6866, Total Step: 49\n",
            "Episode: 77/100, Total Reward: 0.0, Epsilon: 0.6832, Total Step: 49\n",
            "Episode: 78/100, Total Reward: 0.0, Epsilon: 0.6798, Total Step: 49\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 25\n",
            "Episode: 80/100, Total Reward: 0.0, Epsilon: 0.6730, Total Step: 49\n",
            "Episode: 81/100, Total Reward: 0.0, Epsilon: 0.6696, Total Step: 49\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 27\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 25\n",
            "Episode: 84/100, Total Reward: 0.0, Epsilon: 0.6597, Total Step: 49\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 17\n",
            "Episode: 86/100, Total Reward: 0.0, Epsilon: 0.6531, Total Step: 49\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 25\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 15\n",
            "Episode: 89/100, Total Reward: 0.0, Epsilon: 0.6433, Total Step: 49\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 18\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 13\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 19\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 24\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 28\n",
            "Episode: 95/100, Total Reward: 0.0, Epsilon: 0.6243, Total Step: 49\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 22\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 34\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 26\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 28\n",
            "Episode: 100/100, Total Reward: 0.0, Epsilon: 0.6088, Total Step: 49\n",
            "TESTING 11: total_reward 8.0/11 \t (reward: 0.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=7,includeDoors=1,numDistractorItems=0, 1\n",
            "Episode: 1/100, Total Reward: 0.0, Epsilon: 1.0000, Total Step: 49\n",
            "Episode: 2/100, Total Reward: 0.0, Epsilon: 0.9950, Total Step: 49\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 0.0, Epsilon: 0.9851, Total Step: 49\n",
            "Episode: 5/100, Total Reward: 0.0, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 0.0, Epsilon: 0.9752, Total Step: 49\n",
            "Episode: 7/100, Total Reward: 0.0, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 0.0, Epsilon: 0.9655, Total Step: 49\n",
            "Episode: 9/100, Total Reward: 0.0, Epsilon: 0.9607, Total Step: 49\n",
            "Episode: 10/100, Total Reward: 0.0, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 0.0, Epsilon: 0.9511, Total Step: 49\n",
            "Episode: 12/100, Total Reward: 0.0, Epsilon: 0.9464, Total Step: 49\n",
            "Episode: 13/100, Total Reward: 0.0, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 0.0, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 0.0, Epsilon: 0.9322, Total Step: 49\n",
            "Episode: 16/100, Total Reward: 0.0, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 0.0, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 0.0, Epsilon: 0.9183, Total Step: 49\n",
            "Episode: 19/100, Total Reward: 0.0, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 0.0, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 1.0, Epsilon: 0.9046, Total Step: 34\n",
            "Episode: 22/100, Total Reward: 0.0, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 0.0, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 0.0, Epsilon: 0.8911, Total Step: 49\n",
            "Episode: 25/100, Total Reward: 0.0, Epsilon: 0.8867, Total Step: 49\n",
            "Episode: 26/100, Total Reward: 0.0, Epsilon: 0.8822, Total Step: 49\n",
            "Episode: 27/100, Total Reward: 0.0, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 0.0, Epsilon: 0.8734, Total Step: 49\n",
            "Episode: 29/100, Total Reward: 0.0, Epsilon: 0.8691, Total Step: 49\n",
            "Episode: 30/100, Total Reward: 0.0, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 0.0, Epsilon: 0.8604, Total Step: 49\n",
            "Episode: 32/100, Total Reward: 0.0, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 0.0, Epsilon: 0.8518, Total Step: 49\n",
            "Episode: 34/100, Total Reward: 0.0, Epsilon: 0.8475, Total Step: 49\n",
            "Episode: 35/100, Total Reward: 0.0, Epsilon: 0.8433, Total Step: 49\n",
            "Episode: 36/100, Total Reward: 0.0, Epsilon: 0.8391, Total Step: 49\n",
            "Episode: 37/100, Total Reward: 0.0, Epsilon: 0.8349, Total Step: 49\n",
            "Episode: 38/100, Total Reward: 0.0, Epsilon: 0.8307, Total Step: 49\n",
            "Episode: 39/100, Total Reward: 0.0, Epsilon: 0.8266, Total Step: 49\n",
            "Episode: 40/100, Total Reward: 0.0, Epsilon: 0.8224, Total Step: 49\n",
            "Episode: 41/100, Total Reward: 0.0, Epsilon: 0.8183, Total Step: 49\n",
            "Episode: 42/100, Total Reward: 0.0, Epsilon: 0.8142, Total Step: 49\n",
            "Episode: 43/100, Total Reward: 0.0, Epsilon: 0.8102, Total Step: 49\n",
            "Episode: 44/100, Total Reward: 0.0, Epsilon: 0.8061, Total Step: 49\n",
            "Episode: 45/100, Total Reward: 0.0, Epsilon: 0.8021, Total Step: 49\n",
            "Episode: 46/100, Total Reward: 0.0, Epsilon: 0.7981, Total Step: 49\n",
            "Episode: 47/100, Total Reward: 0.0, Epsilon: 0.7941, Total Step: 49\n",
            "Episode: 48/100, Total Reward: 0.0, Epsilon: 0.7901, Total Step: 49\n",
            "Episode: 49/100, Total Reward: 0.0, Epsilon: 0.7862, Total Step: 49\n",
            "Episode: 50/100, Total Reward: 0.0, Epsilon: 0.7822, Total Step: 49\n",
            "Episode: 51/100, Total Reward: 0.0, Epsilon: 0.7783, Total Step: 49\n",
            "Episode: 52/100, Total Reward: 0.0, Epsilon: 0.7744, Total Step: 49\n",
            "Episode: 53/100, Total Reward: 0.0, Epsilon: 0.7705, Total Step: 49\n",
            "Episode: 54/100, Total Reward: 0.0, Epsilon: 0.7667, Total Step: 49\n",
            "Episode: 55/100, Total Reward: 0.0, Epsilon: 0.7629, Total Step: 49\n",
            "Episode: 56/100, Total Reward: 0.0, Epsilon: 0.7590, Total Step: 49\n",
            "Episode: 57/100, Total Reward: 0.0, Epsilon: 0.7553, Total Step: 49\n",
            "Episode: 58/100, Total Reward: 0.0, Epsilon: 0.7515, Total Step: 49\n",
            "Episode: 59/100, Total Reward: 0.0, Epsilon: 0.7477, Total Step: 49\n",
            "Episode: 60/100, Total Reward: 0.0, Epsilon: 0.7440, Total Step: 49\n",
            "Episode: 61/100, Total Reward: 0.0, Epsilon: 0.7403, Total Step: 49\n",
            "Episode: 62/100, Total Reward: 0.0, Epsilon: 0.7366, Total Step: 49\n",
            "Episode: 63/100, Total Reward: 0.0, Epsilon: 0.7329, Total Step: 49\n",
            "Episode: 64/100, Total Reward: 0.0, Epsilon: 0.7292, Total Step: 49\n",
            "Episode: 65/100, Total Reward: 0.0, Epsilon: 0.7256, Total Step: 49\n",
            "Episode: 66/100, Total Reward: 0.0, Epsilon: 0.7219, Total Step: 49\n",
            "Episode: 67/100, Total Reward: 0.0, Epsilon: 0.7183, Total Step: 49\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 47\n",
            "Episode: 69/100, Total Reward: 0.0, Epsilon: 0.7112, Total Step: 49\n",
            "Episode: 70/100, Total Reward: 0.0, Epsilon: 0.7076, Total Step: 49\n",
            "Episode: 71/100, Total Reward: 0.0, Epsilon: 0.7041, Total Step: 49\n",
            "Episode: 72/100, Total Reward: 0.0, Epsilon: 0.7005, Total Step: 49\n",
            "Episode: 73/100, Total Reward: 0.0, Epsilon: 0.6970, Total Step: 49\n",
            "Episode: 74/100, Total Reward: 0.0, Epsilon: 0.6936, Total Step: 49\n",
            "Episode: 75/100, Total Reward: 0.0, Epsilon: 0.6901, Total Step: 49\n",
            "Episode: 76/100, Total Reward: 0.0, Epsilon: 0.6866, Total Step: 49\n",
            "Episode: 77/100, Total Reward: 0.0, Epsilon: 0.6832, Total Step: 49\n",
            "Episode: 78/100, Total Reward: 0.0, Epsilon: 0.6798, Total Step: 49\n",
            "Episode: 79/100, Total Reward: 0.0, Epsilon: 0.6764, Total Step: 49\n",
            "Episode: 80/100, Total Reward: 0.0, Epsilon: 0.6730, Total Step: 49\n",
            "Episode: 81/100, Total Reward: 0.0, Epsilon: 0.6696, Total Step: 49\n",
            "Episode: 82/100, Total Reward: 0.0, Epsilon: 0.6663, Total Step: 49\n",
            "Episode: 83/100, Total Reward: 0.0, Epsilon: 0.6630, Total Step: 49\n",
            "Episode: 84/100, Total Reward: 0.0, Epsilon: 0.6597, Total Step: 49\n",
            "Episode: 85/100, Total Reward: 0.0, Epsilon: 0.6564, Total Step: 49\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 40\n",
            "Episode: 87/100, Total Reward: 0.0, Epsilon: 0.6498, Total Step: 49\n",
            "Episode: 88/100, Total Reward: 0.0, Epsilon: 0.6466, Total Step: 49\n",
            "Episode: 89/100, Total Reward: 0.0, Epsilon: 0.6433, Total Step: 49\n",
            "Episode: 90/100, Total Reward: 0.0, Epsilon: 0.6401, Total Step: 49\n",
            "Episode: 91/100, Total Reward: 0.0, Epsilon: 0.6369, Total Step: 49\n",
            "Episode: 92/100, Total Reward: 0.0, Epsilon: 0.6337, Total Step: 49\n",
            "Episode: 93/100, Total Reward: 0.0, Epsilon: 0.6306, Total Step: 49\n",
            "Episode: 94/100, Total Reward: 0.0, Epsilon: 0.6274, Total Step: 49\n",
            "Episode: 95/100, Total Reward: 0.0, Epsilon: 0.6243, Total Step: 49\n",
            "Episode: 96/100, Total Reward: 0.0, Epsilon: 0.6211, Total Step: 49\n",
            "Episode: 97/100, Total Reward: 0.0, Epsilon: 0.6180, Total Step: 49\n",
            "Episode: 98/100, Total Reward: 0.0, Epsilon: 0.6149, Total Step: 49\n",
            "Episode: 99/100, Total Reward: 0.0, Epsilon: 0.6119, Total Step: 49\n",
            "Episode: 100/100, Total Reward: 0.0, Epsilon: 0.6088, Total Step: 49\n",
            "TESTING 12: total_reward 8.0/12 \t (reward: 0.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=7,includeDoors=1,numDistractorItems=0, 2\n",
            "Episode: 1/100, Total Reward: 0.0, Epsilon: 1.0000, Total Step: 49\n",
            "Episode: 2/100, Total Reward: 0.0, Epsilon: 0.9950, Total Step: 49\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 0.0, Epsilon: 0.9851, Total Step: 49\n",
            "Episode: 5/100, Total Reward: 0.0, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 0.0, Epsilon: 0.9752, Total Step: 49\n",
            "Episode: 7/100, Total Reward: 0.0, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 1.0, Epsilon: 0.9655, Total Step: 38\n",
            "Episode: 9/100, Total Reward: 0.0, Epsilon: 0.9607, Total Step: 49\n",
            "Episode: 10/100, Total Reward: 0.0, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 0.0, Epsilon: 0.9511, Total Step: 49\n",
            "Episode: 12/100, Total Reward: 0.0, Epsilon: 0.9464, Total Step: 49\n",
            "Episode: 13/100, Total Reward: 0.0, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 1.0, Epsilon: 0.9369, Total Step: 38\n",
            "Episode: 15/100, Total Reward: 1.0, Epsilon: 0.9322, Total Step: 6\n",
            "Episode: 16/100, Total Reward: 0.0, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 0.0, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 1.0, Epsilon: 0.9183, Total Step: 8\n",
            "Episode: 19/100, Total Reward: 0.0, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 0.0, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 0.0, Epsilon: 0.9046, Total Step: 49\n",
            "Episode: 22/100, Total Reward: 0.0, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 0.0, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 1.0, Epsilon: 0.8911, Total Step: 17\n",
            "Episode: 25/100, Total Reward: 0.0, Epsilon: 0.8867, Total Step: 49\n",
            "Episode: 26/100, Total Reward: 0.0, Epsilon: 0.8822, Total Step: 49\n",
            "Episode: 27/100, Total Reward: 0.0, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 1.0, Epsilon: 0.8734, Total Step: 17\n",
            "Episode: 29/100, Total Reward: 1.0, Epsilon: 0.8691, Total Step: 15\n",
            "Episode: 30/100, Total Reward: 0.0, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 0.0, Epsilon: 0.8604, Total Step: 49\n",
            "Episode: 32/100, Total Reward: 0.0, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 1.0, Epsilon: 0.8518, Total Step: 21\n",
            "Episode: 34/100, Total Reward: 0.0, Epsilon: 0.8475, Total Step: 49\n",
            "Episode: 35/100, Total Reward: 0.0, Epsilon: 0.8433, Total Step: 49\n",
            "Episode: 36/100, Total Reward: 1.0, Epsilon: 0.8391, Total Step: 10\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 5\n",
            "Episode: 38/100, Total Reward: 0.0, Epsilon: 0.8307, Total Step: 49\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 16\n",
            "Episode: 40/100, Total Reward: 1.0, Epsilon: 0.8224, Total Step: 22\n",
            "Episode: 41/100, Total Reward: 0.0, Epsilon: 0.8183, Total Step: 49\n",
            "Episode: 42/100, Total Reward: 0.0, Epsilon: 0.8142, Total Step: 49\n",
            "Episode: 43/100, Total Reward: 1.0, Epsilon: 0.8102, Total Step: 34\n",
            "Episode: 44/100, Total Reward: 1.0, Epsilon: 0.8061, Total Step: 12\n",
            "Episode: 45/100, Total Reward: 0.0, Epsilon: 0.8021, Total Step: 49\n",
            "Episode: 46/100, Total Reward: 1.0, Epsilon: 0.7981, Total Step: 17\n",
            "Episode: 47/100, Total Reward: 1.0, Epsilon: 0.7941, Total Step: 22\n",
            "Episode: 48/100, Total Reward: 0.0, Epsilon: 0.7901, Total Step: 49\n",
            "Episode: 49/100, Total Reward: 0.0, Epsilon: 0.7862, Total Step: 49\n",
            "Episode: 50/100, Total Reward: 0.0, Epsilon: 0.7822, Total Step: 49\n",
            "Episode: 51/100, Total Reward: 1.0, Epsilon: 0.7783, Total Step: 5\n",
            "Episode: 52/100, Total Reward: 0.0, Epsilon: 0.7744, Total Step: 49\n",
            "Episode: 53/100, Total Reward: 1.0, Epsilon: 0.7705, Total Step: 29\n",
            "Episode: 54/100, Total Reward: 1.0, Epsilon: 0.7667, Total Step: 45\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 11\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 13\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 3\n",
            "Episode: 58/100, Total Reward: 1.0, Epsilon: 0.7515, Total Step: 6\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 9\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 8\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 9\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 20\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 12\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 23\n",
            "Episode: 65/100, Total Reward: 0.0, Epsilon: 0.7256, Total Step: 49\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 6\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 34\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 14\n",
            "Episode: 69/100, Total Reward: 0.0, Epsilon: 0.7112, Total Step: 49\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 13\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 5\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 13\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 11\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 6\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 15\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 7\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 17\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 15\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 11\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 7\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 18\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 13\n",
            "Episode: 83/100, Total Reward: 0.0, Epsilon: 0.6630, Total Step: 49\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 12\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 7\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 9\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 46\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 8\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 15\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 12\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 7\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 11\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 9\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 20\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 7\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 6\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 4\n",
            "Episode: 98/100, Total Reward: 0.0, Epsilon: 0.6149, Total Step: 49\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 6\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 12\n",
            "TESTING 13: total_reward 9.0/13 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=7,includeDoors=1,numDistractorItems=0, 3\n",
            "Episode: 1/100, Total Reward: 1.0, Epsilon: 1.0000, Total Step: 17\n",
            "Episode: 2/100, Total Reward: 1.0, Epsilon: 0.9950, Total Step: 21\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 1.0, Epsilon: 0.9851, Total Step: 16\n",
            "Episode: 5/100, Total Reward: 0.0, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 1.0, Epsilon: 0.9752, Total Step: 4\n",
            "Episode: 7/100, Total Reward: 1.0, Epsilon: 0.9704, Total Step: 25\n",
            "Episode: 8/100, Total Reward: 1.0, Epsilon: 0.9655, Total Step: 16\n",
            "Episode: 9/100, Total Reward: 0.0, Epsilon: 0.9607, Total Step: 49\n",
            "Episode: 10/100, Total Reward: 1.0, Epsilon: 0.9559, Total Step: 5\n",
            "Episode: 11/100, Total Reward: 0.0, Epsilon: 0.9511, Total Step: 49\n",
            "Episode: 12/100, Total Reward: 0.0, Epsilon: 0.9464, Total Step: 49\n",
            "Episode: 13/100, Total Reward: 0.0, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 0.0, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 1.0, Epsilon: 0.9322, Total Step: 17\n",
            "Episode: 16/100, Total Reward: 1.0, Epsilon: 0.9276, Total Step: 4\n",
            "Episode: 17/100, Total Reward: 1.0, Epsilon: 0.9229, Total Step: 28\n",
            "Episode: 18/100, Total Reward: 1.0, Epsilon: 0.9183, Total Step: 3\n",
            "Episode: 19/100, Total Reward: 0.0, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 1.0, Epsilon: 0.9092, Total Step: 20\n",
            "Episode: 21/100, Total Reward: 1.0, Epsilon: 0.9046, Total Step: 19\n",
            "Episode: 22/100, Total Reward: 0.0, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 1.0, Epsilon: 0.8956, Total Step: 18\n",
            "Episode: 24/100, Total Reward: 1.0, Epsilon: 0.8911, Total Step: 11\n",
            "Episode: 25/100, Total Reward: 1.0, Epsilon: 0.8867, Total Step: 17\n",
            "Episode: 26/100, Total Reward: 1.0, Epsilon: 0.8822, Total Step: 44\n",
            "Episode: 27/100, Total Reward: 1.0, Epsilon: 0.8778, Total Step: 43\n",
            "Episode: 28/100, Total Reward: 1.0, Epsilon: 0.8734, Total Step: 5\n",
            "Episode: 29/100, Total Reward: 1.0, Epsilon: 0.8691, Total Step: 33\n",
            "Episode: 30/100, Total Reward: 1.0, Epsilon: 0.8647, Total Step: 28\n",
            "Episode: 31/100, Total Reward: 0.0, Epsilon: 0.8604, Total Step: 49\n",
            "Episode: 32/100, Total Reward: 0.0, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 0.0, Epsilon: 0.8518, Total Step: 49\n",
            "Episode: 34/100, Total Reward: 1.0, Epsilon: 0.8475, Total Step: 11\n",
            "Episode: 35/100, Total Reward: 0.0, Epsilon: 0.8433, Total Step: 49\n",
            "Episode: 36/100, Total Reward: 1.0, Epsilon: 0.8391, Total Step: 41\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 2\n",
            "Episode: 38/100, Total Reward: 0.0, Epsilon: 0.8307, Total Step: 49\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 22\n",
            "Episode: 40/100, Total Reward: 1.0, Epsilon: 0.8224, Total Step: 12\n",
            "Episode: 41/100, Total Reward: 1.0, Epsilon: 0.8183, Total Step: 22\n",
            "Episode: 42/100, Total Reward: 1.0, Epsilon: 0.8142, Total Step: 22\n",
            "Episode: 43/100, Total Reward: 1.0, Epsilon: 0.8102, Total Step: 8\n",
            "Episode: 44/100, Total Reward: 0.0, Epsilon: 0.8061, Total Step: 49\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 32\n",
            "Episode: 46/100, Total Reward: 1.0, Epsilon: 0.7981, Total Step: 12\n",
            "Episode: 47/100, Total Reward: 1.0, Epsilon: 0.7941, Total Step: 49\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 11\n",
            "Episode: 49/100, Total Reward: 1.0, Epsilon: 0.7862, Total Step: 19\n",
            "Episode: 50/100, Total Reward: 1.0, Epsilon: 0.7822, Total Step: 35\n",
            "Episode: 51/100, Total Reward: 1.0, Epsilon: 0.7783, Total Step: 14\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 3\n",
            "Episode: 53/100, Total Reward: 1.0, Epsilon: 0.7705, Total Step: 40\n",
            "Episode: 54/100, Total Reward: 1.0, Epsilon: 0.7667, Total Step: 8\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 5\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 12\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 27\n",
            "Episode: 58/100, Total Reward: 1.0, Epsilon: 0.7515, Total Step: 45\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 24\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 20\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 22\n",
            "Episode: 62/100, Total Reward: 0.0, Epsilon: 0.7366, Total Step: 49\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 27\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 14\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 8\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 20\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 24\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 2\n",
            "Episode: 69/100, Total Reward: 0.0, Epsilon: 0.7112, Total Step: 49\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 18\n",
            "Episode: 71/100, Total Reward: 0.0, Epsilon: 0.7041, Total Step: 49\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 2\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 32\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 6\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 5\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 10\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 5\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 15\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 5\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 4\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 9\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 7\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 34\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 5\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 5\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 2\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 2\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 9\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 15\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 9\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 5\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 3\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 4\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 5\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 13\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 7\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 4\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 10\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 5\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 5\n",
            "TESTING 14: total_reward 10.0/14 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=7,includeDoors=1,numDistractorItems=0, 4\n",
            "Episode: 1/100, Total Reward: 0.0, Epsilon: 1.0000, Total Step: 49\n",
            "Episode: 2/100, Total Reward: 1.0, Epsilon: 0.9950, Total Step: 49\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 0.0, Epsilon: 0.9851, Total Step: 49\n",
            "Episode: 5/100, Total Reward: 0.0, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 1.0, Epsilon: 0.9752, Total Step: 8\n",
            "Episode: 7/100, Total Reward: 0.0, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 1.0, Epsilon: 0.9655, Total Step: 22\n",
            "Episode: 9/100, Total Reward: 1.0, Epsilon: 0.9607, Total Step: 28\n",
            "Episode: 10/100, Total Reward: 1.0, Epsilon: 0.9559, Total Step: 46\n",
            "Episode: 11/100, Total Reward: 0.0, Epsilon: 0.9511, Total Step: 49\n",
            "Episode: 12/100, Total Reward: 1.0, Epsilon: 0.9464, Total Step: 43\n",
            "Episode: 13/100, Total Reward: 0.0, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 0.0, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 1.0, Epsilon: 0.9322, Total Step: 48\n",
            "Episode: 16/100, Total Reward: 0.0, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 0.0, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 0.0, Epsilon: 0.9183, Total Step: 49\n",
            "Episode: 19/100, Total Reward: 0.0, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 0.0, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 0.0, Epsilon: 0.9046, Total Step: 49\n",
            "Episode: 22/100, Total Reward: 0.0, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 0.0, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 0.0, Epsilon: 0.8911, Total Step: 49\n",
            "Episode: 25/100, Total Reward: 0.0, Epsilon: 0.8867, Total Step: 49\n",
            "Episode: 26/100, Total Reward: 1.0, Epsilon: 0.8822, Total Step: 8\n",
            "Episode: 27/100, Total Reward: 0.0, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 0.0, Epsilon: 0.8734, Total Step: 49\n",
            "Episode: 29/100, Total Reward: 0.0, Epsilon: 0.8691, Total Step: 49\n",
            "Episode: 30/100, Total Reward: 0.0, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 1.0, Epsilon: 0.8604, Total Step: 44\n",
            "Episode: 32/100, Total Reward: 1.0, Epsilon: 0.8561, Total Step: 31\n",
            "Episode: 33/100, Total Reward: 0.0, Epsilon: 0.8518, Total Step: 49\n",
            "Episode: 34/100, Total Reward: 0.0, Epsilon: 0.8475, Total Step: 49\n",
            "Episode: 35/100, Total Reward: 1.0, Epsilon: 0.8433, Total Step: 10\n",
            "Episode: 36/100, Total Reward: 1.0, Epsilon: 0.8391, Total Step: 36\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 13\n",
            "Episode: 38/100, Total Reward: 1.0, Epsilon: 0.8307, Total Step: 11\n",
            "Episode: 39/100, Total Reward: 0.0, Epsilon: 0.8266, Total Step: 49\n",
            "Episode: 40/100, Total Reward: 1.0, Epsilon: 0.8224, Total Step: 21\n",
            "Episode: 41/100, Total Reward: 1.0, Epsilon: 0.8183, Total Step: 47\n",
            "Episode: 42/100, Total Reward: 1.0, Epsilon: 0.8142, Total Step: 28\n",
            "Episode: 43/100, Total Reward: 0.0, Epsilon: 0.8102, Total Step: 49\n",
            "Episode: 44/100, Total Reward: 1.0, Epsilon: 0.8061, Total Step: 31\n",
            "Episode: 45/100, Total Reward: 0.0, Epsilon: 0.8021, Total Step: 49\n",
            "Episode: 46/100, Total Reward: 1.0, Epsilon: 0.7981, Total Step: 21\n",
            "Episode: 47/100, Total Reward: 1.0, Epsilon: 0.7941, Total Step: 47\n",
            "Episode: 48/100, Total Reward: 0.0, Epsilon: 0.7901, Total Step: 49\n",
            "Episode: 49/100, Total Reward: 0.0, Epsilon: 0.7862, Total Step: 49\n",
            "Episode: 50/100, Total Reward: 0.0, Epsilon: 0.7822, Total Step: 49\n",
            "Episode: 51/100, Total Reward: 0.0, Epsilon: 0.7783, Total Step: 49\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 8\n",
            "Episode: 53/100, Total Reward: 1.0, Epsilon: 0.7705, Total Step: 12\n",
            "Episode: 54/100, Total Reward: 0.0, Epsilon: 0.7667, Total Step: 49\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 16\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 21\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 22\n",
            "Episode: 58/100, Total Reward: 1.0, Epsilon: 0.7515, Total Step: 16\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 9\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 17\n",
            "Episode: 61/100, Total Reward: 0.0, Epsilon: 0.7403, Total Step: 49\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 8\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 4\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 12\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 8\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 23\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 8\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 34\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 30\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 17\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 9\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 19\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 15\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 9\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 4\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 26\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 8\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 5\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 30\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 9\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 7\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 7\n",
            "Episode: 83/100, Total Reward: 0.0, Epsilon: 0.6630, Total Step: 49\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 6\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 3\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 11\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 10\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 4\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 19\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 20\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 15\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 15\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 5\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 9\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 6\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 16\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 6\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 7\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 11\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 4\n",
            "TESTING 15: total_reward 11.0/15 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=10,includeDoors=1,numDistractorItems=0, 0\n",
            "Episode: 1/100, Total Reward: 1.0, Epsilon: 1.0000, Total Step: 0\n",
            "Episode: 2/100, Total Reward: 0.0, Epsilon: 0.9950, Total Step: 49\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 1.0, Epsilon: 0.9851, Total Step: 8\n",
            "Episode: 5/100, Total Reward: 1.0, Epsilon: 0.9801, Total Step: 11\n",
            "Episode: 6/100, Total Reward: 1.0, Epsilon: 0.9752, Total Step: 8\n",
            "Episode: 7/100, Total Reward: 1.0, Epsilon: 0.9704, Total Step: 37\n",
            "Episode: 8/100, Total Reward: 1.0, Epsilon: 0.9655, Total Step: 17\n",
            "Episode: 9/100, Total Reward: 1.0, Epsilon: 0.9607, Total Step: 9\n",
            "Episode: 10/100, Total Reward: 1.0, Epsilon: 0.9559, Total Step: 15\n",
            "Episode: 11/100, Total Reward: 1.0, Epsilon: 0.9511, Total Step: 39\n",
            "Episode: 12/100, Total Reward: 1.0, Epsilon: 0.9464, Total Step: 20\n",
            "Episode: 13/100, Total Reward: 1.0, Epsilon: 0.9416, Total Step: 2\n",
            "Episode: 14/100, Total Reward: 1.0, Epsilon: 0.9369, Total Step: 47\n",
            "Episode: 15/100, Total Reward: 1.0, Epsilon: 0.9322, Total Step: 1\n",
            "Episode: 16/100, Total Reward: 1.0, Epsilon: 0.9276, Total Step: 3\n",
            "Episode: 17/100, Total Reward: 1.0, Epsilon: 0.9229, Total Step: 1\n",
            "Episode: 18/100, Total Reward: 1.0, Epsilon: 0.9183, Total Step: 43\n",
            "Episode: 19/100, Total Reward: 1.0, Epsilon: 0.9137, Total Step: 15\n",
            "Episode: 20/100, Total Reward: 1.0, Epsilon: 0.9092, Total Step: 20\n",
            "Episode: 21/100, Total Reward: 1.0, Epsilon: 0.9046, Total Step: 10\n",
            "Episode: 22/100, Total Reward: 1.0, Epsilon: 0.9001, Total Step: 0\n",
            "Episode: 23/100, Total Reward: 1.0, Epsilon: 0.8956, Total Step: 4\n",
            "Episode: 24/100, Total Reward: 1.0, Epsilon: 0.8911, Total Step: 2\n",
            "Episode: 25/100, Total Reward: 1.0, Epsilon: 0.8867, Total Step: 6\n",
            "Episode: 26/100, Total Reward: 0.0, Epsilon: 0.8822, Total Step: 49\n",
            "Episode: 27/100, Total Reward: 1.0, Epsilon: 0.8778, Total Step: 3\n",
            "Episode: 28/100, Total Reward: 1.0, Epsilon: 0.8734, Total Step: 11\n",
            "Episode: 29/100, Total Reward: 1.0, Epsilon: 0.8691, Total Step: 29\n",
            "Episode: 30/100, Total Reward: 1.0, Epsilon: 0.8647, Total Step: 24\n",
            "Episode: 31/100, Total Reward: 0.0, Epsilon: 0.8604, Total Step: 49\n",
            "Episode: 32/100, Total Reward: 0.0, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 1.0, Epsilon: 0.8518, Total Step: 1\n",
            "Episode: 34/100, Total Reward: 1.0, Epsilon: 0.8475, Total Step: 2\n",
            "Episode: 35/100, Total Reward: 1.0, Epsilon: 0.8433, Total Step: 2\n",
            "Episode: 36/100, Total Reward: 1.0, Epsilon: 0.8391, Total Step: 3\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 47\n",
            "Episode: 38/100, Total Reward: 1.0, Epsilon: 0.8307, Total Step: 1\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 1\n",
            "Episode: 40/100, Total Reward: 1.0, Epsilon: 0.8224, Total Step: 0\n",
            "Episode: 41/100, Total Reward: 0.0, Epsilon: 0.8183, Total Step: 49\n",
            "Episode: 42/100, Total Reward: 1.0, Epsilon: 0.8142, Total Step: 14\n",
            "Episode: 43/100, Total Reward: 1.0, Epsilon: 0.8102, Total Step: 3\n",
            "Episode: 44/100, Total Reward: 1.0, Epsilon: 0.8061, Total Step: 2\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 0\n",
            "Episode: 46/100, Total Reward: 1.0, Epsilon: 0.7981, Total Step: 43\n",
            "Episode: 47/100, Total Reward: 1.0, Epsilon: 0.7941, Total Step: 0\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 5\n",
            "Episode: 49/100, Total Reward: 1.0, Epsilon: 0.7862, Total Step: 0\n",
            "Episode: 50/100, Total Reward: 1.0, Epsilon: 0.7822, Total Step: 2\n",
            "Episode: 51/100, Total Reward: 1.0, Epsilon: 0.7783, Total Step: 36\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 2\n",
            "Episode: 53/100, Total Reward: 1.0, Epsilon: 0.7705, Total Step: 4\n",
            "Episode: 54/100, Total Reward: 1.0, Epsilon: 0.7667, Total Step: 1\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 36\n",
            "Episode: 56/100, Total Reward: 0.0, Epsilon: 0.7590, Total Step: 49\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 22\n",
            "Episode: 58/100, Total Reward: 1.0, Epsilon: 0.7515, Total Step: 2\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 1\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 1\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 0\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 5\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 20\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 4\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 12\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 0\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 1\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 3\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 9\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 30\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 0\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 0\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 2\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 0\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 1\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 2\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 0\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 2\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 14\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 1\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 20\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 16\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 2\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 8\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 3\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 11\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 1\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 1\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 1\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 0\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 0\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 3\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 1\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 0\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 0\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 0\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 5\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 0\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 0\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 3\n",
            "TESTING 16: total_reward 12.0/16 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=10,includeDoors=1,numDistractorItems=0, 1\n",
            "Episode: 1/100, Total Reward: 0.0, Epsilon: 1.0000, Total Step: 49\n",
            "Episode: 2/100, Total Reward: 1.0, Epsilon: 0.9950, Total Step: 6\n",
            "Episode: 3/100, Total Reward: 1.0, Epsilon: 0.9900, Total Step: 43\n",
            "Episode: 4/100, Total Reward: 0.0, Epsilon: 0.9851, Total Step: 49\n",
            "Episode: 5/100, Total Reward: 1.0, Epsilon: 0.9801, Total Step: 26\n",
            "Episode: 6/100, Total Reward: 1.0, Epsilon: 0.9752, Total Step: 39\n",
            "Episode: 7/100, Total Reward: 1.0, Epsilon: 0.9704, Total Step: 30\n",
            "Episode: 8/100, Total Reward: 1.0, Epsilon: 0.9655, Total Step: 20\n",
            "Episode: 9/100, Total Reward: 1.0, Epsilon: 0.9607, Total Step: 13\n",
            "Episode: 10/100, Total Reward: 0.0, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 1.0, Epsilon: 0.9511, Total Step: 16\n",
            "Episode: 12/100, Total Reward: 1.0, Epsilon: 0.9464, Total Step: 17\n",
            "Episode: 13/100, Total Reward: 0.0, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 1.0, Epsilon: 0.9369, Total Step: 7\n",
            "Episode: 15/100, Total Reward: 1.0, Epsilon: 0.9322, Total Step: 24\n",
            "Episode: 16/100, Total Reward: 0.0, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 0.0, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 0.0, Epsilon: 0.9183, Total Step: 49\n",
            "Episode: 19/100, Total Reward: 0.0, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 1.0, Epsilon: 0.9092, Total Step: 40\n",
            "Episode: 21/100, Total Reward: 1.0, Epsilon: 0.9046, Total Step: 16\n",
            "Episode: 22/100, Total Reward: 0.0, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 0.0, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 1.0, Epsilon: 0.8911, Total Step: 29\n",
            "Episode: 25/100, Total Reward: 0.0, Epsilon: 0.8867, Total Step: 49\n",
            "Episode: 26/100, Total Reward: 0.0, Epsilon: 0.8822, Total Step: 49\n",
            "Episode: 27/100, Total Reward: 0.0, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 1.0, Epsilon: 0.8734, Total Step: 20\n",
            "Episode: 29/100, Total Reward: 0.0, Epsilon: 0.8691, Total Step: 49\n",
            "Episode: 30/100, Total Reward: 0.0, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 0.0, Epsilon: 0.8604, Total Step: 49\n",
            "Episode: 32/100, Total Reward: 1.0, Epsilon: 0.8561, Total Step: 23\n",
            "Episode: 33/100, Total Reward: 1.0, Epsilon: 0.8518, Total Step: 25\n",
            "Episode: 34/100, Total Reward: 1.0, Epsilon: 0.8475, Total Step: 46\n",
            "Episode: 35/100, Total Reward: 1.0, Epsilon: 0.8433, Total Step: 41\n",
            "Episode: 36/100, Total Reward: 1.0, Epsilon: 0.8391, Total Step: 10\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 26\n",
            "Episode: 38/100, Total Reward: 0.0, Epsilon: 0.8307, Total Step: 49\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 9\n",
            "Episode: 40/100, Total Reward: 1.0, Epsilon: 0.8224, Total Step: 41\n",
            "Episode: 41/100, Total Reward: 1.0, Epsilon: 0.8183, Total Step: 32\n",
            "Episode: 42/100, Total Reward: 0.0, Epsilon: 0.8142, Total Step: 49\n",
            "Episode: 43/100, Total Reward: 1.0, Epsilon: 0.8102, Total Step: 13\n",
            "Episode: 44/100, Total Reward: 0.0, Epsilon: 0.8061, Total Step: 49\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 37\n",
            "Episode: 46/100, Total Reward: 0.0, Epsilon: 0.7981, Total Step: 49\n",
            "Episode: 47/100, Total Reward: 1.0, Epsilon: 0.7941, Total Step: 16\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 12\n",
            "Episode: 49/100, Total Reward: 1.0, Epsilon: 0.7862, Total Step: 14\n",
            "Episode: 50/100, Total Reward: 1.0, Epsilon: 0.7822, Total Step: 38\n",
            "Episode: 51/100, Total Reward: 1.0, Epsilon: 0.7783, Total Step: 19\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 12\n",
            "Episode: 53/100, Total Reward: 0.0, Epsilon: 0.7705, Total Step: 49\n",
            "Episode: 54/100, Total Reward: 1.0, Epsilon: 0.7667, Total Step: 47\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 23\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 7\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 26\n",
            "Episode: 58/100, Total Reward: 1.0, Epsilon: 0.7515, Total Step: 7\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 16\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 20\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 21\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 11\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 25\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 22\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 20\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 26\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 46\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 34\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 20\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 33\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 9\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 22\n",
            "Episode: 73/100, Total Reward: 0.0, Epsilon: 0.6970, Total Step: 49\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 16\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 34\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 37\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 30\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 29\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 34\n",
            "Episode: 80/100, Total Reward: 0.0, Epsilon: 0.6730, Total Step: 49\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 21\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 7\n",
            "Episode: 83/100, Total Reward: 0.0, Epsilon: 0.6630, Total Step: 49\n",
            "Episode: 84/100, Total Reward: 0.0, Epsilon: 0.6597, Total Step: 49\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 14\n",
            "Episode: 86/100, Total Reward: 0.0, Epsilon: 0.6531, Total Step: 49\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 9\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 34\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 22\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 8\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 11\n",
            "Episode: 92/100, Total Reward: 0.0, Epsilon: 0.6337, Total Step: 49\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 6\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 11\n",
            "Episode: 95/100, Total Reward: 0.0, Epsilon: 0.6243, Total Step: 49\n",
            "Episode: 96/100, Total Reward: 0.0, Epsilon: 0.6211, Total Step: 49\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 26\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 12\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 43\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 16\n",
            "TESTING 17: total_reward 13.0/17 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=10,includeDoors=1,numDistractorItems=0, 2\n",
            "Episode: 1/100, Total Reward: 0.0, Epsilon: 1.0000, Total Step: 49\n",
            "Episode: 2/100, Total Reward: 1.0, Epsilon: 0.9950, Total Step: 39\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 0.0, Epsilon: 0.9851, Total Step: 49\n",
            "Episode: 5/100, Total Reward: 0.0, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 1.0, Epsilon: 0.9752, Total Step: 33\n",
            "Episode: 7/100, Total Reward: 0.0, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 1.0, Epsilon: 0.9655, Total Step: 47\n",
            "Episode: 9/100, Total Reward: 1.0, Epsilon: 0.9607, Total Step: 21\n",
            "Episode: 10/100, Total Reward: 0.0, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 1.0, Epsilon: 0.9511, Total Step: 42\n",
            "Episode: 12/100, Total Reward: 1.0, Epsilon: 0.9464, Total Step: 49\n",
            "Episode: 13/100, Total Reward: 1.0, Epsilon: 0.9416, Total Step: 31\n",
            "Episode: 14/100, Total Reward: 0.0, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 0.0, Epsilon: 0.9322, Total Step: 49\n",
            "Episode: 16/100, Total Reward: 1.0, Epsilon: 0.9276, Total Step: 26\n",
            "Episode: 17/100, Total Reward: 0.0, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 0.0, Epsilon: 0.9183, Total Step: 49\n",
            "Episode: 19/100, Total Reward: 0.0, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 1.0, Epsilon: 0.9092, Total Step: 47\n",
            "Episode: 21/100, Total Reward: 0.0, Epsilon: 0.9046, Total Step: 49\n",
            "Episode: 22/100, Total Reward: 0.0, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 0.0, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 0.0, Epsilon: 0.8911, Total Step: 49\n",
            "Episode: 25/100, Total Reward: 0.0, Epsilon: 0.8867, Total Step: 49\n",
            "Episode: 26/100, Total Reward: 1.0, Epsilon: 0.8822, Total Step: 39\n",
            "Episode: 27/100, Total Reward: 0.0, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 1.0, Epsilon: 0.8734, Total Step: 29\n",
            "Episode: 29/100, Total Reward: 0.0, Epsilon: 0.8691, Total Step: 49\n",
            "Episode: 30/100, Total Reward: 0.0, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 0.0, Epsilon: 0.8604, Total Step: 49\n",
            "Episode: 32/100, Total Reward: 1.0, Epsilon: 0.8561, Total Step: 15\n",
            "Episode: 33/100, Total Reward: 0.0, Epsilon: 0.8518, Total Step: 49\n",
            "Episode: 34/100, Total Reward: 1.0, Epsilon: 0.8475, Total Step: 21\n",
            "Episode: 35/100, Total Reward: 0.0, Epsilon: 0.8433, Total Step: 49\n",
            "Episode: 36/100, Total Reward: 1.0, Epsilon: 0.8391, Total Step: 31\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 15\n",
            "Episode: 38/100, Total Reward: 1.0, Epsilon: 0.8307, Total Step: 9\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 33\n",
            "Episode: 40/100, Total Reward: 1.0, Epsilon: 0.8224, Total Step: 5\n",
            "Episode: 41/100, Total Reward: 1.0, Epsilon: 0.8183, Total Step: 27\n",
            "Episode: 42/100, Total Reward: 1.0, Epsilon: 0.8142, Total Step: 27\n",
            "Episode: 43/100, Total Reward: 1.0, Epsilon: 0.8102, Total Step: 46\n",
            "Episode: 44/100, Total Reward: 0.0, Epsilon: 0.8061, Total Step: 49\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 26\n",
            "Episode: 46/100, Total Reward: 1.0, Epsilon: 0.7981, Total Step: 22\n",
            "Episode: 47/100, Total Reward: 1.0, Epsilon: 0.7941, Total Step: 17\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 7\n",
            "Episode: 49/100, Total Reward: 1.0, Epsilon: 0.7862, Total Step: 22\n",
            "Episode: 50/100, Total Reward: 1.0, Epsilon: 0.7822, Total Step: 44\n",
            "Episode: 51/100, Total Reward: 1.0, Epsilon: 0.7783, Total Step: 42\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 8\n",
            "Episode: 53/100, Total Reward: 1.0, Epsilon: 0.7705, Total Step: 17\n",
            "Episode: 54/100, Total Reward: 1.0, Epsilon: 0.7667, Total Step: 6\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 18\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 10\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 14\n",
            "Episode: 58/100, Total Reward: 1.0, Epsilon: 0.7515, Total Step: 42\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 10\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 38\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 25\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 4\n",
            "Episode: 63/100, Total Reward: 0.0, Epsilon: 0.7329, Total Step: 49\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 15\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 9\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 36\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 8\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 12\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 9\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 6\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 6\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 16\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 11\n",
            "Episode: 74/100, Total Reward: 0.0, Epsilon: 0.6936, Total Step: 49\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 7\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 12\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 13\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 10\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 5\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 34\n",
            "Episode: 81/100, Total Reward: 0.0, Epsilon: 0.6696, Total Step: 49\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 29\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 34\n",
            "Episode: 84/100, Total Reward: 0.0, Epsilon: 0.6597, Total Step: 49\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 15\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 21\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 4\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 4\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 9\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 10\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 21\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 9\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 9\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 9\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 4\n",
            "Episode: 96/100, Total Reward: 0.0, Epsilon: 0.6211, Total Step: 49\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 11\n",
            "Episode: 98/100, Total Reward: 0.0, Epsilon: 0.6149, Total Step: 49\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 10\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 13\n",
            "TESTING 18: total_reward 14.0/18 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=10,includeDoors=1,numDistractorItems=0, 3\n",
            "Episode: 1/100, Total Reward: 0.0, Epsilon: 1.0000, Total Step: 49\n",
            "Episode: 2/100, Total Reward: 0.0, Epsilon: 0.9950, Total Step: 49\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 0.0, Epsilon: 0.9851, Total Step: 49\n",
            "Episode: 5/100, Total Reward: 0.0, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 0.0, Epsilon: 0.9752, Total Step: 49\n",
            "Episode: 7/100, Total Reward: 0.0, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 0.0, Epsilon: 0.9655, Total Step: 49\n",
            "Episode: 9/100, Total Reward: 0.0, Epsilon: 0.9607, Total Step: 49\n",
            "Episode: 10/100, Total Reward: 0.0, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 0.0, Epsilon: 0.9511, Total Step: 49\n",
            "Episode: 12/100, Total Reward: 0.0, Epsilon: 0.9464, Total Step: 49\n",
            "Episode: 13/100, Total Reward: 0.0, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 0.0, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 0.0, Epsilon: 0.9322, Total Step: 49\n",
            "Episode: 16/100, Total Reward: 0.0, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 0.0, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 0.0, Epsilon: 0.9183, Total Step: 49\n",
            "Episode: 19/100, Total Reward: 0.0, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 0.0, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 0.0, Epsilon: 0.9046, Total Step: 49\n",
            "Episode: 22/100, Total Reward: 0.0, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 0.0, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 0.0, Epsilon: 0.8911, Total Step: 49\n",
            "Episode: 25/100, Total Reward: 0.0, Epsilon: 0.8867, Total Step: 49\n",
            "Episode: 26/100, Total Reward: 0.0, Epsilon: 0.8822, Total Step: 49\n",
            "Episode: 27/100, Total Reward: 0.0, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 0.0, Epsilon: 0.8734, Total Step: 49\n",
            "Episode: 29/100, Total Reward: 0.0, Epsilon: 0.8691, Total Step: 49\n",
            "Episode: 30/100, Total Reward: 0.0, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 0.0, Epsilon: 0.8604, Total Step: 49\n",
            "Episode: 32/100, Total Reward: 0.0, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 0.0, Epsilon: 0.8518, Total Step: 49\n",
            "Episode: 34/100, Total Reward: 0.0, Epsilon: 0.8475, Total Step: 49\n",
            "Episode: 35/100, Total Reward: 0.0, Epsilon: 0.8433, Total Step: 49\n",
            "Episode: 36/100, Total Reward: 1.0, Epsilon: 0.8391, Total Step: 17\n",
            "Episode: 37/100, Total Reward: 0.0, Epsilon: 0.8349, Total Step: 49\n",
            "Episode: 38/100, Total Reward: 0.0, Epsilon: 0.8307, Total Step: 49\n",
            "Episode: 39/100, Total Reward: 0.0, Epsilon: 0.8266, Total Step: 49\n",
            "Episode: 40/100, Total Reward: 0.0, Epsilon: 0.8224, Total Step: 49\n",
            "Episode: 41/100, Total Reward: 0.0, Epsilon: 0.8183, Total Step: 49\n",
            "Episode: 42/100, Total Reward: 0.0, Epsilon: 0.8142, Total Step: 49\n",
            "Episode: 43/100, Total Reward: 0.0, Epsilon: 0.8102, Total Step: 49\n",
            "Episode: 44/100, Total Reward: 1.0, Epsilon: 0.8061, Total Step: 43\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 44\n",
            "Episode: 46/100, Total Reward: 0.0, Epsilon: 0.7981, Total Step: 49\n",
            "Episode: 47/100, Total Reward: 1.0, Epsilon: 0.7941, Total Step: 30\n",
            "Episode: 48/100, Total Reward: 0.0, Epsilon: 0.7901, Total Step: 49\n",
            "Episode: 49/100, Total Reward: 0.0, Epsilon: 0.7862, Total Step: 49\n",
            "Episode: 50/100, Total Reward: 0.0, Epsilon: 0.7822, Total Step: 49\n",
            "Episode: 51/100, Total Reward: 0.0, Epsilon: 0.7783, Total Step: 49\n",
            "Episode: 52/100, Total Reward: 0.0, Epsilon: 0.7744, Total Step: 49\n",
            "Episode: 53/100, Total Reward: 0.0, Epsilon: 0.7705, Total Step: 49\n",
            "Episode: 54/100, Total Reward: 0.0, Epsilon: 0.7667, Total Step: 49\n",
            "Episode: 55/100, Total Reward: 0.0, Epsilon: 0.7629, Total Step: 49\n",
            "Episode: 56/100, Total Reward: 0.0, Epsilon: 0.7590, Total Step: 49\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 8\n",
            "Episode: 58/100, Total Reward: 0.0, Epsilon: 0.7515, Total Step: 49\n",
            "Episode: 59/100, Total Reward: 0.0, Epsilon: 0.7477, Total Step: 49\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 26\n",
            "Episode: 61/100, Total Reward: 0.0, Epsilon: 0.7403, Total Step: 49\n",
            "Episode: 62/100, Total Reward: 0.0, Epsilon: 0.7366, Total Step: 49\n",
            "Episode: 63/100, Total Reward: 0.0, Epsilon: 0.7329, Total Step: 49\n",
            "Episode: 64/100, Total Reward: 0.0, Epsilon: 0.7292, Total Step: 49\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 24\n",
            "Episode: 66/100, Total Reward: 0.0, Epsilon: 0.7219, Total Step: 49\n",
            "Episode: 67/100, Total Reward: 0.0, Epsilon: 0.7183, Total Step: 49\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 38\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 27\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 22\n",
            "Episode: 71/100, Total Reward: 0.0, Epsilon: 0.7041, Total Step: 49\n",
            "Episode: 72/100, Total Reward: 0.0, Epsilon: 0.7005, Total Step: 49\n",
            "Episode: 73/100, Total Reward: 0.0, Epsilon: 0.6970, Total Step: 49\n",
            "Episode: 74/100, Total Reward: 0.0, Epsilon: 0.6936, Total Step: 49\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 13\n",
            "Episode: 76/100, Total Reward: 0.0, Epsilon: 0.6866, Total Step: 49\n",
            "Episode: 77/100, Total Reward: 0.0, Epsilon: 0.6832, Total Step: 49\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 23\n",
            "Episode: 79/100, Total Reward: 0.0, Epsilon: 0.6764, Total Step: 49\n",
            "Episode: 80/100, Total Reward: 0.0, Epsilon: 0.6730, Total Step: 49\n",
            "Episode: 81/100, Total Reward: 0.0, Epsilon: 0.6696, Total Step: 49\n",
            "Episode: 82/100, Total Reward: 0.0, Epsilon: 0.6663, Total Step: 49\n",
            "Episode: 83/100, Total Reward: 0.0, Epsilon: 0.6630, Total Step: 49\n",
            "Episode: 84/100, Total Reward: 0.0, Epsilon: 0.6597, Total Step: 49\n",
            "Episode: 85/100, Total Reward: 0.0, Epsilon: 0.6564, Total Step: 49\n",
            "Episode: 86/100, Total Reward: 0.0, Epsilon: 0.6531, Total Step: 49\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 32\n",
            "Episode: 88/100, Total Reward: 0.0, Epsilon: 0.6466, Total Step: 49\n",
            "Episode: 89/100, Total Reward: 0.0, Epsilon: 0.6433, Total Step: 49\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 39\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 27\n",
            "Episode: 92/100, Total Reward: 0.0, Epsilon: 0.6337, Total Step: 49\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 23\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 12\n",
            "Episode: 95/100, Total Reward: 0.0, Epsilon: 0.6243, Total Step: 49\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 28\n",
            "Episode: 97/100, Total Reward: 0.0, Epsilon: 0.6180, Total Step: 49\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 36\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 30\n",
            "Episode: 100/100, Total Reward: 0.0, Epsilon: 0.6088, Total Step: 49\n",
            "TESTING 19: total_reward 14.0/19 \t (reward: 0.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=10,includeDoors=1,numDistractorItems=0, 4\n",
            "Episode: 1/100, Total Reward: 0.0, Epsilon: 1.0000, Total Step: 49\n",
            "Episode: 2/100, Total Reward: 0.0, Epsilon: 0.9950, Total Step: 49\n",
            "Episode: 3/100, Total Reward: 1.0, Epsilon: 0.9900, Total Step: 10\n",
            "Episode: 4/100, Total Reward: 0.0, Epsilon: 0.9851, Total Step: 49\n",
            "Episode: 5/100, Total Reward: 1.0, Epsilon: 0.9801, Total Step: 7\n",
            "Episode: 6/100, Total Reward: 1.0, Epsilon: 0.9752, Total Step: 30\n",
            "Episode: 7/100, Total Reward: 0.0, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 1.0, Epsilon: 0.9655, Total Step: 1\n",
            "Episode: 9/100, Total Reward: 1.0, Epsilon: 0.9607, Total Step: 13\n",
            "Episode: 10/100, Total Reward: 1.0, Epsilon: 0.9559, Total Step: 9\n",
            "Episode: 11/100, Total Reward: 1.0, Epsilon: 0.9511, Total Step: 36\n",
            "Episode: 12/100, Total Reward: 1.0, Epsilon: 0.9464, Total Step: 5\n",
            "Episode: 13/100, Total Reward: 1.0, Epsilon: 0.9416, Total Step: 35\n",
            "Episode: 14/100, Total Reward: 1.0, Epsilon: 0.9369, Total Step: 23\n",
            "Episode: 15/100, Total Reward: 1.0, Epsilon: 0.9322, Total Step: 5\n",
            "Episode: 16/100, Total Reward: 0.0, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 1.0, Epsilon: 0.9229, Total Step: 15\n",
            "Episode: 18/100, Total Reward: 1.0, Epsilon: 0.9183, Total Step: 18\n",
            "Episode: 19/100, Total Reward: 1.0, Epsilon: 0.9137, Total Step: 4\n",
            "Episode: 20/100, Total Reward: 1.0, Epsilon: 0.9092, Total Step: 1\n",
            "Episode: 21/100, Total Reward: 1.0, Epsilon: 0.9046, Total Step: 31\n",
            "Episode: 22/100, Total Reward: 0.0, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 1.0, Epsilon: 0.8956, Total Step: 26\n",
            "Episode: 24/100, Total Reward: 1.0, Epsilon: 0.8911, Total Step: 2\n",
            "Episode: 25/100, Total Reward: 1.0, Epsilon: 0.8867, Total Step: 2\n",
            "Episode: 26/100, Total Reward: 1.0, Epsilon: 0.8822, Total Step: 35\n",
            "Episode: 27/100, Total Reward: 0.0, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 1.0, Epsilon: 0.8734, Total Step: 1\n",
            "Episode: 29/100, Total Reward: 0.0, Epsilon: 0.8691, Total Step: 49\n",
            "Episode: 30/100, Total Reward: 0.0, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 1.0, Epsilon: 0.8604, Total Step: 1\n",
            "Episode: 32/100, Total Reward: 1.0, Epsilon: 0.8561, Total Step: 5\n",
            "Episode: 33/100, Total Reward: 1.0, Epsilon: 0.8518, Total Step: 9\n",
            "Episode: 34/100, Total Reward: 1.0, Epsilon: 0.8475, Total Step: 5\n",
            "Episode: 35/100, Total Reward: 1.0, Epsilon: 0.8433, Total Step: 11\n",
            "Episode: 36/100, Total Reward: 1.0, Epsilon: 0.8391, Total Step: 2\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 6\n",
            "Episode: 38/100, Total Reward: 1.0, Epsilon: 0.8307, Total Step: 7\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 1\n",
            "Episode: 40/100, Total Reward: 1.0, Epsilon: 0.8224, Total Step: 1\n",
            "Episode: 41/100, Total Reward: 1.0, Epsilon: 0.8183, Total Step: 5\n",
            "Episode: 42/100, Total Reward: 1.0, Epsilon: 0.8142, Total Step: 10\n",
            "Episode: 43/100, Total Reward: 1.0, Epsilon: 0.8102, Total Step: 0\n",
            "Episode: 44/100, Total Reward: 1.0, Epsilon: 0.8061, Total Step: 0\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 29\n",
            "Episode: 46/100, Total Reward: 1.0, Epsilon: 0.7981, Total Step: 25\n",
            "Episode: 47/100, Total Reward: 1.0, Epsilon: 0.7941, Total Step: 2\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 5\n",
            "Episode: 49/100, Total Reward: 1.0, Epsilon: 0.7862, Total Step: 1\n",
            "Episode: 50/100, Total Reward: 0.0, Epsilon: 0.7822, Total Step: 49\n",
            "Episode: 51/100, Total Reward: 1.0, Epsilon: 0.7783, Total Step: 11\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 2\n",
            "Episode: 53/100, Total Reward: 1.0, Epsilon: 0.7705, Total Step: 0\n",
            "Episode: 54/100, Total Reward: 1.0, Epsilon: 0.7667, Total Step: 0\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 0\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 39\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 9\n",
            "Episode: 58/100, Total Reward: 0.0, Epsilon: 0.7515, Total Step: 49\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 5\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 0\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 3\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 0\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 0\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 8\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 32\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 12\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 0\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 49\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 4\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 1\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 0\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 0\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 2\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 0\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 3\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 0\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 0\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 1\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 0\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 4\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 2\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 5\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 4\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 0\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 0\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 0\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 0\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 0\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 3\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 0\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 3\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 0\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 1\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 1\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 2\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 0\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 15\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 1\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 0\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 0\n",
            "TESTING 20: total_reward 15.0/20 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=5,maxDistanceApart=3,includeDoors=0,maxDistractorItemsPerLocation=0, 0\n",
            "Episode: 1/100, Total Reward: 0.5, Epsilon: 1.0000, Total Step: 49\n",
            "Episode: 2/100, Total Reward: 1.0, Epsilon: 0.9950, Total Step: 27\n",
            "Episode: 3/100, Total Reward: 0.5, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 1.0, Epsilon: 0.9851, Total Step: 40\n",
            "Episode: 5/100, Total Reward: 0.5, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 1.0, Epsilon: 0.9752, Total Step: 24\n",
            "Episode: 7/100, Total Reward: 1.0, Epsilon: 0.9704, Total Step: 24\n",
            "Episode: 8/100, Total Reward: 0.0, Epsilon: 0.9655, Total Step: 49\n",
            "Episode: 9/100, Total Reward: 0.0, Epsilon: 0.9607, Total Step: 49\n",
            "Episode: 10/100, Total Reward: 1.0, Epsilon: 0.9559, Total Step: 26\n",
            "Episode: 11/100, Total Reward: 0.0, Epsilon: 0.9511, Total Step: 49\n",
            "Episode: 12/100, Total Reward: 1.0, Epsilon: 0.9464, Total Step: 18\n",
            "Episode: 13/100, Total Reward: 0.5, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 0.5, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 1.0, Epsilon: 0.9322, Total Step: 33\n",
            "Episode: 16/100, Total Reward: 0.5, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 0.5, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 0.0, Epsilon: 0.9183, Total Step: 49\n",
            "Episode: 19/100, Total Reward: 1.0, Epsilon: 0.9137, Total Step: 18\n",
            "Episode: 20/100, Total Reward: 0.5, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 0.0, Epsilon: 0.9046, Total Step: 49\n",
            "Episode: 22/100, Total Reward: 0.5, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 1.0, Epsilon: 0.8956, Total Step: 34\n",
            "Episode: 24/100, Total Reward: 0.5, Epsilon: 0.8911, Total Step: 49\n",
            "Episode: 25/100, Total Reward: 1.0, Epsilon: 0.8867, Total Step: 23\n",
            "Episode: 26/100, Total Reward: 0.0, Epsilon: 0.8822, Total Step: 49\n",
            "Episode: 27/100, Total Reward: 0.5, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 0.5, Epsilon: 0.8734, Total Step: 49\n",
            "Episode: 29/100, Total Reward: 1.0, Epsilon: 0.8691, Total Step: 42\n",
            "Episode: 30/100, Total Reward: 0.5, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 0.0, Epsilon: 0.8604, Total Step: 49\n",
            "Episode: 32/100, Total Reward: 1.0, Epsilon: 0.8561, Total Step: 11\n",
            "Episode: 33/100, Total Reward: 1.0, Epsilon: 0.8518, Total Step: 38\n",
            "Episode: 34/100, Total Reward: 0.5, Epsilon: 0.8475, Total Step: 49\n",
            "Episode: 35/100, Total Reward: 0.0, Epsilon: 0.8433, Total Step: 49\n",
            "Episode: 36/100, Total Reward: 0.5, Epsilon: 0.8391, Total Step: 49\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 40\n",
            "Episode: 38/100, Total Reward: 1.0, Epsilon: 0.8307, Total Step: 33\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 34\n",
            "Episode: 40/100, Total Reward: 0.5, Epsilon: 0.8224, Total Step: 49\n",
            "Episode: 41/100, Total Reward: 1.0, Epsilon: 0.8183, Total Step: 15\n",
            "Episode: 42/100, Total Reward: 0.5, Epsilon: 0.8142, Total Step: 49\n",
            "Episode: 43/100, Total Reward: 1.0, Epsilon: 0.8102, Total Step: 45\n",
            "Episode: 44/100, Total Reward: 0.5, Epsilon: 0.8061, Total Step: 49\n",
            "Episode: 45/100, Total Reward: 0.5, Epsilon: 0.8021, Total Step: 49\n",
            "Episode: 46/100, Total Reward: 1.0, Epsilon: 0.7981, Total Step: 43\n",
            "Episode: 47/100, Total Reward: 0.5, Epsilon: 0.7941, Total Step: 49\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 27\n",
            "Episode: 49/100, Total Reward: 1.0, Epsilon: 0.7862, Total Step: 10\n",
            "Episode: 50/100, Total Reward: 1.0, Epsilon: 0.7822, Total Step: 30\n",
            "Episode: 51/100, Total Reward: 1.0, Epsilon: 0.7783, Total Step: 42\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 20\n",
            "Episode: 53/100, Total Reward: 0.5, Epsilon: 0.7705, Total Step: 49\n",
            "Episode: 54/100, Total Reward: 1.0, Epsilon: 0.7667, Total Step: 26\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 48\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 30\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 23\n",
            "Episode: 58/100, Total Reward: 0.0, Epsilon: 0.7515, Total Step: 49\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 26\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 18\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 26\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 14\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 38\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 47\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 17\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 22\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 18\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 19\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 23\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 18\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 29\n",
            "Episode: 72/100, Total Reward: 0.5, Epsilon: 0.7005, Total Step: 49\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 28\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 12\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 19\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 19\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 9\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 24\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 8\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 13\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 14\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 8\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 11\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 10\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 12\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 17\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 17\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 20\n",
            "Episode: 89/100, Total Reward: 0.5, Epsilon: 0.6433, Total Step: 49\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 19\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 16\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 10\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 10\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 24\n",
            "Episode: 95/100, Total Reward: 0.5, Epsilon: 0.6243, Total Step: 49\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 8\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 24\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 19\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 14\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 17\n",
            "TESTING 21: total_reward 16.0/21 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=5,maxDistanceApart=3,includeDoors=0,maxDistractorItemsPerLocation=0, 1\n",
            "Episode: 1/100, Total Reward: 1.0, Epsilon: 1.0000, Total Step: 35\n",
            "Episode: 2/100, Total Reward: 1.0, Epsilon: 0.9950, Total Step: 15\n",
            "Episode: 3/100, Total Reward: 1.0, Epsilon: 0.9900, Total Step: 44\n",
            "Episode: 4/100, Total Reward: 1.0, Epsilon: 0.9851, Total Step: 10\n",
            "Episode: 5/100, Total Reward: 0.5, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 1.0, Epsilon: 0.9752, Total Step: 19\n",
            "Episode: 7/100, Total Reward: 0.5, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 1.0, Epsilon: 0.9655, Total Step: 18\n",
            "Episode: 9/100, Total Reward: 0.0, Epsilon: 0.9607, Total Step: 49\n",
            "Episode: 10/100, Total Reward: 1.0, Epsilon: 0.9559, Total Step: 5\n",
            "Episode: 11/100, Total Reward: 1.0, Epsilon: 0.9511, Total Step: 49\n",
            "Episode: 12/100, Total Reward: 1.0, Epsilon: 0.9464, Total Step: 22\n",
            "Episode: 13/100, Total Reward: 0.5, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 1.0, Epsilon: 0.9369, Total Step: 47\n",
            "Episode: 15/100, Total Reward: 0.5, Epsilon: 0.9322, Total Step: 49\n",
            "Episode: 16/100, Total Reward: 0.5, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 1.0, Epsilon: 0.9229, Total Step: 35\n",
            "Episode: 18/100, Total Reward: 1.0, Epsilon: 0.9183, Total Step: 9\n",
            "Episode: 19/100, Total Reward: 0.5, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 1.0, Epsilon: 0.9092, Total Step: 14\n",
            "Episode: 21/100, Total Reward: 1.0, Epsilon: 0.9046, Total Step: 29\n",
            "Episode: 22/100, Total Reward: 1.0, Epsilon: 0.9001, Total Step: 24\n",
            "Episode: 23/100, Total Reward: 0.5, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 0.5, Epsilon: 0.8911, Total Step: 49\n",
            "Episode: 25/100, Total Reward: 1.0, Epsilon: 0.8867, Total Step: 22\n",
            "Episode: 26/100, Total Reward: 1.0, Epsilon: 0.8822, Total Step: 19\n",
            "Episode: 27/100, Total Reward: 0.5, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 1.0, Epsilon: 0.8734, Total Step: 34\n",
            "Episode: 29/100, Total Reward: 1.0, Epsilon: 0.8691, Total Step: 32\n",
            "Episode: 30/100, Total Reward: 1.0, Epsilon: 0.8647, Total Step: 19\n",
            "Episode: 31/100, Total Reward: 0.5, Epsilon: 0.8604, Total Step: 49\n",
            "Episode: 32/100, Total Reward: 0.5, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 0.5, Epsilon: 0.8518, Total Step: 49\n",
            "Episode: 34/100, Total Reward: 1.0, Epsilon: 0.8475, Total Step: 8\n",
            "Episode: 35/100, Total Reward: 1.0, Epsilon: 0.8433, Total Step: 25\n",
            "Episode: 36/100, Total Reward: 1.0, Epsilon: 0.8391, Total Step: 5\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 19\n",
            "Episode: 38/100, Total Reward: 1.0, Epsilon: 0.8307, Total Step: 30\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 34\n",
            "Episode: 40/100, Total Reward: 0.5, Epsilon: 0.8224, Total Step: 49\n",
            "Episode: 41/100, Total Reward: 1.0, Epsilon: 0.8183, Total Step: 13\n",
            "Episode: 42/100, Total Reward: 1.0, Epsilon: 0.8142, Total Step: 28\n",
            "Episode: 43/100, Total Reward: 1.0, Epsilon: 0.8102, Total Step: 8\n",
            "Episode: 44/100, Total Reward: 1.0, Epsilon: 0.8061, Total Step: 21\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 37\n",
            "Episode: 46/100, Total Reward: 1.0, Epsilon: 0.7981, Total Step: 16\n",
            "Episode: 47/100, Total Reward: 1.0, Epsilon: 0.7941, Total Step: 23\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 35\n",
            "Episode: 49/100, Total Reward: 1.0, Epsilon: 0.7862, Total Step: 12\n",
            "Episode: 50/100, Total Reward: 1.0, Epsilon: 0.7822, Total Step: 29\n",
            "Episode: 51/100, Total Reward: 1.0, Epsilon: 0.7783, Total Step: 12\n",
            "Episode: 52/100, Total Reward: 0.5, Epsilon: 0.7744, Total Step: 49\n",
            "Episode: 53/100, Total Reward: 1.0, Epsilon: 0.7705, Total Step: 11\n",
            "Episode: 54/100, Total Reward: 0.5, Epsilon: 0.7667, Total Step: 49\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 25\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 11\n",
            "Episode: 57/100, Total Reward: 0.5, Epsilon: 0.7553, Total Step: 49\n",
            "Episode: 58/100, Total Reward: 1.0, Epsilon: 0.7515, Total Step: 12\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 10\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 9\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 9\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 13\n",
            "Episode: 63/100, Total Reward: 0.0, Epsilon: 0.7329, Total Step: 49\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 11\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 45\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 16\n",
            "Episode: 67/100, Total Reward: 0.5, Epsilon: 0.7183, Total Step: 49\n",
            "Episode: 68/100, Total Reward: 0.5, Epsilon: 0.7147, Total Step: 49\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 6\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 4\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 14\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 10\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 8\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 17\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 7\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 14\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 16\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 5\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 6\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 5\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 4\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 14\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 5\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 12\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 15\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 18\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 21\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 29\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 28\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 5\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 6\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 12\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 7\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 6\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 8\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 12\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 4\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 5\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 5\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 7\n",
            "TESTING 22: total_reward 17.0/22 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=5,maxDistanceApart=3,includeDoors=0,maxDistractorItemsPerLocation=0, 2\n",
            "Episode: 1/100, Total Reward: 1.0, Epsilon: 1.0000, Total Step: 42\n",
            "Episode: 2/100, Total Reward: 0.0, Epsilon: 0.9950, Total Step: 49\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 0.0, Epsilon: 0.9851, Total Step: 49\n",
            "Episode: 5/100, Total Reward: 1.0, Epsilon: 0.9801, Total Step: 11\n",
            "Episode: 6/100, Total Reward: 0.5, Epsilon: 0.9752, Total Step: 49\n",
            "Episode: 7/100, Total Reward: 0.0, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 0.0, Epsilon: 0.9655, Total Step: 49\n",
            "Episode: 9/100, Total Reward: 1.0, Epsilon: 0.9607, Total Step: 29\n",
            "Episode: 10/100, Total Reward: 0.0, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 0.0, Epsilon: 0.9511, Total Step: 49\n",
            "Episode: 12/100, Total Reward: 0.5, Epsilon: 0.9464, Total Step: 49\n",
            "Episode: 13/100, Total Reward: 0.5, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 1.0, Epsilon: 0.9369, Total Step: 21\n",
            "Episode: 15/100, Total Reward: 0.0, Epsilon: 0.9322, Total Step: 49\n",
            "Episode: 16/100, Total Reward: 1.0, Epsilon: 0.9276, Total Step: 12\n",
            "Episode: 17/100, Total Reward: 0.0, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 0.5, Epsilon: 0.9183, Total Step: 49\n",
            "Episode: 19/100, Total Reward: 1.0, Epsilon: 0.9137, Total Step: 35\n",
            "Episode: 20/100, Total Reward: 0.5, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 1.0, Epsilon: 0.9046, Total Step: 49\n",
            "Episode: 22/100, Total Reward: 0.5, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 0.0, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 1.0, Epsilon: 0.8911, Total Step: 13\n",
            "Episode: 25/100, Total Reward: 1.0, Epsilon: 0.8867, Total Step: 31\n",
            "Episode: 26/100, Total Reward: 0.5, Epsilon: 0.8822, Total Step: 49\n",
            "Episode: 27/100, Total Reward: 1.0, Epsilon: 0.8778, Total Step: 14\n",
            "Episode: 28/100, Total Reward: 0.5, Epsilon: 0.8734, Total Step: 49\n",
            "Episode: 29/100, Total Reward: 1.0, Epsilon: 0.8691, Total Step: 42\n",
            "Episode: 30/100, Total Reward: 0.0, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 1.0, Epsilon: 0.8604, Total Step: 22\n",
            "Episode: 32/100, Total Reward: 1.0, Epsilon: 0.8561, Total Step: 41\n",
            "Episode: 33/100, Total Reward: 0.5, Epsilon: 0.8518, Total Step: 49\n",
            "Episode: 34/100, Total Reward: 1.0, Epsilon: 0.8475, Total Step: 24\n",
            "Episode: 35/100, Total Reward: 0.5, Epsilon: 0.8433, Total Step: 49\n",
            "Episode: 36/100, Total Reward: 1.0, Epsilon: 0.8391, Total Step: 27\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 29\n",
            "Episode: 38/100, Total Reward: 1.0, Epsilon: 0.8307, Total Step: 38\n",
            "Episode: 39/100, Total Reward: 0.5, Epsilon: 0.8266, Total Step: 49\n",
            "Episode: 40/100, Total Reward: 0.5, Epsilon: 0.8224, Total Step: 49\n",
            "Episode: 41/100, Total Reward: 0.5, Epsilon: 0.8183, Total Step: 49\n",
            "Episode: 42/100, Total Reward: 0.0, Epsilon: 0.8142, Total Step: 49\n",
            "Episode: 43/100, Total Reward: 0.5, Epsilon: 0.8102, Total Step: 49\n",
            "Episode: 44/100, Total Reward: 0.5, Epsilon: 0.8061, Total Step: 49\n",
            "Episode: 45/100, Total Reward: 0.0, Epsilon: 0.8021, Total Step: 49\n",
            "Episode: 46/100, Total Reward: 0.5, Epsilon: 0.7981, Total Step: 49\n",
            "Episode: 47/100, Total Reward: 1.0, Epsilon: 0.7941, Total Step: 42\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 26\n",
            "Episode: 49/100, Total Reward: 0.5, Epsilon: 0.7862, Total Step: 49\n",
            "Episode: 50/100, Total Reward: 0.0, Epsilon: 0.7822, Total Step: 49\n",
            "Episode: 51/100, Total Reward: 1.0, Epsilon: 0.7783, Total Step: 25\n",
            "Episode: 52/100, Total Reward: 0.5, Epsilon: 0.7744, Total Step: 49\n",
            "Episode: 53/100, Total Reward: 1.0, Epsilon: 0.7705, Total Step: 28\n",
            "Episode: 54/100, Total Reward: 1.0, Epsilon: 0.7667, Total Step: 27\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 12\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 28\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 15\n",
            "Episode: 58/100, Total Reward: 1.0, Epsilon: 0.7515, Total Step: 9\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 17\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 21\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 20\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 27\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 16\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 37\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 22\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 16\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 26\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 22\n",
            "Episode: 69/100, Total Reward: 0.5, Epsilon: 0.7112, Total Step: 49\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 13\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 38\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 9\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 14\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 7\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 9\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 19\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 9\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 42\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 30\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 5\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 35\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 17\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 14\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 35\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 21\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 8\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 17\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 14\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 7\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 19\n",
            "Episode: 91/100, Total Reward: 0.5, Epsilon: 0.6369, Total Step: 49\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 36\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 37\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 7\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 12\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 11\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 9\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 16\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 16\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 16\n",
            "TESTING 23: total_reward 18.0/23 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=5,maxDistanceApart=3,includeDoors=0,maxDistractorItemsPerLocation=0, 3\n",
            "Episode: 1/100, Total Reward: 0.5, Epsilon: 1.0000, Total Step: 49\n",
            "Episode: 2/100, Total Reward: 1.0, Epsilon: 0.9950, Total Step: 11\n",
            "Episode: 3/100, Total Reward: 1.0, Epsilon: 0.9900, Total Step: 22\n",
            "Episode: 4/100, Total Reward: 1.0, Epsilon: 0.9851, Total Step: 29\n",
            "Episode: 5/100, Total Reward: 1.0, Epsilon: 0.9801, Total Step: 23\n",
            "Episode: 6/100, Total Reward: 0.5, Epsilon: 0.9752, Total Step: 49\n",
            "Episode: 7/100, Total Reward: 1.0, Epsilon: 0.9704, Total Step: 45\n",
            "Episode: 8/100, Total Reward: 0.5, Epsilon: 0.9655, Total Step: 49\n",
            "Episode: 9/100, Total Reward: 0.5, Epsilon: 0.9607, Total Step: 49\n",
            "Episode: 10/100, Total Reward: 0.5, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 1.0, Epsilon: 0.9511, Total Step: 34\n",
            "Episode: 12/100, Total Reward: 1.0, Epsilon: 0.9464, Total Step: 38\n",
            "Episode: 13/100, Total Reward: 1.0, Epsilon: 0.9416, Total Step: 18\n",
            "Episode: 14/100, Total Reward: 1.0, Epsilon: 0.9369, Total Step: 48\n",
            "Episode: 15/100, Total Reward: 1.0, Epsilon: 0.9322, Total Step: 37\n",
            "Episode: 16/100, Total Reward: 1.0, Epsilon: 0.9276, Total Step: 22\n",
            "Episode: 17/100, Total Reward: 0.5, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 1.0, Epsilon: 0.9183, Total Step: 33\n",
            "Episode: 19/100, Total Reward: 1.0, Epsilon: 0.9137, Total Step: 22\n",
            "Episode: 20/100, Total Reward: 0.5, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 1.0, Epsilon: 0.9046, Total Step: 31\n",
            "Episode: 22/100, Total Reward: 1.0, Epsilon: 0.9001, Total Step: 14\n",
            "Episode: 23/100, Total Reward: 0.5, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 0.5, Epsilon: 0.8911, Total Step: 49\n",
            "Episode: 25/100, Total Reward: 0.0, Epsilon: 0.8867, Total Step: 49\n",
            "Episode: 26/100, Total Reward: 1.0, Epsilon: 0.8822, Total Step: 48\n",
            "Episode: 27/100, Total Reward: 0.5, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 1.0, Epsilon: 0.8734, Total Step: 23\n",
            "Episode: 29/100, Total Reward: 0.5, Epsilon: 0.8691, Total Step: 49\n",
            "Episode: 30/100, Total Reward: 1.0, Epsilon: 0.8647, Total Step: 42\n",
            "Episode: 31/100, Total Reward: 1.0, Epsilon: 0.8604, Total Step: 35\n",
            "Episode: 32/100, Total Reward: 1.0, Epsilon: 0.8561, Total Step: 47\n",
            "Episode: 33/100, Total Reward: 1.0, Epsilon: 0.8518, Total Step: 42\n",
            "Episode: 34/100, Total Reward: 0.5, Epsilon: 0.8475, Total Step: 49\n",
            "Episode: 35/100, Total Reward: 1.0, Epsilon: 0.8433, Total Step: 10\n",
            "Episode: 36/100, Total Reward: 0.5, Epsilon: 0.8391, Total Step: 49\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 18\n",
            "Episode: 38/100, Total Reward: 1.0, Epsilon: 0.8307, Total Step: 28\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 10\n",
            "Episode: 40/100, Total Reward: 0.5, Epsilon: 0.8224, Total Step: 49\n",
            "Episode: 41/100, Total Reward: 1.0, Epsilon: 0.8183, Total Step: 42\n",
            "Episode: 42/100, Total Reward: 1.0, Epsilon: 0.8142, Total Step: 14\n",
            "Episode: 43/100, Total Reward: 1.0, Epsilon: 0.8102, Total Step: 44\n",
            "Episode: 44/100, Total Reward: 1.0, Epsilon: 0.8061, Total Step: 22\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 22\n",
            "Episode: 46/100, Total Reward: 1.0, Epsilon: 0.7981, Total Step: 14\n",
            "Episode: 47/100, Total Reward: 1.0, Epsilon: 0.7941, Total Step: 46\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 22\n",
            "Episode: 49/100, Total Reward: 1.0, Epsilon: 0.7862, Total Step: 29\n",
            "Episode: 50/100, Total Reward: 1.0, Epsilon: 0.7822, Total Step: 15\n",
            "Episode: 51/100, Total Reward: 1.0, Epsilon: 0.7783, Total Step: 17\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 38\n",
            "Episode: 53/100, Total Reward: 1.0, Epsilon: 0.7705, Total Step: 16\n",
            "Episode: 54/100, Total Reward: 1.0, Epsilon: 0.7667, Total Step: 45\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 8\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 19\n",
            "Episode: 57/100, Total Reward: 0.5, Epsilon: 0.7553, Total Step: 49\n",
            "Episode: 58/100, Total Reward: 1.0, Epsilon: 0.7515, Total Step: 22\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 6\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 26\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 17\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 23\n",
            "Episode: 63/100, Total Reward: 0.5, Epsilon: 0.7329, Total Step: 49\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 19\n",
            "Episode: 65/100, Total Reward: 0.5, Epsilon: 0.7256, Total Step: 49\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 9\n",
            "Episode: 67/100, Total Reward: 0.5, Epsilon: 0.7183, Total Step: 49\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 19\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 38\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 7\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 12\n",
            "Episode: 72/100, Total Reward: 0.5, Epsilon: 0.7005, Total Step: 49\n",
            "Episode: 73/100, Total Reward: 0.5, Epsilon: 0.6970, Total Step: 49\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 26\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 17\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 14\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 26\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 15\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 9\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 13\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 9\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 9\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 13\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 25\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 17\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 45\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 43\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 17\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 39\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 9\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 26\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 11\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 12\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 6\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 28\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 9\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 11\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 16\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 9\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 33\n",
            "TESTING 24: total_reward 18.5/24 \t (reward: 0.5)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=5,maxDistanceApart=3,includeDoors=0,maxDistractorItemsPerLocation=0, 4\n",
            "Episode: 1/100, Total Reward: 0.5, Epsilon: 1.0000, Total Step: 49\n",
            "Episode: 2/100, Total Reward: 1.0, Epsilon: 0.9950, Total Step: 48\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 1.0, Epsilon: 0.9851, Total Step: 28\n",
            "Episode: 5/100, Total Reward: 0.5, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 0.5, Epsilon: 0.9752, Total Step: 49\n",
            "Episode: 7/100, Total Reward: 0.0, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 0.5, Epsilon: 0.9655, Total Step: 49\n",
            "Episode: 9/100, Total Reward: 1.0, Epsilon: 0.9607, Total Step: 29\n",
            "Episode: 10/100, Total Reward: 0.5, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 1.0, Epsilon: 0.9511, Total Step: 33\n",
            "Episode: 12/100, Total Reward: 1.0, Epsilon: 0.9464, Total Step: 40\n",
            "Episode: 13/100, Total Reward: 0.0, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 0.5, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 0.5, Epsilon: 0.9322, Total Step: 49\n",
            "Episode: 16/100, Total Reward: 1.0, Epsilon: 0.9276, Total Step: 35\n",
            "Episode: 17/100, Total Reward: 0.0, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 1.0, Epsilon: 0.9183, Total Step: 38\n",
            "Episode: 19/100, Total Reward: 1.0, Epsilon: 0.9137, Total Step: 26\n",
            "Episode: 20/100, Total Reward: 0.5, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 0.0, Epsilon: 0.9046, Total Step: 49\n",
            "Episode: 22/100, Total Reward: 1.0, Epsilon: 0.9001, Total Step: 31\n",
            "Episode: 23/100, Total Reward: 0.0, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 0.5, Epsilon: 0.8911, Total Step: 49\n",
            "Episode: 25/100, Total Reward: 0.0, Epsilon: 0.8867, Total Step: 49\n",
            "Episode: 26/100, Total Reward: 1.0, Epsilon: 0.8822, Total Step: 46\n",
            "Episode: 27/100, Total Reward: 1.0, Epsilon: 0.8778, Total Step: 23\n",
            "Episode: 28/100, Total Reward: 1.0, Epsilon: 0.8734, Total Step: 28\n",
            "Episode: 29/100, Total Reward: 0.5, Epsilon: 0.8691, Total Step: 49\n",
            "Episode: 30/100, Total Reward: 0.5, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 1.0, Epsilon: 0.8604, Total Step: 24\n",
            "Episode: 32/100, Total Reward: 0.5, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 0.0, Epsilon: 0.8518, Total Step: 49\n",
            "Episode: 34/100, Total Reward: 0.0, Epsilon: 0.8475, Total Step: 49\n",
            "Episode: 35/100, Total Reward: 0.5, Epsilon: 0.8433, Total Step: 49\n",
            "Episode: 36/100, Total Reward: 1.0, Epsilon: 0.8391, Total Step: 37\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 41\n",
            "Episode: 38/100, Total Reward: 1.0, Epsilon: 0.8307, Total Step: 41\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 40\n",
            "Episode: 40/100, Total Reward: 1.0, Epsilon: 0.8224, Total Step: 42\n",
            "Episode: 41/100, Total Reward: 0.0, Epsilon: 0.8183, Total Step: 49\n",
            "Episode: 42/100, Total Reward: 1.0, Epsilon: 0.8142, Total Step: 44\n",
            "Episode: 43/100, Total Reward: 1.0, Epsilon: 0.8102, Total Step: 24\n",
            "Episode: 44/100, Total Reward: 1.0, Epsilon: 0.8061, Total Step: 20\n",
            "Episode: 45/100, Total Reward: 0.5, Epsilon: 0.8021, Total Step: 49\n",
            "Episode: 46/100, Total Reward: 1.0, Epsilon: 0.7981, Total Step: 17\n",
            "Episode: 47/100, Total Reward: 1.0, Epsilon: 0.7941, Total Step: 40\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 11\n",
            "Episode: 49/100, Total Reward: 1.0, Epsilon: 0.7862, Total Step: 49\n",
            "Episode: 50/100, Total Reward: 0.5, Epsilon: 0.7822, Total Step: 49\n",
            "Episode: 51/100, Total Reward: 0.5, Epsilon: 0.7783, Total Step: 49\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 11\n",
            "Episode: 53/100, Total Reward: 1.0, Epsilon: 0.7705, Total Step: 22\n",
            "Episode: 54/100, Total Reward: 1.0, Epsilon: 0.7667, Total Step: 22\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 42\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 20\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 18\n",
            "Episode: 58/100, Total Reward: 0.5, Epsilon: 0.7515, Total Step: 49\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 25\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 39\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 45\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 14\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 18\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 34\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 11\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 40\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 36\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 24\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 12\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 30\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 17\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 16\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 16\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 19\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 42\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 25\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 17\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 26\n",
            "Episode: 79/100, Total Reward: 0.0, Epsilon: 0.6764, Total Step: 49\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 7\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 28\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 12\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 6\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 6\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 30\n",
            "Episode: 86/100, Total Reward: 0.5, Epsilon: 0.6531, Total Step: 49\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 17\n",
            "Episode: 88/100, Total Reward: 0.5, Epsilon: 0.6466, Total Step: 49\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 27\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 20\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 14\n",
            "Episode: 92/100, Total Reward: 0.5, Epsilon: 0.6337, Total Step: 49\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 8\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 27\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 9\n",
            "Episode: 96/100, Total Reward: 0.5, Epsilon: 0.6211, Total Step: 49\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 7\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 12\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 11\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 9\n",
            "TESTING 25: total_reward 19.5/25 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=8,maxDistanceApart=4,includeDoors=0,maxDistractorItemsPerLocation=0, 0\n",
            "Episode: 1/100, Total Reward: 0.0, Epsilon: 1.0000, Total Step: 49\n",
            "Episode: 2/100, Total Reward: 0.0, Epsilon: 0.9950, Total Step: 49\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 0.5, Epsilon: 0.9851, Total Step: 49\n",
            "Episode: 5/100, Total Reward: 0.5, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 0.5, Epsilon: 0.9752, Total Step: 49\n",
            "Episode: 7/100, Total Reward: 1.0, Epsilon: 0.9704, Total Step: 30\n",
            "Episode: 8/100, Total Reward: 0.5, Epsilon: 0.9655, Total Step: 49\n",
            "Episode: 9/100, Total Reward: 0.5, Epsilon: 0.9607, Total Step: 49\n",
            "Episode: 10/100, Total Reward: 0.5, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 1.0, Epsilon: 0.9511, Total Step: 41\n",
            "Episode: 12/100, Total Reward: 0.0, Epsilon: 0.9464, Total Step: 49\n",
            "Episode: 13/100, Total Reward: 1.0, Epsilon: 0.9416, Total Step: 23\n",
            "Episode: 14/100, Total Reward: 0.0, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 0.5, Epsilon: 0.9322, Total Step: 49\n",
            "Episode: 16/100, Total Reward: 0.5, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 0.0, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 0.5, Epsilon: 0.9183, Total Step: 49\n",
            "Episode: 19/100, Total Reward: 0.0, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 0.0, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 0.5, Epsilon: 0.9046, Total Step: 49\n",
            "Episode: 22/100, Total Reward: 0.5, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 0.0, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 0.5, Epsilon: 0.8911, Total Step: 49\n",
            "Episode: 25/100, Total Reward: 1.0, Epsilon: 0.8867, Total Step: 17\n",
            "Episode: 26/100, Total Reward: 0.5, Epsilon: 0.8822, Total Step: 49\n",
            "Episode: 27/100, Total Reward: 0.5, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 0.0, Epsilon: 0.8734, Total Step: 49\n",
            "Episode: 29/100, Total Reward: 0.5, Epsilon: 0.8691, Total Step: 49\n",
            "Episode: 30/100, Total Reward: 1.0, Epsilon: 0.8647, Total Step: 34\n",
            "Episode: 31/100, Total Reward: 1.0, Epsilon: 0.8604, Total Step: 10\n",
            "Episode: 32/100, Total Reward: 1.0, Epsilon: 0.8561, Total Step: 34\n",
            "Episode: 33/100, Total Reward: 1.0, Epsilon: 0.8518, Total Step: 37\n",
            "Episode: 34/100, Total Reward: 1.0, Epsilon: 0.8475, Total Step: 28\n",
            "Episode: 35/100, Total Reward: 1.0, Epsilon: 0.8433, Total Step: 13\n",
            "Episode: 36/100, Total Reward: 0.5, Epsilon: 0.8391, Total Step: 49\n",
            "Episode: 37/100, Total Reward: 0.5, Epsilon: 0.8349, Total Step: 49\n",
            "Episode: 38/100, Total Reward: 0.5, Epsilon: 0.8307, Total Step: 49\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 33\n",
            "Episode: 40/100, Total Reward: 1.0, Epsilon: 0.8224, Total Step: 11\n",
            "Episode: 41/100, Total Reward: 0.5, Epsilon: 0.8183, Total Step: 49\n",
            "Episode: 42/100, Total Reward: 1.0, Epsilon: 0.8142, Total Step: 47\n",
            "Episode: 43/100, Total Reward: 1.0, Epsilon: 0.8102, Total Step: 23\n",
            "Episode: 44/100, Total Reward: 1.0, Epsilon: 0.8061, Total Step: 13\n",
            "Episode: 45/100, Total Reward: 0.5, Epsilon: 0.8021, Total Step: 49\n",
            "Episode: 46/100, Total Reward: 0.5, Epsilon: 0.7981, Total Step: 49\n",
            "Episode: 47/100, Total Reward: 0.5, Epsilon: 0.7941, Total Step: 49\n",
            "Episode: 48/100, Total Reward: 0.5, Epsilon: 0.7901, Total Step: 49\n",
            "Episode: 49/100, Total Reward: 1.0, Epsilon: 0.7862, Total Step: 22\n",
            "Episode: 50/100, Total Reward: 1.0, Epsilon: 0.7822, Total Step: 26\n",
            "Episode: 51/100, Total Reward: 0.5, Epsilon: 0.7783, Total Step: 49\n",
            "Episode: 52/100, Total Reward: 0.5, Epsilon: 0.7744, Total Step: 49\n",
            "Episode: 53/100, Total Reward: 1.0, Epsilon: 0.7705, Total Step: 10\n",
            "Episode: 54/100, Total Reward: 1.0, Epsilon: 0.7667, Total Step: 34\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 28\n",
            "Episode: 56/100, Total Reward: 0.5, Epsilon: 0.7590, Total Step: 49\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 41\n",
            "Episode: 58/100, Total Reward: 0.5, Epsilon: 0.7515, Total Step: 49\n",
            "Episode: 59/100, Total Reward: 0.5, Epsilon: 0.7477, Total Step: 49\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 30\n",
            "Episode: 61/100, Total Reward: 0.5, Epsilon: 0.7403, Total Step: 49\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 47\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 21\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 48\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 15\n",
            "Episode: 66/100, Total Reward: 0.5, Epsilon: 0.7219, Total Step: 49\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 6\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 49\n",
            "Episode: 69/100, Total Reward: 0.5, Epsilon: 0.7112, Total Step: 49\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 9\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 36\n",
            "Episode: 72/100, Total Reward: 0.5, Epsilon: 0.7005, Total Step: 49\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 31\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 9\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 38\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 15\n",
            "Episode: 77/100, Total Reward: 0.5, Epsilon: 0.6832, Total Step: 49\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 29\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 20\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 25\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 47\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 7\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 34\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 24\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 44\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 10\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 28\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 13\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 6\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 14\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 9\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 19\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 10\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 36\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 8\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 10\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 30\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 6\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 31\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 23\n",
            "TESTING 26: total_reward 20.5/26 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=8,maxDistanceApart=4,includeDoors=0,maxDistractorItemsPerLocation=0, 1\n",
            "Episode: 1/100, Total Reward: 0.0, Epsilon: 1.0000, Total Step: 49\n",
            "Episode: 2/100, Total Reward: 1.0, Epsilon: 0.9950, Total Step: 43\n",
            "Episode: 3/100, Total Reward: 0.5, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 0.5, Epsilon: 0.9851, Total Step: 49\n",
            "Episode: 5/100, Total Reward: 0.5, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 1.0, Epsilon: 0.9752, Total Step: 24\n",
            "Episode: 7/100, Total Reward: 0.5, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 0.5, Epsilon: 0.9655, Total Step: 49\n",
            "Episode: 9/100, Total Reward: 0.0, Epsilon: 0.9607, Total Step: 49\n",
            "Episode: 10/100, Total Reward: 0.5, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 0.0, Epsilon: 0.9511, Total Step: 49\n",
            "Episode: 12/100, Total Reward: 0.5, Epsilon: 0.9464, Total Step: 49\n",
            "Episode: 13/100, Total Reward: 0.5, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 0.0, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 1.0, Epsilon: 0.9322, Total Step: 49\n",
            "Episode: 16/100, Total Reward: 1.0, Epsilon: 0.9276, Total Step: 39\n",
            "Episode: 17/100, Total Reward: 0.5, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 0.0, Epsilon: 0.9183, Total Step: 49\n",
            "Episode: 19/100, Total Reward: 1.0, Epsilon: 0.9137, Total Step: 37\n",
            "Episode: 20/100, Total Reward: 0.0, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 0.0, Epsilon: 0.9046, Total Step: 49\n",
            "Episode: 22/100, Total Reward: 0.5, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 0.0, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 0.0, Epsilon: 0.8911, Total Step: 49\n",
            "Episode: 25/100, Total Reward: 0.0, Epsilon: 0.8867, Total Step: 49\n",
            "Episode: 26/100, Total Reward: 0.0, Epsilon: 0.8822, Total Step: 49\n",
            "Episode: 27/100, Total Reward: 0.5, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 0.0, Epsilon: 0.8734, Total Step: 49\n",
            "Episode: 29/100, Total Reward: 0.5, Epsilon: 0.8691, Total Step: 49\n",
            "Episode: 30/100, Total Reward: 0.5, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 0.5, Epsilon: 0.8604, Total Step: 49\n",
            "Episode: 32/100, Total Reward: 0.0, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 1.0, Epsilon: 0.8518, Total Step: 10\n",
            "Episode: 34/100, Total Reward: 0.0, Epsilon: 0.8475, Total Step: 49\n",
            "Episode: 35/100, Total Reward: 1.0, Epsilon: 0.8433, Total Step: 18\n",
            "Episode: 36/100, Total Reward: 0.0, Epsilon: 0.8391, Total Step: 49\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 11\n",
            "Episode: 38/100, Total Reward: 0.5, Epsilon: 0.8307, Total Step: 49\n",
            "Episode: 39/100, Total Reward: 0.0, Epsilon: 0.8266, Total Step: 49\n",
            "Episode: 40/100, Total Reward: 0.0, Epsilon: 0.8224, Total Step: 49\n",
            "Episode: 41/100, Total Reward: 0.0, Epsilon: 0.8183, Total Step: 49\n",
            "Episode: 42/100, Total Reward: 1.0, Epsilon: 0.8142, Total Step: 25\n",
            "Episode: 43/100, Total Reward: 0.5, Epsilon: 0.8102, Total Step: 49\n",
            "Episode: 44/100, Total Reward: 0.0, Epsilon: 0.8061, Total Step: 49\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 33\n",
            "Episode: 46/100, Total Reward: 1.0, Epsilon: 0.7981, Total Step: 14\n",
            "Episode: 47/100, Total Reward: 0.5, Epsilon: 0.7941, Total Step: 49\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 36\n",
            "Episode: 49/100, Total Reward: 0.5, Epsilon: 0.7862, Total Step: 49\n",
            "Episode: 50/100, Total Reward: 0.5, Epsilon: 0.7822, Total Step: 49\n",
            "Episode: 51/100, Total Reward: 0.5, Epsilon: 0.7783, Total Step: 49\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 31\n",
            "Episode: 53/100, Total Reward: 1.0, Epsilon: 0.7705, Total Step: 7\n",
            "Episode: 54/100, Total Reward: 0.5, Epsilon: 0.7667, Total Step: 49\n",
            "Episode: 55/100, Total Reward: 0.5, Epsilon: 0.7629, Total Step: 49\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 23\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 26\n",
            "Episode: 58/100, Total Reward: 1.0, Epsilon: 0.7515, Total Step: 16\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 22\n",
            "Episode: 60/100, Total Reward: 0.5, Epsilon: 0.7440, Total Step: 49\n",
            "Episode: 61/100, Total Reward: 0.5, Epsilon: 0.7403, Total Step: 49\n",
            "Episode: 62/100, Total Reward: 0.5, Epsilon: 0.7366, Total Step: 49\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 8\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 40\n",
            "Episode: 65/100, Total Reward: 0.0, Epsilon: 0.7256, Total Step: 49\n",
            "Episode: 66/100, Total Reward: 0.0, Epsilon: 0.7219, Total Step: 49\n",
            "Episode: 67/100, Total Reward: 0.5, Epsilon: 0.7183, Total Step: 49\n",
            "Episode: 68/100, Total Reward: 0.5, Epsilon: 0.7147, Total Step: 49\n",
            "Episode: 69/100, Total Reward: 0.0, Epsilon: 0.7112, Total Step: 49\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 11\n",
            "Episode: 71/100, Total Reward: 0.5, Epsilon: 0.7041, Total Step: 49\n",
            "Episode: 72/100, Total Reward: 0.5, Epsilon: 0.7005, Total Step: 49\n",
            "Episode: 73/100, Total Reward: 0.5, Epsilon: 0.6970, Total Step: 49\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 40\n",
            "Episode: 75/100, Total Reward: 0.5, Epsilon: 0.6901, Total Step: 49\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 40\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 23\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 40\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 33\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 12\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 11\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 12\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 27\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 25\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 6\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 18\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 22\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 11\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 18\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 20\n",
            "Episode: 91/100, Total Reward: 0.0, Epsilon: 0.6369, Total Step: 49\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 12\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 13\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 32\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 27\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 17\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 18\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 18\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 8\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 35\n",
            "TESTING 27: total_reward 21.5/27 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=8,maxDistanceApart=4,includeDoors=0,maxDistractorItemsPerLocation=0, 2\n",
            "Episode: 1/100, Total Reward: 0.5, Epsilon: 1.0000, Total Step: 49\n",
            "Episode: 2/100, Total Reward: 0.0, Epsilon: 0.9950, Total Step: 49\n",
            "Episode: 3/100, Total Reward: 0.5, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 0.0, Epsilon: 0.9851, Total Step: 49\n",
            "Episode: 5/100, Total Reward: 0.0, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 1.0, Epsilon: 0.9752, Total Step: 28\n",
            "Episode: 7/100, Total Reward: 0.0, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 0.5, Epsilon: 0.9655, Total Step: 49\n",
            "Episode: 9/100, Total Reward: 1.0, Epsilon: 0.9607, Total Step: 38\n",
            "Episode: 10/100, Total Reward: 0.5, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 0.0, Epsilon: 0.9511, Total Step: 49\n",
            "Episode: 12/100, Total Reward: 0.5, Epsilon: 0.9464, Total Step: 49\n",
            "Episode: 13/100, Total Reward: 0.0, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 0.0, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 0.5, Epsilon: 0.9322, Total Step: 49\n",
            "Episode: 16/100, Total Reward: 0.5, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 0.0, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 0.5, Epsilon: 0.9183, Total Step: 49\n",
            "Episode: 19/100, Total Reward: 0.0, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 0.0, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 0.0, Epsilon: 0.9046, Total Step: 49\n",
            "Episode: 22/100, Total Reward: 0.0, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 0.5, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 0.0, Epsilon: 0.8911, Total Step: 49\n",
            "Episode: 25/100, Total Reward: 0.5, Epsilon: 0.8867, Total Step: 49\n",
            "Episode: 26/100, Total Reward: 1.0, Epsilon: 0.8822, Total Step: 18\n",
            "Episode: 27/100, Total Reward: 1.0, Epsilon: 0.8778, Total Step: 5\n",
            "Episode: 28/100, Total Reward: 0.0, Epsilon: 0.8734, Total Step: 49\n",
            "Episode: 29/100, Total Reward: 0.0, Epsilon: 0.8691, Total Step: 49\n",
            "Episode: 30/100, Total Reward: 0.0, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 1.0, Epsilon: 0.8604, Total Step: 46\n",
            "Episode: 32/100, Total Reward: 0.5, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 0.0, Epsilon: 0.8518, Total Step: 49\n",
            "Episode: 34/100, Total Reward: 1.0, Epsilon: 0.8475, Total Step: 20\n",
            "Episode: 35/100, Total Reward: 0.5, Epsilon: 0.8433, Total Step: 49\n",
            "Episode: 36/100, Total Reward: 0.5, Epsilon: 0.8391, Total Step: 49\n",
            "Episode: 37/100, Total Reward: 0.5, Epsilon: 0.8349, Total Step: 49\n",
            "Episode: 38/100, Total Reward: 0.0, Epsilon: 0.8307, Total Step: 49\n",
            "Episode: 39/100, Total Reward: 0.5, Epsilon: 0.8266, Total Step: 49\n",
            "Episode: 40/100, Total Reward: 0.5, Epsilon: 0.8224, Total Step: 49\n",
            "Episode: 41/100, Total Reward: 1.0, Epsilon: 0.8183, Total Step: 26\n",
            "Episode: 42/100, Total Reward: 1.0, Epsilon: 0.8142, Total Step: 14\n",
            "Episode: 43/100, Total Reward: 0.5, Epsilon: 0.8102, Total Step: 49\n",
            "Episode: 44/100, Total Reward: 1.0, Epsilon: 0.8061, Total Step: 44\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 11\n",
            "Episode: 46/100, Total Reward: 0.5, Epsilon: 0.7981, Total Step: 49\n",
            "Episode: 47/100, Total Reward: 0.0, Epsilon: 0.7941, Total Step: 49\n",
            "Episode: 48/100, Total Reward: 0.0, Epsilon: 0.7901, Total Step: 49\n",
            "Episode: 49/100, Total Reward: 0.5, Epsilon: 0.7862, Total Step: 49\n",
            "Episode: 50/100, Total Reward: 0.5, Epsilon: 0.7822, Total Step: 49\n",
            "Episode: 51/100, Total Reward: 1.0, Epsilon: 0.7783, Total Step: 24\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 16\n",
            "Episode: 53/100, Total Reward: 1.0, Epsilon: 0.7705, Total Step: 25\n",
            "Episode: 54/100, Total Reward: 0.5, Epsilon: 0.7667, Total Step: 49\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 17\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 12\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 34\n",
            "Episode: 58/100, Total Reward: 0.5, Epsilon: 0.7515, Total Step: 49\n",
            "Episode: 59/100, Total Reward: 0.5, Epsilon: 0.7477, Total Step: 49\n",
            "Episode: 60/100, Total Reward: 0.5, Epsilon: 0.7440, Total Step: 49\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 37\n",
            "Episode: 62/100, Total Reward: 1.0, Epsilon: 0.7366, Total Step: 35\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 28\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 18\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 20\n",
            "Episode: 66/100, Total Reward: 0.0, Epsilon: 0.7219, Total Step: 49\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 10\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 33\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 47\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 23\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 32\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 40\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 21\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 14\n",
            "Episode: 75/100, Total Reward: 0.5, Epsilon: 0.6901, Total Step: 49\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 19\n",
            "Episode: 77/100, Total Reward: 0.5, Epsilon: 0.6832, Total Step: 49\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 14\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 17\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 6\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 27\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 26\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 24\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 31\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 11\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 17\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 6\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 36\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 26\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 24\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 41\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 12\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 43\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 24\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 22\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 30\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 44\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 10\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 38\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 27\n",
            "TESTING 28: total_reward 22.5/28 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=8,maxDistanceApart=4,includeDoors=0,maxDistractorItemsPerLocation=0, 3\n",
            "Episode: 1/100, Total Reward: 0.5, Epsilon: 1.0000, Total Step: 49\n",
            "Episode: 2/100, Total Reward: 1.0, Epsilon: 0.9950, Total Step: 13\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 0.5, Epsilon: 0.9851, Total Step: 49\n",
            "Episode: 5/100, Total Reward: 0.0, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 0.5, Epsilon: 0.9752, Total Step: 49\n",
            "Episode: 7/100, Total Reward: 1.0, Epsilon: 0.9704, Total Step: 11\n",
            "Episode: 8/100, Total Reward: 0.5, Epsilon: 0.9655, Total Step: 49\n",
            "Episode: 9/100, Total Reward: 1.0, Epsilon: 0.9607, Total Step: 5\n",
            "Episode: 10/100, Total Reward: 0.5, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 0.0, Epsilon: 0.9511, Total Step: 49\n",
            "Episode: 12/100, Total Reward: 0.0, Epsilon: 0.9464, Total Step: 49\n",
            "Episode: 13/100, Total Reward: 0.0, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 0.0, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 1.0, Epsilon: 0.9322, Total Step: 19\n",
            "Episode: 16/100, Total Reward: 0.5, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 1.0, Epsilon: 0.9229, Total Step: 20\n",
            "Episode: 18/100, Total Reward: 0.5, Epsilon: 0.9183, Total Step: 49\n",
            "Episode: 19/100, Total Reward: 0.5, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 0.0, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 0.5, Epsilon: 0.9046, Total Step: 49\n",
            "Episode: 22/100, Total Reward: 1.0, Epsilon: 0.9001, Total Step: 12\n",
            "Episode: 23/100, Total Reward: 0.5, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 1.0, Epsilon: 0.8911, Total Step: 4\n",
            "Episode: 25/100, Total Reward: 1.0, Epsilon: 0.8867, Total Step: 45\n",
            "Episode: 26/100, Total Reward: 0.0, Epsilon: 0.8822, Total Step: 49\n",
            "Episode: 27/100, Total Reward: 0.0, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 0.5, Epsilon: 0.8734, Total Step: 49\n",
            "Episode: 29/100, Total Reward: 0.5, Epsilon: 0.8691, Total Step: 49\n",
            "Episode: 30/100, Total Reward: 0.5, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 1.0, Epsilon: 0.8604, Total Step: 21\n",
            "Episode: 32/100, Total Reward: 0.5, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 1.0, Epsilon: 0.8518, Total Step: 31\n",
            "Episode: 34/100, Total Reward: 1.0, Epsilon: 0.8475, Total Step: 36\n",
            "Episode: 35/100, Total Reward: 1.0, Epsilon: 0.8433, Total Step: 34\n",
            "Episode: 36/100, Total Reward: 1.0, Epsilon: 0.8391, Total Step: 10\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 10\n",
            "Episode: 38/100, Total Reward: 1.0, Epsilon: 0.8307, Total Step: 15\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 37\n",
            "Episode: 40/100, Total Reward: 1.0, Epsilon: 0.8224, Total Step: 41\n",
            "Episode: 41/100, Total Reward: 1.0, Epsilon: 0.8183, Total Step: 28\n",
            "Episode: 42/100, Total Reward: 1.0, Epsilon: 0.8142, Total Step: 22\n",
            "Episode: 43/100, Total Reward: 1.0, Epsilon: 0.8102, Total Step: 14\n",
            "Episode: 44/100, Total Reward: 1.0, Epsilon: 0.8061, Total Step: 33\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 30\n",
            "Episode: 46/100, Total Reward: 1.0, Epsilon: 0.7981, Total Step: 9\n",
            "Episode: 47/100, Total Reward: 0.5, Epsilon: 0.7941, Total Step: 49\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 5\n",
            "Episode: 49/100, Total Reward: 1.0, Epsilon: 0.7862, Total Step: 29\n",
            "Episode: 50/100, Total Reward: 1.0, Epsilon: 0.7822, Total Step: 41\n",
            "Episode: 51/100, Total Reward: 1.0, Epsilon: 0.7783, Total Step: 3\n",
            "Episode: 52/100, Total Reward: 1.0, Epsilon: 0.7744, Total Step: 9\n",
            "Episode: 53/100, Total Reward: 0.5, Epsilon: 0.7705, Total Step: 49\n",
            "Episode: 54/100, Total Reward: 1.0, Epsilon: 0.7667, Total Step: 10\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 43\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 33\n",
            "Episode: 57/100, Total Reward: 0.5, Epsilon: 0.7553, Total Step: 49\n",
            "Episode: 58/100, Total Reward: 0.5, Epsilon: 0.7515, Total Step: 49\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 37\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 9\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 27\n",
            "Episode: 62/100, Total Reward: 0.0, Epsilon: 0.7366, Total Step: 49\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 49\n",
            "Episode: 64/100, Total Reward: 1.0, Epsilon: 0.7292, Total Step: 9\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 39\n",
            "Episode: 66/100, Total Reward: 1.0, Epsilon: 0.7219, Total Step: 13\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 8\n",
            "Episode: 68/100, Total Reward: 1.0, Epsilon: 0.7147, Total Step: 11\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 45\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 10\n",
            "Episode: 71/100, Total Reward: 1.0, Epsilon: 0.7041, Total Step: 17\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 24\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 3\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 13\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 25\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 48\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 6\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 17\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 4\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 4\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 4\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 3\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 6\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 6\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 20\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 9\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 6\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 8\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 4\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 7\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 5\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 9\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 10\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 8\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 11\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 7\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 8\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 8\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 10\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 5\n",
            "TESTING 29: total_reward 23.5/29 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=8,maxDistanceApart=4,includeDoors=0,maxDistractorItemsPerLocation=0, 4\n",
            "Episode: 1/100, Total Reward: 0.0, Epsilon: 1.0000, Total Step: 49\n",
            "Episode: 2/100, Total Reward: 0.0, Epsilon: 0.9950, Total Step: 49\n",
            "Episode: 3/100, Total Reward: 1.0, Epsilon: 0.9900, Total Step: 34\n",
            "Episode: 4/100, Total Reward: 0.0, Epsilon: 0.9851, Total Step: 49\n",
            "Episode: 5/100, Total Reward: 0.0, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 1.0, Epsilon: 0.9752, Total Step: 13\n",
            "Episode: 7/100, Total Reward: 0.5, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 0.5, Epsilon: 0.9655, Total Step: 49\n",
            "Episode: 9/100, Total Reward: 0.5, Epsilon: 0.9607, Total Step: 49\n",
            "Episode: 10/100, Total Reward: 0.0, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 0.0, Epsilon: 0.9511, Total Step: 49\n",
            "Episode: 12/100, Total Reward: 1.0, Epsilon: 0.9464, Total Step: 34\n",
            "Episode: 13/100, Total Reward: 0.5, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 0.0, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 0.0, Epsilon: 0.9322, Total Step: 49\n",
            "Episode: 16/100, Total Reward: 0.0, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 0.0, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 0.0, Epsilon: 0.9183, Total Step: 49\n",
            "Episode: 19/100, Total Reward: 0.5, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 0.5, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 0.0, Epsilon: 0.9046, Total Step: 49\n",
            "Episode: 22/100, Total Reward: 0.0, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 1.0, Epsilon: 0.8956, Total Step: 31\n",
            "Episode: 24/100, Total Reward: 1.0, Epsilon: 0.8911, Total Step: 26\n",
            "Episode: 25/100, Total Reward: 1.0, Epsilon: 0.8867, Total Step: 19\n",
            "Episode: 26/100, Total Reward: 0.5, Epsilon: 0.8822, Total Step: 49\n",
            "Episode: 27/100, Total Reward: 1.0, Epsilon: 0.8778, Total Step: 44\n",
            "Episode: 28/100, Total Reward: 1.0, Epsilon: 0.8734, Total Step: 19\n",
            "Episode: 29/100, Total Reward: 1.0, Epsilon: 0.8691, Total Step: 28\n",
            "Episode: 30/100, Total Reward: 0.0, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 0.5, Epsilon: 0.8604, Total Step: 49\n",
            "Episode: 32/100, Total Reward: 0.5, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 0.5, Epsilon: 0.8518, Total Step: 49\n",
            "Episode: 34/100, Total Reward: 0.0, Epsilon: 0.8475, Total Step: 49\n",
            "Episode: 35/100, Total Reward: 0.0, Epsilon: 0.8433, Total Step: 49\n",
            "Episode: 36/100, Total Reward: 0.5, Epsilon: 0.8391, Total Step: 49\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 35\n",
            "Episode: 38/100, Total Reward: 0.5, Epsilon: 0.8307, Total Step: 49\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 32\n",
            "Episode: 40/100, Total Reward: 1.0, Epsilon: 0.8224, Total Step: 37\n",
            "Episode: 41/100, Total Reward: 0.5, Epsilon: 0.8183, Total Step: 49\n",
            "Episode: 42/100, Total Reward: 0.0, Epsilon: 0.8142, Total Step: 49\n",
            "Episode: 43/100, Total Reward: 1.0, Epsilon: 0.8102, Total Step: 17\n",
            "Episode: 44/100, Total Reward: 0.0, Epsilon: 0.8061, Total Step: 49\n",
            "Episode: 45/100, Total Reward: 0.5, Epsilon: 0.8021, Total Step: 49\n",
            "Episode: 46/100, Total Reward: 0.5, Epsilon: 0.7981, Total Step: 49\n",
            "Episode: 47/100, Total Reward: 0.5, Epsilon: 0.7941, Total Step: 49\n",
            "Episode: 48/100, Total Reward: 1.0, Epsilon: 0.7901, Total Step: 38\n",
            "Episode: 49/100, Total Reward: 0.5, Epsilon: 0.7862, Total Step: 49\n",
            "Episode: 50/100, Total Reward: 1.0, Epsilon: 0.7822, Total Step: 26\n",
            "Episode: 51/100, Total Reward: 0.5, Epsilon: 0.7783, Total Step: 49\n",
            "Episode: 52/100, Total Reward: 0.5, Epsilon: 0.7744, Total Step: 49\n",
            "Episode: 53/100, Total Reward: 1.0, Epsilon: 0.7705, Total Step: 29\n",
            "Episode: 54/100, Total Reward: 0.0, Epsilon: 0.7667, Total Step: 49\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 18\n",
            "Episode: 56/100, Total Reward: 0.0, Epsilon: 0.7590, Total Step: 49\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 18\n",
            "Episode: 58/100, Total Reward: 1.0, Epsilon: 0.7515, Total Step: 39\n",
            "Episode: 59/100, Total Reward: 1.0, Epsilon: 0.7477, Total Step: 24\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 37\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 43\n",
            "Episode: 62/100, Total Reward: 0.5, Epsilon: 0.7366, Total Step: 49\n",
            "Episode: 63/100, Total Reward: 1.0, Epsilon: 0.7329, Total Step: 16\n",
            "Episode: 64/100, Total Reward: 0.5, Epsilon: 0.7292, Total Step: 49\n",
            "Episode: 65/100, Total Reward: 0.5, Epsilon: 0.7256, Total Step: 49\n",
            "Episode: 66/100, Total Reward: 0.5, Epsilon: 0.7219, Total Step: 49\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 18\n",
            "Episode: 68/100, Total Reward: 0.0, Epsilon: 0.7147, Total Step: 49\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 19\n",
            "Episode: 70/100, Total Reward: 1.0, Epsilon: 0.7076, Total Step: 15\n",
            "Episode: 71/100, Total Reward: 0.5, Epsilon: 0.7041, Total Step: 49\n",
            "Episode: 72/100, Total Reward: 1.0, Epsilon: 0.7005, Total Step: 42\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 27\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 48\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 31\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 16\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 46\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 10\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 13\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 18\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 29\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 39\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 43\n",
            "Episode: 84/100, Total Reward: 1.0, Epsilon: 0.6597, Total Step: 12\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 8\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 20\n",
            "Episode: 87/100, Total Reward: 0.0, Epsilon: 0.6498, Total Step: 49\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 15\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 16\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 12\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 6\n",
            "Episode: 92/100, Total Reward: 0.5, Epsilon: 0.6337, Total Step: 49\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 15\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 22\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 22\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 17\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 28\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 16\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 18\n",
            "Episode: 100/100, Total Reward: 1.0, Epsilon: 0.6088, Total Step: 24\n",
            "TESTING 30: total_reward 24.5/30 \t (reward: 1.0)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=11,maxDistanceApart=5,includeDoors=0,maxDistractorItemsPerLocation=0, 0\n",
            "Episode: 1/100, Total Reward: 0.0, Epsilon: 1.0000, Total Step: 49\n",
            "Episode: 2/100, Total Reward: 0.5, Epsilon: 0.9950, Total Step: 49\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 1.0, Epsilon: 0.9851, Total Step: 45\n",
            "Episode: 5/100, Total Reward: 0.0, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 1.0, Epsilon: 0.9752, Total Step: 37\n",
            "Episode: 7/100, Total Reward: 0.0, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 0.5, Epsilon: 0.9655, Total Step: 49\n",
            "Episode: 9/100, Total Reward: 0.0, Epsilon: 0.9607, Total Step: 49\n",
            "Episode: 10/100, Total Reward: 0.0, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 0.0, Epsilon: 0.9511, Total Step: 49\n",
            "Episode: 12/100, Total Reward: 0.0, Epsilon: 0.9464, Total Step: 49\n",
            "Episode: 13/100, Total Reward: 0.0, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 0.5, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 0.0, Epsilon: 0.9322, Total Step: 49\n",
            "Episode: 16/100, Total Reward: 0.0, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 0.0, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 0.0, Epsilon: 0.9183, Total Step: 49\n",
            "Episode: 19/100, Total Reward: 0.0, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 0.0, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 0.0, Epsilon: 0.9046, Total Step: 49\n",
            "Episode: 22/100, Total Reward: 0.0, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 0.0, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 0.0, Epsilon: 0.8911, Total Step: 49\n",
            "Episode: 25/100, Total Reward: 0.5, Epsilon: 0.8867, Total Step: 49\n",
            "Episode: 26/100, Total Reward: 0.0, Epsilon: 0.8822, Total Step: 49\n",
            "Episode: 27/100, Total Reward: 0.0, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 0.0, Epsilon: 0.8734, Total Step: 49\n",
            "Episode: 29/100, Total Reward: 0.0, Epsilon: 0.8691, Total Step: 49\n",
            "Episode: 30/100, Total Reward: 1.0, Epsilon: 0.8647, Total Step: 34\n",
            "Episode: 31/100, Total Reward: 1.0, Epsilon: 0.8604, Total Step: 40\n",
            "Episode: 32/100, Total Reward: 0.5, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 0.0, Epsilon: 0.8518, Total Step: 49\n",
            "Episode: 34/100, Total Reward: 0.0, Epsilon: 0.8475, Total Step: 49\n",
            "Episode: 35/100, Total Reward: 0.5, Epsilon: 0.8433, Total Step: 49\n",
            "Episode: 36/100, Total Reward: 0.0, Epsilon: 0.8391, Total Step: 49\n",
            "Episode: 37/100, Total Reward: 0.5, Epsilon: 0.8349, Total Step: 49\n",
            "Episode: 38/100, Total Reward: 0.5, Epsilon: 0.8307, Total Step: 49\n",
            "Episode: 39/100, Total Reward: 0.5, Epsilon: 0.8266, Total Step: 49\n",
            "Episode: 40/100, Total Reward: 0.0, Epsilon: 0.8224, Total Step: 49\n",
            "Episode: 41/100, Total Reward: 0.0, Epsilon: 0.8183, Total Step: 49\n",
            "Episode: 42/100, Total Reward: 0.5, Epsilon: 0.8142, Total Step: 49\n",
            "Episode: 43/100, Total Reward: 0.0, Epsilon: 0.8102, Total Step: 49\n",
            "Episode: 44/100, Total Reward: 0.0, Epsilon: 0.8061, Total Step: 49\n",
            "Episode: 45/100, Total Reward: 0.0, Epsilon: 0.8021, Total Step: 49\n",
            "Episode: 46/100, Total Reward: 0.0, Epsilon: 0.7981, Total Step: 49\n",
            "Episode: 47/100, Total Reward: 0.5, Epsilon: 0.7941, Total Step: 49\n",
            "Episode: 48/100, Total Reward: 0.5, Epsilon: 0.7901, Total Step: 49\n",
            "Episode: 49/100, Total Reward: 0.0, Epsilon: 0.7862, Total Step: 49\n",
            "Episode: 50/100, Total Reward: 0.0, Epsilon: 0.7822, Total Step: 49\n",
            "Episode: 51/100, Total Reward: 0.5, Epsilon: 0.7783, Total Step: 49\n",
            "Episode: 52/100, Total Reward: 0.5, Epsilon: 0.7744, Total Step: 49\n",
            "Episode: 53/100, Total Reward: 0.5, Epsilon: 0.7705, Total Step: 49\n",
            "Episode: 54/100, Total Reward: 1.0, Epsilon: 0.7667, Total Step: 39\n",
            "Episode: 55/100, Total Reward: 0.0, Epsilon: 0.7629, Total Step: 49\n",
            "Episode: 56/100, Total Reward: 1.0, Epsilon: 0.7590, Total Step: 32\n",
            "Episode: 57/100, Total Reward: 0.0, Epsilon: 0.7553, Total Step: 49\n",
            "Episode: 58/100, Total Reward: 0.0, Epsilon: 0.7515, Total Step: 49\n",
            "Episode: 59/100, Total Reward: 0.5, Epsilon: 0.7477, Total Step: 49\n",
            "Episode: 60/100, Total Reward: 0.0, Epsilon: 0.7440, Total Step: 49\n",
            "Episode: 61/100, Total Reward: 0.0, Epsilon: 0.7403, Total Step: 49\n",
            "Episode: 62/100, Total Reward: 0.5, Epsilon: 0.7366, Total Step: 49\n",
            "Episode: 63/100, Total Reward: 0.5, Epsilon: 0.7329, Total Step: 49\n",
            "Episode: 64/100, Total Reward: 0.0, Epsilon: 0.7292, Total Step: 49\n",
            "Episode: 65/100, Total Reward: 0.5, Epsilon: 0.7256, Total Step: 49\n",
            "Episode: 66/100, Total Reward: 0.5, Epsilon: 0.7219, Total Step: 49\n",
            "Episode: 67/100, Total Reward: 0.0, Epsilon: 0.7183, Total Step: 49\n",
            "Episode: 68/100, Total Reward: 0.5, Epsilon: 0.7147, Total Step: 49\n",
            "Episode: 69/100, Total Reward: 0.5, Epsilon: 0.7112, Total Step: 49\n",
            "Episode: 70/100, Total Reward: 0.5, Epsilon: 0.7076, Total Step: 49\n",
            "Episode: 71/100, Total Reward: 0.5, Epsilon: 0.7041, Total Step: 49\n",
            "Episode: 72/100, Total Reward: 0.5, Epsilon: 0.7005, Total Step: 49\n",
            "Episode: 73/100, Total Reward: 0.5, Epsilon: 0.6970, Total Step: 49\n",
            "Episode: 74/100, Total Reward: 1.0, Epsilon: 0.6936, Total Step: 34\n",
            "Episode: 75/100, Total Reward: 0.5, Epsilon: 0.6901, Total Step: 49\n",
            "Episode: 76/100, Total Reward: 0.5, Epsilon: 0.6866, Total Step: 49\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 37\n",
            "Episode: 78/100, Total Reward: 0.5, Epsilon: 0.6798, Total Step: 49\n",
            "Episode: 79/100, Total Reward: 0.5, Epsilon: 0.6764, Total Step: 49\n",
            "Episode: 80/100, Total Reward: 0.0, Epsilon: 0.6730, Total Step: 49\n",
            "Episode: 81/100, Total Reward: 0.5, Epsilon: 0.6696, Total Step: 49\n",
            "Episode: 82/100, Total Reward: 0.5, Epsilon: 0.6663, Total Step: 49\n",
            "Episode: 83/100, Total Reward: 0.5, Epsilon: 0.6630, Total Step: 49\n",
            "Episode: 84/100, Total Reward: 0.5, Epsilon: 0.6597, Total Step: 49\n",
            "Episode: 85/100, Total Reward: 0.5, Epsilon: 0.6564, Total Step: 49\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 43\n",
            "Episode: 87/100, Total Reward: 0.5, Epsilon: 0.6498, Total Step: 49\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 31\n",
            "Episode: 89/100, Total Reward: 0.5, Epsilon: 0.6433, Total Step: 49\n",
            "Episode: 90/100, Total Reward: 0.5, Epsilon: 0.6401, Total Step: 49\n",
            "Episode: 91/100, Total Reward: 0.5, Epsilon: 0.6369, Total Step: 49\n",
            "Episode: 92/100, Total Reward: 0.5, Epsilon: 0.6337, Total Step: 49\n",
            "Episode: 93/100, Total Reward: 0.5, Epsilon: 0.6306, Total Step: 49\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 41\n",
            "Episode: 95/100, Total Reward: 0.5, Epsilon: 0.6243, Total Step: 49\n",
            "Episode: 96/100, Total Reward: 0.5, Epsilon: 0.6211, Total Step: 49\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 46\n",
            "Episode: 98/100, Total Reward: 0.5, Epsilon: 0.6149, Total Step: 49\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 30\n",
            "Episode: 100/100, Total Reward: 0.5, Epsilon: 0.6088, Total Step: 49\n",
            "TESTING 31: total_reward 25.0/31 \t (reward: 0.5)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=11,maxDistanceApart=5,includeDoors=0,maxDistractorItemsPerLocation=0, 1\n",
            "Episode: 1/100, Total Reward: 0.0, Epsilon: 1.0000, Total Step: 49\n",
            "Episode: 2/100, Total Reward: 0.0, Epsilon: 0.9950, Total Step: 49\n",
            "Episode: 3/100, Total Reward: 0.0, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 0.0, Epsilon: 0.9851, Total Step: 49\n",
            "Episode: 5/100, Total Reward: 0.0, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 0.0, Epsilon: 0.9752, Total Step: 49\n",
            "Episode: 7/100, Total Reward: 0.5, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 1.0, Epsilon: 0.9655, Total Step: 40\n",
            "Episode: 9/100, Total Reward: 0.5, Epsilon: 0.9607, Total Step: 49\n",
            "Episode: 10/100, Total Reward: 0.5, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 0.0, Epsilon: 0.9511, Total Step: 49\n",
            "Episode: 12/100, Total Reward: 0.0, Epsilon: 0.9464, Total Step: 49\n",
            "Episode: 13/100, Total Reward: 0.5, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 0.0, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 0.0, Epsilon: 0.9322, Total Step: 49\n",
            "Episode: 16/100, Total Reward: 0.0, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 0.0, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 1.0, Epsilon: 0.9183, Total Step: 42\n",
            "Episode: 19/100, Total Reward: 0.0, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 0.0, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 0.0, Epsilon: 0.9046, Total Step: 49\n",
            "Episode: 22/100, Total Reward: 1.0, Epsilon: 0.9001, Total Step: 31\n",
            "Episode: 23/100, Total Reward: 0.0, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 0.0, Epsilon: 0.8911, Total Step: 49\n",
            "Episode: 25/100, Total Reward: 0.0, Epsilon: 0.8867, Total Step: 49\n",
            "Episode: 26/100, Total Reward: 0.0, Epsilon: 0.8822, Total Step: 49\n",
            "Episode: 27/100, Total Reward: 0.5, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 0.0, Epsilon: 0.8734, Total Step: 49\n",
            "Episode: 29/100, Total Reward: 0.0, Epsilon: 0.8691, Total Step: 49\n",
            "Episode: 30/100, Total Reward: 0.0, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 0.0, Epsilon: 0.8604, Total Step: 49\n",
            "Episode: 32/100, Total Reward: 0.5, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 0.0, Epsilon: 0.8518, Total Step: 49\n",
            "Episode: 34/100, Total Reward: 0.5, Epsilon: 0.8475, Total Step: 49\n",
            "Episode: 35/100, Total Reward: 0.5, Epsilon: 0.8433, Total Step: 49\n",
            "Episode: 36/100, Total Reward: 0.0, Epsilon: 0.8391, Total Step: 49\n",
            "Episode: 37/100, Total Reward: 0.0, Epsilon: 0.8349, Total Step: 49\n",
            "Episode: 38/100, Total Reward: 0.0, Epsilon: 0.8307, Total Step: 49\n",
            "Episode: 39/100, Total Reward: 0.5, Epsilon: 0.8266, Total Step: 49\n",
            "Episode: 40/100, Total Reward: 0.0, Epsilon: 0.8224, Total Step: 49\n",
            "Episode: 41/100, Total Reward: 0.5, Epsilon: 0.8183, Total Step: 49\n",
            "Episode: 42/100, Total Reward: 0.0, Epsilon: 0.8142, Total Step: 49\n",
            "Episode: 43/100, Total Reward: 0.0, Epsilon: 0.8102, Total Step: 49\n",
            "Episode: 44/100, Total Reward: 0.0, Epsilon: 0.8061, Total Step: 49\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 17\n",
            "Episode: 46/100, Total Reward: 0.5, Epsilon: 0.7981, Total Step: 49\n",
            "Episode: 47/100, Total Reward: 0.0, Epsilon: 0.7941, Total Step: 49\n",
            "Episode: 48/100, Total Reward: 0.0, Epsilon: 0.7901, Total Step: 49\n",
            "Episode: 49/100, Total Reward: 0.5, Epsilon: 0.7862, Total Step: 49\n",
            "Episode: 50/100, Total Reward: 0.0, Epsilon: 0.7822, Total Step: 49\n",
            "Episode: 51/100, Total Reward: 0.5, Epsilon: 0.7783, Total Step: 49\n",
            "Episode: 52/100, Total Reward: 0.0, Epsilon: 0.7744, Total Step: 49\n",
            "Episode: 53/100, Total Reward: 0.5, Epsilon: 0.7705, Total Step: 49\n",
            "Episode: 54/100, Total Reward: 0.5, Epsilon: 0.7667, Total Step: 49\n",
            "Episode: 55/100, Total Reward: 0.5, Epsilon: 0.7629, Total Step: 49\n",
            "Episode: 56/100, Total Reward: 0.0, Epsilon: 0.7590, Total Step: 49\n",
            "Episode: 57/100, Total Reward: 0.0, Epsilon: 0.7553, Total Step: 49\n",
            "Episode: 58/100, Total Reward: 1.0, Epsilon: 0.7515, Total Step: 48\n",
            "Episode: 59/100, Total Reward: 0.5, Epsilon: 0.7477, Total Step: 49\n",
            "Episode: 60/100, Total Reward: 1.0, Epsilon: 0.7440, Total Step: 41\n",
            "Episode: 61/100, Total Reward: 1.0, Epsilon: 0.7403, Total Step: 44\n",
            "Episode: 62/100, Total Reward: 0.0, Epsilon: 0.7366, Total Step: 49\n",
            "Episode: 63/100, Total Reward: 0.5, Epsilon: 0.7329, Total Step: 49\n",
            "Episode: 64/100, Total Reward: 0.5, Epsilon: 0.7292, Total Step: 49\n",
            "Episode: 65/100, Total Reward: 0.5, Epsilon: 0.7256, Total Step: 49\n",
            "Episode: 66/100, Total Reward: 0.5, Epsilon: 0.7219, Total Step: 49\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 21\n",
            "Episode: 68/100, Total Reward: 0.5, Epsilon: 0.7147, Total Step: 49\n",
            "Episode: 69/100, Total Reward: 1.0, Epsilon: 0.7112, Total Step: 10\n",
            "Episode: 70/100, Total Reward: 0.5, Epsilon: 0.7076, Total Step: 49\n",
            "Episode: 71/100, Total Reward: 0.5, Epsilon: 0.7041, Total Step: 49\n",
            "Episode: 72/100, Total Reward: 0.5, Epsilon: 0.7005, Total Step: 49\n",
            "Episode: 73/100, Total Reward: 0.5, Epsilon: 0.6970, Total Step: 49\n",
            "Episode: 74/100, Total Reward: 0.5, Epsilon: 0.6936, Total Step: 49\n",
            "Episode: 75/100, Total Reward: 1.0, Epsilon: 0.6901, Total Step: 36\n",
            "Episode: 76/100, Total Reward: 0.5, Epsilon: 0.6866, Total Step: 49\n",
            "Episode: 77/100, Total Reward: 1.0, Epsilon: 0.6832, Total Step: 33\n",
            "Episode: 78/100, Total Reward: 1.0, Epsilon: 0.6798, Total Step: 38\n",
            "Episode: 79/100, Total Reward: 1.0, Epsilon: 0.6764, Total Step: 9\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 20\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 31\n",
            "Episode: 82/100, Total Reward: 0.5, Epsilon: 0.6663, Total Step: 49\n",
            "Episode: 83/100, Total Reward: 0.5, Epsilon: 0.6630, Total Step: 49\n",
            "Episode: 84/100, Total Reward: 0.5, Epsilon: 0.6597, Total Step: 49\n",
            "Episode: 85/100, Total Reward: 1.0, Epsilon: 0.6564, Total Step: 29\n",
            "Episode: 86/100, Total Reward: 1.0, Epsilon: 0.6531, Total Step: 44\n",
            "Episode: 87/100, Total Reward: 1.0, Epsilon: 0.6498, Total Step: 18\n",
            "Episode: 88/100, Total Reward: 0.5, Epsilon: 0.6466, Total Step: 49\n",
            "Episode: 89/100, Total Reward: 1.0, Epsilon: 0.6433, Total Step: 34\n",
            "Episode: 90/100, Total Reward: 1.0, Epsilon: 0.6401, Total Step: 11\n",
            "Episode: 91/100, Total Reward: 1.0, Epsilon: 0.6369, Total Step: 35\n",
            "Episode: 92/100, Total Reward: 1.0, Epsilon: 0.6337, Total Step: 26\n",
            "Episode: 93/100, Total Reward: 1.0, Epsilon: 0.6306, Total Step: 21\n",
            "Episode: 94/100, Total Reward: 1.0, Epsilon: 0.6274, Total Step: 26\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 31\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 10\n",
            "Episode: 97/100, Total Reward: 1.0, Epsilon: 0.6180, Total Step: 10\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 11\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 45\n",
            "Episode: 100/100, Total Reward: 0.5, Epsilon: 0.6088, Total Step: 49\n",
            "TESTING 32: total_reward 25.5/32 \t (reward: 0.5)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=11,maxDistanceApart=5,includeDoors=0,maxDistractorItemsPerLocation=0, 2\n",
            "Episode: 1/100, Total Reward: 0.0, Epsilon: 1.0000, Total Step: 49\n",
            "Episode: 2/100, Total Reward: 0.0, Epsilon: 0.9950, Total Step: 49\n",
            "Episode: 3/100, Total Reward: 0.5, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 1.0, Epsilon: 0.9851, Total Step: 35\n",
            "Episode: 5/100, Total Reward: 0.5, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 0.0, Epsilon: 0.9752, Total Step: 49\n",
            "Episode: 7/100, Total Reward: 1.0, Epsilon: 0.9704, Total Step: 22\n",
            "Episode: 8/100, Total Reward: 0.0, Epsilon: 0.9655, Total Step: 49\n",
            "Episode: 9/100, Total Reward: 0.0, Epsilon: 0.9607, Total Step: 49\n",
            "Episode: 10/100, Total Reward: 1.0, Epsilon: 0.9559, Total Step: 41\n",
            "Episode: 11/100, Total Reward: 0.0, Epsilon: 0.9511, Total Step: 49\n",
            "Episode: 12/100, Total Reward: 0.5, Epsilon: 0.9464, Total Step: 49\n",
            "Episode: 13/100, Total Reward: 0.5, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 1.0, Epsilon: 0.9369, Total Step: 14\n",
            "Episode: 15/100, Total Reward: 0.0, Epsilon: 0.9322, Total Step: 49\n",
            "Episode: 16/100, Total Reward: 0.5, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 0.0, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 0.0, Epsilon: 0.9183, Total Step: 49\n",
            "Episode: 19/100, Total Reward: 0.5, Epsilon: 0.9137, Total Step: 49\n",
            "Episode: 20/100, Total Reward: 0.5, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 1.0, Epsilon: 0.9046, Total Step: 28\n",
            "Episode: 22/100, Total Reward: 0.0, Epsilon: 0.9001, Total Step: 49\n",
            "Episode: 23/100, Total Reward: 0.0, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 1.0, Epsilon: 0.8911, Total Step: 49\n",
            "Episode: 25/100, Total Reward: 0.0, Epsilon: 0.8867, Total Step: 49\n",
            "Episode: 26/100, Total Reward: 0.5, Epsilon: 0.8822, Total Step: 49\n",
            "Episode: 27/100, Total Reward: 0.0, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 0.0, Epsilon: 0.8734, Total Step: 49\n",
            "Episode: 29/100, Total Reward: 1.0, Epsilon: 0.8691, Total Step: 36\n",
            "Episode: 30/100, Total Reward: 0.0, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 0.0, Epsilon: 0.8604, Total Step: 49\n",
            "Episode: 32/100, Total Reward: 0.0, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 0.5, Epsilon: 0.8518, Total Step: 49\n",
            "Episode: 34/100, Total Reward: 0.0, Epsilon: 0.8475, Total Step: 49\n",
            "Episode: 35/100, Total Reward: 0.5, Epsilon: 0.8433, Total Step: 49\n",
            "Episode: 36/100, Total Reward: 1.0, Epsilon: 0.8391, Total Step: 40\n",
            "Episode: 37/100, Total Reward: 0.0, Epsilon: 0.8349, Total Step: 49\n",
            "Episode: 38/100, Total Reward: 0.5, Epsilon: 0.8307, Total Step: 49\n",
            "Episode: 39/100, Total Reward: 1.0, Epsilon: 0.8266, Total Step: 38\n",
            "Episode: 40/100, Total Reward: 0.0, Epsilon: 0.8224, Total Step: 49\n",
            "Episode: 41/100, Total Reward: 0.5, Epsilon: 0.8183, Total Step: 49\n",
            "Episode: 42/100, Total Reward: 0.5, Epsilon: 0.8142, Total Step: 49\n",
            "Episode: 43/100, Total Reward: 0.0, Epsilon: 0.8102, Total Step: 49\n",
            "Episode: 44/100, Total Reward: 0.5, Epsilon: 0.8061, Total Step: 49\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 24\n",
            "Episode: 46/100, Total Reward: 0.5, Epsilon: 0.7981, Total Step: 49\n",
            "Episode: 47/100, Total Reward: 0.5, Epsilon: 0.7941, Total Step: 49\n",
            "Episode: 48/100, Total Reward: 0.0, Epsilon: 0.7901, Total Step: 49\n",
            "Episode: 49/100, Total Reward: 0.0, Epsilon: 0.7862, Total Step: 49\n",
            "Episode: 50/100, Total Reward: 0.5, Epsilon: 0.7822, Total Step: 49\n",
            "Episode: 51/100, Total Reward: 1.0, Epsilon: 0.7783, Total Step: 37\n",
            "Episode: 52/100, Total Reward: 0.5, Epsilon: 0.7744, Total Step: 49\n",
            "Episode: 53/100, Total Reward: 0.5, Epsilon: 0.7705, Total Step: 49\n",
            "Episode: 54/100, Total Reward: 1.0, Epsilon: 0.7667, Total Step: 29\n",
            "Episode: 55/100, Total Reward: 1.0, Epsilon: 0.7629, Total Step: 36\n",
            "Episode: 56/100, Total Reward: 0.0, Epsilon: 0.7590, Total Step: 49\n",
            "Episode: 57/100, Total Reward: 1.0, Epsilon: 0.7553, Total Step: 44\n",
            "Episode: 58/100, Total Reward: 0.5, Epsilon: 0.7515, Total Step: 49\n",
            "Episode: 59/100, Total Reward: 0.0, Epsilon: 0.7477, Total Step: 49\n",
            "Episode: 60/100, Total Reward: 0.5, Epsilon: 0.7440, Total Step: 49\n",
            "Episode: 61/100, Total Reward: 0.0, Epsilon: 0.7403, Total Step: 49\n",
            "Episode: 62/100, Total Reward: 0.0, Epsilon: 0.7366, Total Step: 49\n",
            "Episode: 63/100, Total Reward: 0.0, Epsilon: 0.7329, Total Step: 49\n",
            "Episode: 64/100, Total Reward: 0.0, Epsilon: 0.7292, Total Step: 49\n",
            "Episode: 65/100, Total Reward: 1.0, Epsilon: 0.7256, Total Step: 41\n",
            "Episode: 66/100, Total Reward: 0.5, Epsilon: 0.7219, Total Step: 49\n",
            "Episode: 67/100, Total Reward: 1.0, Epsilon: 0.7183, Total Step: 20\n",
            "Episode: 68/100, Total Reward: 0.5, Epsilon: 0.7147, Total Step: 49\n",
            "Episode: 69/100, Total Reward: 0.5, Epsilon: 0.7112, Total Step: 49\n",
            "Episode: 70/100, Total Reward: 0.5, Epsilon: 0.7076, Total Step: 49\n",
            "Episode: 71/100, Total Reward: 0.5, Epsilon: 0.7041, Total Step: 49\n",
            "Episode: 72/100, Total Reward: 0.5, Epsilon: 0.7005, Total Step: 49\n",
            "Episode: 73/100, Total Reward: 1.0, Epsilon: 0.6970, Total Step: 28\n",
            "Episode: 74/100, Total Reward: 0.5, Epsilon: 0.6936, Total Step: 49\n",
            "Episode: 75/100, Total Reward: 0.5, Epsilon: 0.6901, Total Step: 49\n",
            "Episode: 76/100, Total Reward: 1.0, Epsilon: 0.6866, Total Step: 17\n",
            "Episode: 77/100, Total Reward: 0.5, Epsilon: 0.6832, Total Step: 49\n",
            "Episode: 78/100, Total Reward: 0.5, Epsilon: 0.6798, Total Step: 49\n",
            "Episode: 79/100, Total Reward: 0.5, Epsilon: 0.6764, Total Step: 49\n",
            "Episode: 80/100, Total Reward: 1.0, Epsilon: 0.6730, Total Step: 33\n",
            "Episode: 81/100, Total Reward: 1.0, Epsilon: 0.6696, Total Step: 27\n",
            "Episode: 82/100, Total Reward: 1.0, Epsilon: 0.6663, Total Step: 31\n",
            "Episode: 83/100, Total Reward: 1.0, Epsilon: 0.6630, Total Step: 36\n",
            "Episode: 84/100, Total Reward: 0.5, Epsilon: 0.6597, Total Step: 49\n",
            "Episode: 85/100, Total Reward: 0.5, Epsilon: 0.6564, Total Step: 49\n",
            "Episode: 86/100, Total Reward: 0.5, Epsilon: 0.6531, Total Step: 49\n",
            "Episode: 87/100, Total Reward: 0.5, Epsilon: 0.6498, Total Step: 49\n",
            "Episode: 88/100, Total Reward: 1.0, Epsilon: 0.6466, Total Step: 20\n",
            "Episode: 89/100, Total Reward: 0.5, Epsilon: 0.6433, Total Step: 49\n",
            "Episode: 90/100, Total Reward: 0.5, Epsilon: 0.6401, Total Step: 49\n",
            "Episode: 91/100, Total Reward: 0.5, Epsilon: 0.6369, Total Step: 49\n",
            "Episode: 92/100, Total Reward: 0.5, Epsilon: 0.6337, Total Step: 49\n",
            "Episode: 93/100, Total Reward: 0.5, Epsilon: 0.6306, Total Step: 49\n",
            "Episode: 94/100, Total Reward: 0.5, Epsilon: 0.6274, Total Step: 49\n",
            "Episode: 95/100, Total Reward: 1.0, Epsilon: 0.6243, Total Step: 40\n",
            "Episode: 96/100, Total Reward: 1.0, Epsilon: 0.6211, Total Step: 16\n",
            "Episode: 97/100, Total Reward: 0.5, Epsilon: 0.6180, Total Step: 49\n",
            "Episode: 98/100, Total Reward: 1.0, Epsilon: 0.6149, Total Step: 15\n",
            "Episode: 99/100, Total Reward: 1.0, Epsilon: 0.6119, Total Step: 47\n",
            "Episode: 100/100, Total Reward: 0.5, Epsilon: 0.6088, Total Step: 49\n",
            "TESTING 33: total_reward 26.0/33 \t (reward: 0.5)\n",
            "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=11,maxDistanceApart=5,includeDoors=0,maxDistractorItemsPerLocation=0, 3\n",
            "Episode: 1/100, Total Reward: 1.0, Epsilon: 1.0000, Total Step: 19\n",
            "Episode: 2/100, Total Reward: 1.0, Epsilon: 0.9950, Total Step: 30\n",
            "Episode: 3/100, Total Reward: 0.5, Epsilon: 0.9900, Total Step: 49\n",
            "Episode: 4/100, Total Reward: 0.5, Epsilon: 0.9851, Total Step: 49\n",
            "Episode: 5/100, Total Reward: 0.5, Epsilon: 0.9801, Total Step: 49\n",
            "Episode: 6/100, Total Reward: 0.5, Epsilon: 0.9752, Total Step: 49\n",
            "Episode: 7/100, Total Reward: 0.0, Epsilon: 0.9704, Total Step: 49\n",
            "Episode: 8/100, Total Reward: 0.5, Epsilon: 0.9655, Total Step: 49\n",
            "Episode: 9/100, Total Reward: 0.5, Epsilon: 0.9607, Total Step: 49\n",
            "Episode: 10/100, Total Reward: 0.5, Epsilon: 0.9559, Total Step: 49\n",
            "Episode: 11/100, Total Reward: 0.0, Epsilon: 0.9511, Total Step: 49\n",
            "Episode: 12/100, Total Reward: 0.5, Epsilon: 0.9464, Total Step: 49\n",
            "Episode: 13/100, Total Reward: 0.5, Epsilon: 0.9416, Total Step: 49\n",
            "Episode: 14/100, Total Reward: 0.5, Epsilon: 0.9369, Total Step: 49\n",
            "Episode: 15/100, Total Reward: 0.5, Epsilon: 0.9322, Total Step: 49\n",
            "Episode: 16/100, Total Reward: 0.5, Epsilon: 0.9276, Total Step: 49\n",
            "Episode: 17/100, Total Reward: 0.5, Epsilon: 0.9229, Total Step: 49\n",
            "Episode: 18/100, Total Reward: 1.0, Epsilon: 0.9183, Total Step: 23\n",
            "Episode: 19/100, Total Reward: 1.0, Epsilon: 0.9137, Total Step: 24\n",
            "Episode: 20/100, Total Reward: 0.5, Epsilon: 0.9092, Total Step: 49\n",
            "Episode: 21/100, Total Reward: 0.5, Epsilon: 0.9046, Total Step: 49\n",
            "Episode: 22/100, Total Reward: 1.0, Epsilon: 0.9001, Total Step: 34\n",
            "Episode: 23/100, Total Reward: 0.5, Epsilon: 0.8956, Total Step: 49\n",
            "Episode: 24/100, Total Reward: 0.5, Epsilon: 0.8911, Total Step: 49\n",
            "Episode: 25/100, Total Reward: 0.5, Epsilon: 0.8867, Total Step: 49\n",
            "Episode: 26/100, Total Reward: 0.5, Epsilon: 0.8822, Total Step: 49\n",
            "Episode: 27/100, Total Reward: 0.5, Epsilon: 0.8778, Total Step: 49\n",
            "Episode: 28/100, Total Reward: 0.5, Epsilon: 0.8734, Total Step: 49\n",
            "Episode: 29/100, Total Reward: 1.0, Epsilon: 0.8691, Total Step: 31\n",
            "Episode: 30/100, Total Reward: 0.0, Epsilon: 0.8647, Total Step: 49\n",
            "Episode: 31/100, Total Reward: 1.0, Epsilon: 0.8604, Total Step: 43\n",
            "Episode: 32/100, Total Reward: 0.5, Epsilon: 0.8561, Total Step: 49\n",
            "Episode: 33/100, Total Reward: 0.5, Epsilon: 0.8518, Total Step: 49\n",
            "Episode: 34/100, Total Reward: 0.0, Epsilon: 0.8475, Total Step: 49\n",
            "Episode: 35/100, Total Reward: 0.5, Epsilon: 0.8433, Total Step: 49\n",
            "Episode: 36/100, Total Reward: 0.5, Epsilon: 0.8391, Total Step: 49\n",
            "Episode: 37/100, Total Reward: 1.0, Epsilon: 0.8349, Total Step: 22\n",
            "Episode: 38/100, Total Reward: 0.5, Epsilon: 0.8307, Total Step: 49\n",
            "Episode: 39/100, Total Reward: 0.5, Epsilon: 0.8266, Total Step: 49\n",
            "Episode: 40/100, Total Reward: 0.5, Epsilon: 0.8224, Total Step: 49\n",
            "Episode: 41/100, Total Reward: 1.0, Epsilon: 0.8183, Total Step: 48\n",
            "Episode: 42/100, Total Reward: 0.5, Epsilon: 0.8142, Total Step: 49\n",
            "Episode: 43/100, Total Reward: 1.0, Epsilon: 0.8102, Total Step: 11\n",
            "Episode: 44/100, Total Reward: 0.5, Epsilon: 0.8061, Total Step: 49\n",
            "Episode: 45/100, Total Reward: 1.0, Epsilon: 0.8021, Total Step: 9\n",
            "Episode: 46/100, Total Reward: 0.5, Epsilon: 0.7981, Total Step: 49\n",
            "Episode: 47/100, Total Reward: 0.5, Epsilon: 0.7941, Total Step: 49\n",
            "Episode: 48/100, Total Reward: 0.5, Epsilon: 0.7901, Total Step: 49\n",
            "Episode: 49/100, Total Reward: 0.5, Epsilon: 0.7862, Total Step: 49\n",
            "Episode: 50/100, Total Reward: 1.0, Epsilon: 0.7822, Total Step: 28\n",
            "Episode: 51/100, Total Reward: 0.5, Epsilon: 0.7783, Total Step: 49\n",
            "Episode: 52/100, Total Reward: 0.5, Epsilon: 0.7744, Total Step: 49\n",
            "Episode: 53/100, Total Reward: 1.0, Epsilon: 0.7705, Total Step: 19\n",
            "Episode: 54/100, Total Reward: 0.5, Epsilon: 0.7667, Total Step: 49\n",
            "Episode: 55/100, Total Reward: 0.5, Epsilon: 0.7629, Total Step: 49\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-4b78fc2c783c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mpoints\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-8b5f92be680d>\u001b[0m in \u001b[0;36mrun_all\u001b[0;34m(environments, games, seeds)\u001b[0m\n\u001b[1;32m     32\u001b[0m                            epsilon=EPSILON)\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m           \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPISODES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTHRESHOLD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m           \u001b[0;31m# run the policy to get the plan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-dc7ae8936af9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env, num_episodes, threshold)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# Update the target model periodically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-dc7ae8936af9>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_q_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "results = run_all(environments, games, seeds)\n",
        "points = 0\n",
        "for k, v in results.items():\n",
        "  points += v[-1]\n",
        "  if v[-1] < 1:\n",
        "    print(v[-1], k)\n",
        "print(f\"{points}/{len(results)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acQaep88V13E"
      },
      "source": [
        "# Grading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDcg7IENXtj1"
      },
      "source": [
        "Grading will consist of testing all three environments (regular, stochastic, punishment), two games per environment (coin, mapreader), four sets of parameters per game, and five seeds. There will be a total of 120 tests.\n",
        "There will also be five additional hidden seeds.\n",
        "\n",
        "**Grading:**\n",
        "- 1 point for each correct plan in public games configurations, per algorithm (120 points)\n",
        "- 1 point for each correct plan in hidden game configuration, per algorithm (120 points)\n",
        "\n",
        "Please note that since these environments can be stochastic, and we can only run for so many policy iterations, we will give some leeway with the results for plans and reward values. We will have a benchmark average score across many seeds for an environment, and compare your outputs to our benchmark.\n",
        "\n",
        "Maximum total points: 240\n",
        "\n",
        "# Grading will be conducted by visual inspection (and/or the use of a private autograder) of cell outputs under the \"Testing Suite\" heading. We will compare your plans and reward to our rubric/reference implementations. We will add cells to your notebook at grading time to load and test our hidden world configuration files.\n",
        "\n",
        "We will visually inspect the entire notebook to check if your algorithm implementations include details that are inconsistent with the assignment (e.g., hard-coding values or actions to pass tests) and to make sure no cells were altered to provide unearned grading results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA0XSuitsCP5"
      },
      "source": [
        "# Submission\n",
        "\n",
        "Upload this notebook with the name `submission.ipynb` file to Gradescope. The autograder will only run successfully if your file is named this way. You must ensure that you have removed all print statements from **your** code, or the autograder may fail to run.\n",
        "\n",
        "We've added appropriate comments to the top of certain cells for the autograder to export (`# export`). You do NOT have to do anything (e.g. remove print statements) to cells we have provided - anything related to those have been handled for you. You are responsible for ensuring your own code has no syntax errors or unnecessary print statements. You ***CANNOT*** modify the export comments at the top of the cells, or the autograder will fail to run on your submission.\n",
        "\n",
        "You should ***not*** add any cells to the notebook when submitting. You're welcome to add any code as you need to extra cells when testing, but you must remove them when submitting.\n",
        "\n",
        "If you identify an issue with the autograder, please feel free to reach out to us on Ed Discussion, or email rsudhakar9@gatech.edu, with a subject line including \"CS 3600\"."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "NMJadvTO-O7z",
        "f0GaE8xE-T6A",
        "2sGpjl4N4YP9",
        "UhgPPbfb9ANG"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ade0f6ba1195472fb29f9d4141832cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2274c8560d340a1ae38e85f25817c90",
              "IPY_MODEL_01d2bdd36f814e3fb134a3e9448c85ad",
              "IPY_MODEL_8a9983fac2884ddb9507bfa3065cf12c"
            ],
            "layout": "IPY_MODEL_dd4fbf4066374253ad16143f96a1b1f6"
          }
        },
        "f2274c8560d340a1ae38e85f25817c90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bd0c526ec5548ef94c53abdedd8fa95",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f78d1ce3a6da4c71a515616f4e448b6a",
            "value": "tokenizer_config.json:â€‡100%"
          }
        },
        "01d2bdd36f814e3fb134a3e9448c85ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9176817e2ea84b62979d94147e480530",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef3c33a2a74246999f326a8331208ec8",
            "value": 26
          }
        },
        "8a9983fac2884ddb9507bfa3065cf12c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e06cc9e741f4d3a817ac368d30b19ff",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f943b84dce384188ac572eab0a24967f",
            "value": "â€‡26.0/26.0â€‡[00:00&lt;00:00,â€‡801B/s]"
          }
        },
        "dd4fbf4066374253ad16143f96a1b1f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bd0c526ec5548ef94c53abdedd8fa95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f78d1ce3a6da4c71a515616f4e448b6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9176817e2ea84b62979d94147e480530": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef3c33a2a74246999f326a8331208ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e06cc9e741f4d3a817ac368d30b19ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f943b84dce384188ac572eab0a24967f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1e26d30e6fe4b3f8eacce4b2e499363": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c6ff33b38d94cb48a79d37671741eca",
              "IPY_MODEL_63e00825f6e34fbba86863fd162bf93a",
              "IPY_MODEL_9967e32e6143496f8893e4a8941a2043"
            ],
            "layout": "IPY_MODEL_d9da8f9cd4ba4eab91778d86a309baa6"
          }
        },
        "8c6ff33b38d94cb48a79d37671741eca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d4cd11d435f47c09ee3b560fc53044f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6cb62602bbff41c9a0a9a0e2ac7fde3d",
            "value": "config.json:â€‡100%"
          }
        },
        "63e00825f6e34fbba86863fd162bf93a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acb90a41322e4018a19e00cc2e97f236",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c364fb971bb148b89489b1dd4858e6a9",
            "value": 665
          }
        },
        "9967e32e6143496f8893e4a8941a2043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3acbd604c6b3487eae42ffb73da068e9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_490f64b6f88540c69b52187946a6fc93",
            "value": "â€‡665/665â€‡[00:00&lt;00:00,â€‡15.2kB/s]"
          }
        },
        "d9da8f9cd4ba4eab91778d86a309baa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d4cd11d435f47c09ee3b560fc53044f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cb62602bbff41c9a0a9a0e2ac7fde3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "acb90a41322e4018a19e00cc2e97f236": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c364fb971bb148b89489b1dd4858e6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3acbd604c6b3487eae42ffb73da068e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "490f64b6f88540c69b52187946a6fc93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5ba1c8986ff4ef593201abe949df2d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba9e8fc74a8f46ce87b1512c1e7c1546",
              "IPY_MODEL_14bca33ada594a618759ee84c1ff3494",
              "IPY_MODEL_70c03d55ad6e42b2ad28bd4e94d6c864"
            ],
            "layout": "IPY_MODEL_42a76986c2114f0285f1d113cfaef9e8"
          }
        },
        "ba9e8fc74a8f46ce87b1512c1e7c1546": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b050daab9124a86be8855f82b651a31",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7798dbd0a7404904a11b0199ce3b8ed0",
            "value": "vocab.json:â€‡100%"
          }
        },
        "14bca33ada594a618759ee84c1ff3494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_100bb8dbe1a441e9adf1e81013b7ed5c",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d23744705b6c4a4986ae9a0051ffa767",
            "value": 1042301
          }
        },
        "70c03d55ad6e42b2ad28bd4e94d6c864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf879b0dcb684ba68cf666aee89c9ccb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0eb6a7c0ba214a8ea69091c6b561c5f2",
            "value": "â€‡1.04M/1.04Mâ€‡[00:00&lt;00:00,â€‡2.39MB/s]"
          }
        },
        "42a76986c2114f0285f1d113cfaef9e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b050daab9124a86be8855f82b651a31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7798dbd0a7404904a11b0199ce3b8ed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "100bb8dbe1a441e9adf1e81013b7ed5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d23744705b6c4a4986ae9a0051ffa767": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf879b0dcb684ba68cf666aee89c9ccb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0eb6a7c0ba214a8ea69091c6b561c5f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a72ce34d14e49e9942b611381134e24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47c8e12c443c4a6a947b4b7e8bccb8a6",
              "IPY_MODEL_8d73812e12354b6fb8d9b9ad928c8d86",
              "IPY_MODEL_28813c105e7946b9b4fe9ae500b39023"
            ],
            "layout": "IPY_MODEL_a95d2510d9ad4df683fbfb22e5b4dde0"
          }
        },
        "47c8e12c443c4a6a947b4b7e8bccb8a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1c710352c2740abb073ced77e824987",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_99bf57b7e6714f9ba8ac3e67b0852a22",
            "value": "merges.txt:â€‡100%"
          }
        },
        "8d73812e12354b6fb8d9b9ad928c8d86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1280b721212e463398dad9a9269ec2e4",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6837c8777f244b8a5a48aa23960d0ed",
            "value": 456318
          }
        },
        "28813c105e7946b9b4fe9ae500b39023": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d20276010dd343df9a5498795131f57e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_125b82cd74184e2f9399afafd391c511",
            "value": "â€‡456k/456kâ€‡[00:00&lt;00:00,â€‡14.2MB/s]"
          }
        },
        "a95d2510d9ad4df683fbfb22e5b4dde0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1c710352c2740abb073ced77e824987": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99bf57b7e6714f9ba8ac3e67b0852a22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1280b721212e463398dad9a9269ec2e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6837c8777f244b8a5a48aa23960d0ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d20276010dd343df9a5498795131f57e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "125b82cd74184e2f9399afafd391c511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eba7a4d7aa8e425b9cbe8e31c459483e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bfe3ae9de1674d0baa92919aa8ff476c",
              "IPY_MODEL_fc4f9c38b9a1488c9aa2d17829723785",
              "IPY_MODEL_b01ce2a8b721466590e8aff1a3bfd10d"
            ],
            "layout": "IPY_MODEL_4a5f68ab463643a085166a75405b5d44"
          }
        },
        "bfe3ae9de1674d0baa92919aa8ff476c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45e81bb07cf64455a91aaebb9b592cb8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f14b30395edd4bf0802d9c9a6fb7baad",
            "value": "tokenizer.json:â€‡100%"
          }
        },
        "fc4f9c38b9a1488c9aa2d17829723785": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29a68ad51db7437cb1e708f82597ba4b",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1d5a481148b40e59c1aeb859e6bf5ce",
            "value": 1355256
          }
        },
        "b01ce2a8b721466590e8aff1a3bfd10d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe936253ad4c4b049fe050ae6f7e62ef",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0901f8bc04c747a0841b5108f0a01da0",
            "value": "â€‡1.36M/1.36Mâ€‡[00:00&lt;00:00,â€‡3.14MB/s]"
          }
        },
        "4a5f68ab463643a085166a75405b5d44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45e81bb07cf64455a91aaebb9b592cb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f14b30395edd4bf0802d9c9a6fb7baad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29a68ad51db7437cb1e708f82597ba4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1d5a481148b40e59c1aeb859e6bf5ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe936253ad4c4b049fe050ae6f7e62ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0901f8bc04c747a0841b5108f0a01da0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}