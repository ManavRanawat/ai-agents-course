{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 0 - Part 2: Language Model (LM) Fine-tuning with Huggingface\n",
    "\n",
    "In this assignment, you will implement Pytorch code to train a language model (LM) using the [ðŸ¤— Transformers](https://github.com/huggingface/transformers) library. You will fine-tune a pre-trained GPT-2 model on a [Harry Potter corpus](https://huggingface.co/datasets/WutYee/HarryPotter_books_1to7), and evaluate the model on a lauguage modeling task (a.k.a. next token prediction). If you are familiar with ðŸ¤— Transformers and ðŸ¤— Datasets, feel free to skip steps 0 through 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Installation\n",
    "If you are using Google Colab or a fresh Python environment, you will need to install the required libraries:\n",
    "\n",
    "Uncomment and run the following cell to install ðŸ¤— Transformers and ðŸ¤— Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `transformers`: Provides pre-trained models like GPT-2 for fine-tuning.\n",
    "- `datasets`: Offers easy access to datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Preparing the dataset\n",
    "We will use the [Harry Potter corpus](https://huggingface.co/datasets/WutYee/HarryPotter_books_1to7) dataset to fine-tune the GPT-2. The ðŸ¤— Datasets library makes it simple to load datasets.\n",
    "\n",
    "Run the following code to load the dataset using `load_dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Harry Potter corpus dataset\n",
    "datasets = load_dataset('WutYee/HarryPotter_books_1to7')\n",
    "\n",
    "# Preview the dataset structure\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As shown in `DatasetDict`, the dataset is typically split into subsets like `train`, `test`, or `validation`. To access a specific example, you must choose a split and an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access an example from the 'train' split\n",
    "example = datasets[\"train\"][10]\n",
    "\n",
    "# Print the example\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Preprocessing the Dataset\n",
    "To fine-tune GPT-2, we need to tokenize the dataset text into a format the model can process. To tokenize all our texts with the same vocabulary that was used when training the model, we have to download a pretrained tokenizer. This is all done by the `AutoTokenizer` class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer will split the text into tokens and convert them to numerical IDs.\n",
    "\n",
    "We can now call the tokenizer on all our texts. This is very simple, using the `map` method from the Datasets library. First we define a function that call the tokenizer on our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply it to all the splits in our `datasets` object, using `batched=True` and 4 processes to speed up the preprocessing. We won't need the `text` column afterward, so we discard it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the tokenizer to the dataset\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now look at an element of our datasets, we will see the text have been replaced by the `input_ids` the model will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the harder part: we need to concatenate all our texts together then split the result in small chunks of a certain `block_size`. To do this, we will use the `map` method again, with the option `batched=True`. This option actually lets us change the number of examples in the datasets by returning a different number of examples than we got. This way, we can create our new samples from a batch of examples.\n",
    "\n",
    "First, we grab the maximum length our model was pretrained with. This might be a big too big to fit in your GPU RAM, so here we take a bit less at just 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we write the preprocessing function that will group our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it\n",
    "    # instead of this drop, you can customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First note that we duplicate the inputs for our labels. This is because the model of the ðŸ¤— Transformers library apply the shifting to the right, so we don't need to do it manually.\n",
    "\n",
    "Also note that by default, the `map` method will send a batch of 1,000 examples to be treated by the preprocessing function. So here, we will drop the remainder to make the concatenated tokenized texts a multiple of `block_size` every 1,000 examples. You can adjust this behavior by passing a higher batch size (which will also be processed slower). You can also speed-up the preprocessing by using multiprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can check our datasets have changed: now the samples contain chunks of `block_size` contiguous tokens, potentially spanning over several of our original texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been cleaned, we're ready to train our model. ðŸ¤— Transformers provides APIs and tools to easily download and train pretrained LM models. First we load the pre-trained GPT-2 model using `AutoModelForCausalLM.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to implement fine-tuning for a pre-trained GPT-2 model and evaluate the model on a language modeling task.\n",
    "\n",
    "### Step 3: Fine-Tuning the GPT-2 Model. \n",
    "\n",
    "1. **Implement the Training Loop and Perplexity Evaluation**:  \n",
    "   - Write a training loop to fine-tune the GPT-2 model\n",
    "   - You may experiment with different optimizers, learning rates, and batch sizes. Here are the default values to start with:\n",
    "         - Learning rate: 2e-5\n",
    "         - Optimzer: AdamW\n",
    "         - Batch size: 8\n",
    "   - Include an evaluation function to calculate **perplexity** on the validation set at the end of each epoch.  \n",
    "   - You may refer to open-source trainer implementations such as [miniGPT](https://github.com/karpathy/minGPT/blob/master/mingpt/trainer.py#L81) for guidance.\n",
    "\n",
    "2. **Validation and Test Evaluation**:  \n",
    "   - After each epoch, evaluate your model on the **validation set** and record the perplexity.  \n",
    "   - Once training is complete (after 3 epochs), evaluate the final model on the **test set**.\n",
    "\n",
    "Your goal is to achieve a **perplexity** in the range of **30â€“50** after **3 epochs** of training.\n",
    "\n",
    "To receive full credit, you must report the following:\n",
    "\n",
    "- Training loss and perplexity on the **validation set** for each of the 3 epochs.  \n",
    "- The final perplexity score on the **test set**.\n",
    "\n",
    "e.g.,\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Output**\n",
    "\n",
    "| Epoch | Training Loss | Perplexity on Validation Set |\n",
    "|-------|---------------|-----------------------------|\n",
    "|   1   |     3.14    |          18.37             |\n",
    "|   2   |     2.98    |          17.83             |\n",
    "|   3   |     2.91    |          17.73             |\n",
    "\n",
    "**Final Perplexity on the Test Set**: **43.46**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Submit your code and PDF\n",
    "\n",
    "### Submit your code\n",
    "\n",
    "Zip the `hw0` directory, and submit the zip file to `HW 0 - Code` on Gradescope.\n",
    "\n",
    "### Submit your PDF\n",
    "\n",
    "1. Re-run all cells in order. Make sure that the outputs of all cells are displayed correctly.\n",
    "2. Convert the notebook to PDF: \n",
    "- Download the .ipynb file to your local machine.\n",
    "- Use a tool like `nbconvert` to convert the notebook to PDF.\n",
    "- Alternatively, if you encounter issues with nbconvert, you can save the notebook webpage as a PDF.\n",
    "3. Review the PDF file: Look at the PDF file and make sure all your codes are displayed correctly. \n",
    "5. Submit your PDF and the notebook file on Gradescope: `HW 0 - Part 2 PDF`\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
